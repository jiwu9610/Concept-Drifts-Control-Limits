# -*- coding: utf-8 -*-
"""oscilator_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ewzAPHowX5r8awwLIAwPlaCuMnkNjMO
"""

"""
EWMA concept-drift illustration – nonlinear coupled oscillators
────────────────────────────────────────────────────────────────
• Training & IC data (baseline):
      m1 = 1,  m2 = 2,  k1 = 1,  k2 = 2,  k3 = 1.5,  c1 = 0.1, c2 = 0.2
• OC data (80 % of monitoring stream):
      m1 ← 1.1·m1,   m2 ← 1.2·m2,   k1 ← 1.3·k1
"""

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 0. Imports & global styling
# ╚═══════════════════════════════════════════════════════════════════════════╝
import numpy as np
import torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.model_selection import KFold

RNG = np.random.default_rng(40)

plt.rcParams.update({
    "font.size": 14, "axes.titlesize": 18, "axes.labelsize": 16,
    "legend.fontsize": 14, "xtick.labelsize": 12, "ytick.labelsize": 12,
    "figure.dpi": 160
})

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 1.  Oscillator simulator & data generator
# ╚═══════════════════════════════════════════════════════════════════════════╝
def simulate_system(t, init, p):
    m1, m2, k1, k2, k3, c1, c2 = (p[k] for k in
                                  ("m1", "m2", "k1", "k2", "k3", "c1", "c2"))
    def f(state, _, m1, m2, k1, k2, k3, c1, c2):
        p1, v1, p2, v2 = state
        delta = p1 - p2
        phi   = delta / (1 + abs(delta))
        return [v1,
                (-k1 * p1 - c1 * v1 + k3 * phi) / m1,
                v2,
                (-k2 * p2 - c2 * v2 - k3 * phi) / m2]
    return odeint(f, init, t,
                  args=tuple(p[k] for k in
                             ("m1", "m2", "k1", "k2", "k3", "c1", "c2")))

def generate_stream(n_samples, params, noise_sd=0.03):
    t    = np.linspace(0, 30, n_samples)
    sol  = simulate_system(t, np.array([0.5, 0.0, -0.5, 0.0]), params)
    p1, v1, p2, v2 = sol.T
    delta = p1 - p2
    phi   = delta / (1 + np.abs(delta))
    X = np.column_stack([p1, v1, p2, v2])
    y = (0.5 * (params['m1'] * v1**2 + params['m2'] * v2**2) +
         0.5 * (params['k1'] * p1**2 + params['k2'] * p2**2) +
         params['k3'] * phi +
         RNG.normal(0.0, noise_sd, n_samples))
    return X, y

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 2.  MLP model & helpers
# ╚═══════════════════════════════════════════════════════════════════════════╝
class MLP(nn.Module):
    def __init__(self, p):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(p, 5), nn.ReLU(),
            nn.Linear(5, 1)
        )
    def forward(self, x): return self.net(x)

def train(model, X, y, alpha, epochs=6_000, lr=1e-3):
    opt, loss_fn = optim.Adam(model.parameters(), lr=lr, weight_decay=alpha), nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad()
        loss_fn(model(X).squeeze(), y).backward()
        opt.step()
    return model

def score_vectors(X, y, model, alpha):
    W, b = list(model.net[-1].parameters())          # last Linear layer
    grads, loss_fn = [], nn.MSELoss()
    for xi, yi in zip(X, y):
        model.zero_grad(set_to_none=True)
        loss = loss_fn(model(xi[None]).squeeze(), yi) + alpha * (W.square().sum() + b.square().sum())
        loss.backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()          # (n, 6)

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 3.  EWMA-T² with k-factor scaling
# ╚═══════════════════════════════════════════════════════════════════════════╝
def ewma_T2(stream, lam, mu, Sigma_inv, n_train):
    z, out = np.zeros_like(mu), np.empty(len(stream))
    for t, s in enumerate(stream, 1):
        z = lam * s + (1 - lam) * z
        num = (lam/(2-lam))*(1-(1-lam)**(2*t)) + (3.72/n_train)*(1-(1-lam)**t)**2
        den = (lam/(2-lam))*(1-(1-lam)**(2*t)) + (1   /n_train)*(1-(1-lam)**t)**2
        k   = np.sqrt(num/den)
        diff = (z/k) - mu
        out[t-1] = diff @ Sigma_inv @ diff
    return out

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 4.  Double bootstrap UCL
# ╚═══════════════════════════════════════════════════════════════════════════╝
def bootstrap_ucl(X_tr, y_tr, model_tr, *, alpha, lam, lam_cov,
                  B_outer=50, B_inner=200, M=1_000, perc=99.9):
    S_tr = score_vectors(X_tr, y_tr, model_tr, alpha)
    mu   = S_tr.mean(axis=0)
    Sig  = np.cov(S_tr.T) + lam_cov * np.eye(S_tr.shape[1])
    Sig_inv = np.linalg.pinv(Sig, hermitian=True)
    n_tr = len(X_tr)

    res = np.empty((B_outer*B_inner, M))
    for b in range(B_outer):
        idx_boot = RNG.choice(n_tr, n_tr, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n_tr), idx_boot)
        mdl_b = train(MLP(X_tr.shape[1]).to(X_tr.device),
                      X_tr[idx_boot], y_tr[idx_boot],
                      alpha, epochs=1_000, lr=1e-3)
        S_oob = score_vectors(X_tr[idx_oob], y_tr[idx_oob], mdl_b, alpha)
        for j in range(B_inner):
            idx = RNG.choice(len(S_oob), M, replace=True)
            res[b*B_inner+j] = ewma_T2(S_oob[idx], lam, mu, Sig_inv, n_tr)

    return np.percentile(res, perc, axis=0), mu, Sig_inv

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 5.  Cross-validated R² helper
# ╚═══════════════════════════════════════════════════════════════════════════╝
def cv_r2(model_fn, X, y, alpha, k=5):
    kf, scores = KFold(n_splits=k, shuffle=True, random_state=42), []
    for tr_idx, val_idx in kf.split(X):
        mdl = model_fn()
        train(mdl, X[tr_idx], y[tr_idx], alpha, epochs=6_000, lr=1e-3)
        y_pred = mdl(X[val_idx]).detach().numpy().ravel()
        scores.append(r2_score(y[val_idx].numpy(), y_pred))
    return np.mean(scores)

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 6.  Main routine
# ╚═══════════════════════════════════════════════════════════════════════════╝
def run_monitoring():
    # hyper-params
    alpha, lam, lam_cov = 1e-3, 0.01, 1e-1
    n_tr, n_ic, n_oc    = 3000, 200, 800
    M, perc_ucl         = n_ic+n_oc, 99.9

    base  = dict(m1=1.0, m2=2.0, k1=1.0, k2=2.0, k3=1.5, c1=0.1, c2=0.2)
    shift = {**base, "m1":1.1*base["m1"], "m2":1.2*base["m2"], "k1":1.3*base["k1"]}

    # data
    X_tr, y_tr = generate_stream(n_tr, base)
    X_ic, y_ic = generate_stream(n_ic, base)
    X_oc, y_oc = generate_stream(n_oc, shift)
    X_mon, y_mon = np.vstack([X_ic, X_oc]), np.hstack([y_ic, y_oc])

    # scaling
    scaler_X, scaler_y = StandardScaler(), StandardScaler()
    X_tr_sc  = torch.as_tensor(scaler_X.fit_transform(X_tr), dtype=torch.float32)
    y_tr_sc  = torch.as_tensor(scaler_y.fit_transform(y_tr[:,None]).ravel(), dtype=torch.float32)
    X_mon_sc = torch.as_tensor(scaler_X.transform(X_mon), dtype=torch.float32)
    y_mon_sc = torch.as_tensor(scaler_y.transform(y_mon[:,None]).ravel(), dtype=torch.float32)

    # baseline model & training R²
    model = train(MLP(X_tr_sc.shape[1]), X_tr_sc, y_tr_sc, alpha)
    train_r2 = r2_score(y_tr_sc.numpy(), model(X_tr_sc).detach().numpy())
    print(f"Training R² ≈ {train_r2:.3f}")

    # 5-fold CV R²
    cv_r2_score = cv_r2(lambda: MLP(X_tr_sc.shape[1]), X_tr_sc, y_tr_sc, alpha, k=5)
    print(f"5-fold CV R² ≈ {cv_r2_score:.3f}")

    # bootstrap UCL
    print("Bootstrapping UCL …")
    UCL, mu, Sig_inv = bootstrap_ucl(X_tr_sc, y_tr_sc, model,
                                     alpha=alpha, lam=lam, lam_cov=lam_cov,
                                     B_outer=100, B_inner=200, M=M, perc=perc_ucl)

    # monitoring statistics
    S_mon = score_vectors(X_mon_sc, y_mon_sc, model, alpha)
    T2    = ewma_T2(S_mon, lam, mu, Sig_inv, len(X_tr_sc))

    # plot
    fig, ax = plt.subplots(figsize=(10,6))
    ax.plot(T2, label=r'$T^{\,2}$ statistic', lw=1.7)
    ax.plot(UCL, '-.',  lw=2, color='#d95f02',
    label=r'UCL  ($\alpha = 0.001$)')
    ax.axvline(n_ic-0.5, color='k', lw=1.3, ls='--', label='change-point')
    ax.axvspan(0, n_ic, facecolor='#1f77b4', alpha=0.06)
    ax.axvspan(n_ic, M, facecolor='#d95f02', alpha=0.06)

    ax.set_yscale('log')
    ax.set_xlabel('Sample index $i$')
    ax.set_ylabel(r'$T^{\,2}$  (log scale)')
    #ax.set_title('EWMA Control Chart – nonlinear oscillator\n'
                 #rf'5-fold CV $R^2 \approx {cv_r2_score:.3f}$')
    ax.grid(alpha=0.3, which='both')
    ax.legend()
    plt.tight_layout(); plt.show()

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 7.  Execute
# ╚═══════════════════════════════════════════════════════════════════════════╝
if __name__ == "__main__":
    run_monitoring()

import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt

# ---------- coupled-oscillator simulator ----------
def simulate_system(t, init, p):
    m1, m2, k1, k2, k3, c1, c2 = p
    def f(state, _, m1, m2, k1, k2, k3, c1, c2):
        p1, v1, p2, v2 = state
        delta = p1 - p2
        phi   = delta / (1 + abs(delta))
        return [v1,
                (-k1 * p1 - c1 * v1 + k3 * phi) / m1,
                v2,
                (-k2 * p2 - c2 * v2 - k3 * phi) / m2]
    return odeint(f, init, t, args=tuple(p))

# baseline parameters & grid
params   = (1.0, 2.0, 1.0, 2.0, 1.5, 0.1, 0.2)     # m1,m2,k1,k2,k3,c1,c2
t        = np.linspace(0, 30, 3_000)               # Δt ≈ 0.01 s
p1, _, p2, _ = simulate_system(t, [0.5, 0.0, -0.5, 0.0], params).T

# -------- full view --------
plt.figure(figsize=(10, 4))
plt.plot(t, p1, label=r"$p_1(t)$")
plt.plot(t, p2, label=r"$p_2(t)$")
plt.title("Coupled-oscillator positions (0–30 s)")
plt.xlabel("time $t$ (s)");  plt.ylabel("position (m)")
plt.legend();  plt.grid(alpha=.3);  plt.tight_layout()

# -------- zoom view --------
t_zoom = 2.0
mask   = t <= t_zoom
plt.figure(figsize=(10, 4))
plt.plot(t[mask], p1[mask], label=r"$p_1(t)$")
plt.plot(t[mask], p2[mask], label=r"$p_2(t)$")
plt.title(f"Zoom-in: first {t_zoom} s")
plt.xlabel("time $t$ (s)");  plt.ylabel("position (m)")
plt.legend();  plt.grid(alpha=.3);  plt.tight_layout()
plt.show()

rho = np.corrcoef(p1, p2)[0, 1]      # p1, p2 as in the plot
print(rho)          #  0.38

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt

fig, ax = plt.subplots(2, 1, figsize=(8,6))
plot_acf(p1, lags=100, ax=ax[0]);  ax[0].set_title("ACF of $p_1$")
plot_pacf(p1, lags=100, ax=ax[1]); ax[1].set_title("PACF of $p_1$")
plt.tight_layout(); plt.show()

"""
EWMA concept-drift illustration – nonlinear coupled oscillators
────────────────────────────────────────────────────────────────
• Training & IC data (baseline):
      m1 = 1,  m2 = 2,  k1 = 1,  k2 = 2,  k3 = 1.5,  c1 = 0.1, c2 = 0.2
• OC data (80 % of monitoring stream):
      m1 ← 1.1·m1,   m2 ← 1.2·m2,   k1 ← 1.3·k1
"""

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 0. Imports & global styling
# ╚═══════════════════════════════════════════════════════════════════════════╝
import numpy as np
import torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.model_selection import KFold

RNG = np.random.default_rng(40)

plt.rcParams.update({
    "font.size": 14, "axes.titlesize": 18, "axes.labelsize": 16,
    "legend.fontsize": 14, "xtick.labelsize": 12, "ytick.labelsize": 12,
    "figure.dpi": 160
})

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 1.  Oscillator simulator & data generator
# ╚═══════════════════════════════════════════════════════════════════════════╝
def simulate_system(t, init, p):
    m1, m2, k1, k2, k3, c1, c2 = (p[k] for k in
                                  ("m1", "m2", "k1", "k2", "k3", "c1", "c2"))
    def f(state, _, m1, m2, k1, k2, k3, c1, c2):
        p1, v1, p2, v2 = state
        delta = p1 - p2
        phi   = delta / (1 + abs(delta))
        return [v1,
                (-k1 * p1 - c1 * v1 + k3 * phi) / m1,
                v2,
                (-k2 * p2 - c2 * v2 - k3 * phi) / m2]
    return odeint(f, init, t,
                  args=tuple(p[k] for k in
                             ("m1", "m2", "k1", "k2", "k3", "c1", "c2")))

def generate_stream(n_samples, params, noise_sd=0.3):
    t    = np.linspace(0, 30, n_samples)
    sol  = simulate_system(t, np.array([0.5, 0.0, -0.5, 0.0]), params)
    p1, v1, p2, v2 = sol.T
    delta = p1 - p2
    phi   = delta / (1 + np.abs(delta))
    X = np.column_stack([p1, v1, p2, v2])
    y = (0.5 * (params['m1'] * v1**2 + params['m2'] * v2**2) +
         0.5 * (params['k1'] * p1**2 + params['k2'] * p2**2) +
         params['k3'] * phi +
         RNG.normal(0.0, noise_sd, n_samples))
    return X, y

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 2.  MLP model & helpers
# ╚═══════════════════════════════════════════════════════════════════════════╝
class MLP(nn.Module):
    def __init__(self, p):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(p, 5), nn.ReLU(),
            nn.Linear(5, 1)
        )
    def forward(self, x): return self.net(x)

def train(model, X, y, alpha, epochs=6_000, lr=1e-3):
    opt, loss_fn = optim.Adam(model.parameters(), lr=lr, weight_decay=alpha), nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad()
        loss_fn(model(X).squeeze(), y).backward()
        opt.step()
    return model

def score_vectors(X, y, model, alpha):
    W, b = list(model.net[-1].parameters())          # last Linear layer
    grads, loss_fn = [], nn.MSELoss()
    for xi, yi in zip(X, y):
        model.zero_grad(set_to_none=True)
        loss = loss_fn(model(xi[None]).squeeze(), yi) + alpha * (W.square().sum() + b.square().sum())
        loss.backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()          # (n, 6)

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 3.  EWMA-T² with k-factor scaling
# ╚═══════════════════════════════════════════════════════════════════════════╝
def ewma_T2(stream, lam, mu, Sigma_inv, n_train):
    z, out = np.zeros_like(mu), np.empty(len(stream))
    for t, s in enumerate(stream, 1):
        z = lam * s + (1 - lam) * z
        num = (lam/(2-lam))*(1-(1-lam)**(2*t)) + (3.72/n_train)*(1-(1-lam)**t)**2
        den = (lam/(2-lam))*(1-(1-lam)**(2*t)) + (1   /n_train)*(1-(1-lam)**t)**2
        k   = np.sqrt(num/den)
        diff = (z/k) - mu
        out[t-1] = diff @ Sigma_inv @ diff
    return out

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 4.  Double bootstrap UCL
# ╚═══════════════════════════════════════════════════════════════════════════╝
def bootstrap_ucl(X_tr, y_tr, model_tr, *, alpha, lam, lam_cov,
                  B_outer=50, B_inner=200, M=1_000, perc=99.9):
    S_tr = score_vectors(X_tr, y_tr, model_tr, alpha)
    mu   = S_tr.mean(axis=0)
    Sig  = np.cov(S_tr.T) + lam_cov * np.eye(S_tr.shape[1])
    Sig_inv = np.linalg.pinv(Sig, hermitian=True)
    n_tr = len(X_tr)

    res = np.empty((B_outer*B_inner, M))
    for b in range(B_outer):
        idx_boot = RNG.choice(n_tr, n_tr, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n_tr), idx_boot)
        mdl_b = train(MLP(X_tr.shape[1]).to(X_tr.device),
                      X_tr[idx_boot], y_tr[idx_boot],
                      alpha, epochs=1_000, lr=1e-3)
        S_oob = score_vectors(X_tr[idx_oob], y_tr[idx_oob], mdl_b, alpha)
        for j in range(B_inner):
            idx = RNG.choice(len(S_oob), M, replace=True)
            res[b*B_inner+j] = ewma_T2(S_oob[idx], lam, mu, Sig_inv, n_tr)

    return np.percentile(res, perc, axis=0), mu, Sig_inv

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 5.  Cross-validated R² helper
# ╚═══════════════════════════════════════════════════════════════════════════╝
def cv_r2(model_fn, X, y, alpha, k=5):
    kf, scores = KFold(n_splits=k, shuffle=True, random_state=42), []
    for tr_idx, val_idx in kf.split(X):
        mdl = model_fn()
        train(mdl, X[tr_idx], y[tr_idx], alpha, epochs=6_000, lr=1e-3)
        y_pred = mdl(X[val_idx]).detach().numpy().ravel()
        scores.append(r2_score(y[val_idx].numpy(), y_pred))
    return np.mean(scores)

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 6.  Main routine
# ╚═══════════════════════════════════════════════════════════════════════════╝
def run_monitoring():
    # hyper-params
    alpha, lam, lam_cov = 1e-3, 0.01, 1.0
    n_tr, n_ic, n_oc    = 3_000, 200, 800
    M, perc_ucl         = n_ic+n_oc, 99.9

    base  = dict(m1=1.0, m2=2.0, k1=1.0, k2=2.0, k3=1.5, c1=0.1, c2=0.2)
    shift = {**base, "m1":1.1*base["m1"], "m2":1.2*base["m2"], "k1":1.3*base["k1"]}

    # data
    X_tr, y_tr = generate_stream(n_tr, base)
    X_ic, y_ic = generate_stream(n_ic, base)
    X_oc, y_oc = generate_stream(n_oc, shift)
    X_mon, y_mon = np.vstack([X_ic, X_oc]), np.hstack([y_ic, y_oc])

    # scaling
    scaler_X, scaler_y = StandardScaler(), StandardScaler()
    X_tr_sc  = torch.as_tensor(scaler_X.fit_transform(X_tr), dtype=torch.float32)
    y_tr_sc  = torch.as_tensor(scaler_y.fit_transform(y_tr[:,None]).ravel(), dtype=torch.float32)
    X_mon_sc = torch.as_tensor(scaler_X.transform(X_mon), dtype=torch.float32)
    y_mon_sc = torch.as_tensor(scaler_y.transform(y_mon[:,None]).ravel(), dtype=torch.float32)

    # baseline model & training R²
    model = train(MLP(X_tr_sc.shape[1]), X_tr_sc, y_tr_sc, alpha)
    train_r2 = r2_score(y_tr_sc.numpy(), model(X_tr_sc).detach().numpy())
    print(f"Training R² ≈ {train_r2:.3f}")

    # 5-fold CV R²
    cv_r2_score = cv_r2(lambda: MLP(X_tr_sc.shape[1]), X_tr_sc, y_tr_sc, alpha, k=5)
    print(f"5-fold CV R² ≈ {cv_r2_score:.3f}")

    # bootstrap UCL
    print("Bootstrapping UCL …")
    UCL, mu, Sig_inv = bootstrap_ucl(X_tr_sc, y_tr_sc, model,
                                     alpha=alpha, lam=lam, lam_cov=lam_cov,
                                     B_outer=100, B_inner=200, M=M, perc=perc_ucl)

    # monitoring statistics
    S_mon = score_vectors(X_mon_sc, y_mon_sc, model, alpha)
    T2    = ewma_T2(S_mon, lam, mu, Sig_inv, len(X_tr_sc))

    # plot
    fig, ax = plt.subplots(figsize=(10,6))
    ax.plot(T2, label=r'$T^{\,2}$ statistic', lw=1.7)
    ax.plot(UCL, '--', color='#d95f02', label=f'UCL ({perc_ucl:.1f}th %ile)')
    ax.axvline(n_ic-0.5, color='k', lw=1.3, ls='--', label='shift point')
    ax.axvspan(0, n_ic, facecolor='#1f77b4', alpha=0.06)
    ax.axvspan(n_ic, M, facecolor='#d95f02', alpha=0.06)

    ax.set_yscale('log')
    ax.set_xlabel('Sample index $i$')
    ax.set_ylabel(r'$T^{\,2}$  (log scale)')
    #ax.set_title('EWMA Control Chart – nonlinear oscillator\n'
                 #rf'5-fold CV $R^2 \approx {cv_r2_score:.3f}$')
    ax.grid(alpha=0.3, which='both')
    ax.legend()
    plt.tight_layout(); plt.show()

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 7.  Execute
# ╚═══════════════════════════════════════════════════════════════════════════╝
if __name__ == "__main__":
    run_monitoring()

"""
EWMA concept-drift illustration – nonlinear coupled oscillators
────────────────────────────────────────────────────────────────
• Training & IC data (baseline):
      m1 = 1,  m2 = 2,  k1 = 1,  k2 = 2,  k3 = 1.5,  c1 = 0.1, c2 = 0.2
• OC data (80 % of monitoring stream):
      m1 ← 1.1·m1,   m2 ← 1.2·m2,   k1 ← 1.3·k1
"""

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 0. Imports & global styling
# ╚═══════════════════════════════════════════════════════════════════════════╝
import numpy as np
import torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.model_selection import KFold

RNG = np.random.default_rng(40)

plt.rcParams.update({
    "font.size": 14, "axes.titlesize": 18, "axes.labelsize": 16,
    "legend.fontsize": 14, "xtick.labelsize": 12, "ytick.labelsize": 12,
    "figure.dpi": 160
})

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 1.  Oscillator simulator & data generator
# ╚═══════════════════════════════════════════════════════════════════════════╝
def simulate_system(t, init, p):
    m1, m2, k1, k2, k3, c1, c2 = (p[k] for k in
                                  ("m1", "m2", "k1", "k2", "k3", "c1", "c2"))
    def f(state, _, m1, m2, k1, k2, k3, c1, c2):
        p1, v1, p2, v2 = state
        delta = p1 - p2
        phi   = delta / (1 + abs(delta))
        return [v1,
                (-k1 * p1 - c1 * v1 + k3 * phi) / m1,
                v2,
                (-k2 * p2 - c2 * v2 - k3 * phi) / m2]
    return odeint(f, init, t,
                  args=tuple(p[k] for k in
                             ("m1", "m2", "k1", "k2", "k3", "c1", "c2")))

def generate_stream(n_samples, params, noise_sd=0.3):
    t    = np.linspace(0, 30, n_samples)
    sol  = simulate_system(t, np.array([0.5, 0.0, -0.5, 0.0]), params)
    p1, v1, p2, v2 = sol.T
    delta = p1 - p2
    phi   = delta / (1 + np.abs(delta))
    X = np.column_stack([p1, v1, p2, v2])
    y = (0.5 * (params['m1'] * v1**2 + params['m2'] * v2**2) +
         0.5 * (params['k1'] * p1**2 + params['k2'] * p2**2) +
         params['k3'] * phi +
         RNG.normal(0.0, noise_sd, n_samples))
    return X, y

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 2.  MLP model & helpers
# ╚═══════════════════════════════════════════════════════════════════════════╝
class MLP(nn.Module):
    def __init__(self, p):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(p, 5), nn.ReLU(),
            nn.Linear(5, 1)
        )
    def forward(self, x): return self.net(x)

def train(model, X, y, alpha, epochs=6_000, lr=1e-3):
    opt, loss_fn = optim.Adam(model.parameters(), lr=lr, weight_decay=alpha), nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad()
        loss_fn(model(X).squeeze(), y).backward()
        opt.step()
    return model

def score_vectors(X, y, model, alpha):
    W, b = list(model.net[-1].parameters())          # last Linear layer
    grads, loss_fn = [], nn.MSELoss()
    for xi, yi in zip(X, y):
        model.zero_grad(set_to_none=True)
        loss = loss_fn(model(xi[None]).squeeze(), yi) + alpha * (W.square().sum() + b.square().sum())
        loss.backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()          # (n, 6)

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 3.  EWMA-T² with k-factor scaling
# ╚═══════════════════════════════════════════════════════════════════════════╝
def ewma_T2(stream, lam, mu, Sigma_inv, n_train):
    z, out = np.zeros_like(mu), np.empty(len(stream))
    for t, s in enumerate(stream, 1):
        z = lam * s + (1 - lam) * z
        num = (lam/(2-lam))*(1-(1-lam)**(2*t)) + (3.72/n_train)*(1-(1-lam)**t)**2
        den = (lam/(2-lam))*(1-(1-lam)**(2*t)) + (1   /n_train)*(1-(1-lam)**t)**2
        k   = np.sqrt(num/den)
        diff = (z/k) - mu
        out[t-1] = diff @ Sigma_inv @ diff
    return out

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 4.  Double bootstrap UCL
# ╚═══════════════════════════════════════════════════════════════════════════╝
def bootstrap_ucl(X_tr, y_tr, model_tr, *, alpha, lam, lam_cov,
                  B_outer=50, B_inner=200, M=1_000, perc=99.9):
    S_tr = score_vectors(X_tr, y_tr, model_tr, alpha)
    mu   = S_tr.mean(axis=0)
    Sig  = np.cov(S_tr.T) + lam_cov * np.eye(S_tr.shape[1])
    Sig_inv = np.linalg.pinv(Sig, hermitian=True)
    n_tr = len(X_tr)

    res = np.empty((B_outer*B_inner, M))
    for b in range(B_outer):
        idx_boot = RNG.choice(n_tr, n_tr, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n_tr), idx_boot)
        mdl_b = train(MLP(X_tr.shape[1]).to(X_tr.device),
                      X_tr[idx_boot], y_tr[idx_boot],
                      alpha, epochs=1_000, lr=1e-3)
        S_oob = score_vectors(X_tr[idx_oob], y_tr[idx_oob], mdl_b, alpha)
        for j in range(B_inner):
            idx = RNG.choice(len(S_oob), M, replace=True)
            res[b*B_inner+j] = ewma_T2(S_oob[idx], lam, mu, Sig_inv, n_tr)

    return np.percentile(res, perc, axis=0), mu, Sig_inv

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 5.  Cross-validated R² helper
# ╚═══════════════════════════════════════════════════════════════════════════╝
def cv_r2(model_fn, X, y, alpha, k=5):
    kf, scores = KFold(n_splits=k, shuffle=True, random_state=42), []
    for tr_idx, val_idx in kf.split(X):
        mdl = model_fn()
        train(mdl, X[tr_idx], y[tr_idx], alpha, epochs=6_000, lr=1e-3)
        y_pred = mdl(X[val_idx]).detach().numpy().ravel()
        scores.append(r2_score(y[val_idx].numpy(), y_pred))
    return np.mean(scores)

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 6.  Main routine
# ╚═══════════════════════════════════════════════════════════════════════════╝
def run_monitoring():
    # hyper-params
    alpha, lam, lam_cov = 1e-3, 0.01, 1.0
    n_tr, n_ic, n_oc    = 3_000, 200, 800
    M, perc_ucl         = n_ic+n_oc, 99.9

    base  = dict(m1=1.0, m2=2.0, k1=1.0, k2=2.0, k3=1.5, c1=0.1, c2=0.2)
    shift = {**base, "m1":1.1*base["m1"], "m2":1.2*base["m2"], "k1":1.3*base["k1"]}

    # data
    X_tr, y_tr = generate_stream(n_tr, base)
    X_ic, y_ic = generate_stream(n_ic, base)
    X_oc, y_oc = generate_stream(n_oc, shift)
    X_mon, y_mon = np.vstack([X_ic, X_oc]), np.hstack([y_ic, y_oc])

    # scaling
    scaler_X, scaler_y = StandardScaler(), StandardScaler()
    X_tr_sc  = torch.as_tensor(scaler_X.fit_transform(X_tr), dtype=torch.float32)
    y_tr_sc  = torch.as_tensor(scaler_y.fit_transform(y_tr[:,None]).ravel(), dtype=torch.float32)
    X_mon_sc = torch.as_tensor(scaler_X.transform(X_mon), dtype=torch.float32)
    y_mon_sc = torch.as_tensor(scaler_y.transform(y_mon[:,None]).ravel(), dtype=torch.float32)

    # baseline model & training R²
    model = train(MLP(X_tr_sc.shape[1]), X_tr_sc, y_tr_sc, alpha)
    train_r2 = r2_score(y_tr_sc.numpy(), model(X_tr_sc).detach().numpy())
    print(f"Training R² ≈ {train_r2:.3f}")

    # 5-fold CV R²
    cv_r2_score = cv_r2(lambda: MLP(X_tr_sc.shape[1]), X_tr_sc, y_tr_sc, alpha, k=5)
    print(f"5-fold CV R² ≈ {cv_r2_score:.3f}")

    # bootstrap UCL
    print("Bootstrapping UCL …")
    UCL, mu, Sig_inv = bootstrap_ucl(X_tr_sc, y_tr_sc, model,
                                     alpha=alpha, lam=lam, lam_cov=lam_cov,
                                     B_outer=100, B_inner=200, M=M, perc=perc_ucl)

    # monitoring statistics
    S_mon = score_vectors(X_mon_sc, y_mon_sc, model, alpha)
    T2    = ewma_T2(S_mon, lam, mu, Sig_inv, len(X_tr_sc))

    # plot
    fig, ax = plt.subplots(figsize=(10,6))
    ax.plot(T2, label=r'$T^{\,2}$ statistic', lw=1.7)
    ax.plot(UCL, '--', color='#d95f02', label=f'UCL ({perc_ucl:.1f}th %ile)')
    ax.axvline(n_ic-0.5, color='k', lw=1.3, ls='--', label='shift point')
    ax.axvspan(0, n_ic, facecolor='#1f77b4', alpha=0.06)
    ax.axvspan(n_ic, M, facecolor='#d95f02', alpha=0.06)

    ax.set_yscale('log')
    ax.set_xlabel('Sample index $i$')
    ax.set_ylabel(r'$T^{\,2}$  (log scale)')
    #ax.set_title('EWMA Control Chart – nonlinear oscillator\n'
                 #rf'5-fold CV $R^2 \approx {cv_r2_score:.3f}$')
    ax.grid(alpha=0.3, which='both')
    ax.legend()
    plt.tight_layout(); plt.show()

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 7.  Execute
# ╚═══════════════════════════════════════════════════════════════════════════╝
if __name__ == "__main__":
    run_monitoring()

"""
EWMA concept-drift illustration – nonlinear coupled oscillators
────────────────────────────────────────────────────────────────
• Training & IC data (baseline):
      m1 = 1,  m2 = 2,  k1 = 1,  k2 = 2,  k3 = 1.5,  c1 = 0.1, c2 = 0.2
• OC data (80 % of monitoring stream):
      m1 ← 1.1·m1,   m2 ← 1.2·m2,   k1 ← 1.3·k1
"""

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 0. Imports & global styling
# ╚═══════════════════════════════════════════════════════════════════════════╝
import numpy as np
import torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.model_selection import KFold

RNG = np.random.default_rng(40)

plt.rcParams.update({
    "font.size": 14, "axes.titlesize": 18, "axes.labelsize": 16,
    "legend.fontsize": 14, "xtick.labelsize": 12, "ytick.labelsize": 12,
    "figure.dpi": 160
})

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 1.  Oscillator simulator & data generator
# ╚═══════════════════════════════════════════════════════════════════════════╝
def simulate_system(t, init, p):
    m1, m2, k1, k2, k3, c1, c2 = (p[k] for k in
                                  ("m1", "m2", "k1", "k2", "k3", "c1", "c2"))
    def f(state, _, m1, m2, k1, k2, k3, c1, c2):
        p1, v1, p2, v2 = state
        delta = p1 - p2
        phi   = delta / (1 + abs(delta))
        return [v1,
                (-k1 * p1 - c1 * v1 + k3 * phi) / m1,
                v2,
                (-k2 * p2 - c2 * v2 - k3 * phi) / m2]
    return odeint(f, init, t,
                  args=tuple(p[k] for k in
                             ("m1", "m2", "k1", "k2", "k3", "c1", "c2")))

def generate_stream(n_samples, params, noise_sd=0.3):
    t    = np.linspace(0, 30, n_samples)
    sol  = simulate_system(t, np.array([0.5, 0.0, -0.5, 0.0]), params)
    p1, v1, p2, v2 = sol.T
    delta = p1 - p2
    phi   = delta / (1 + np.abs(delta))
    X = np.column_stack([p1, v1, p2, v2])
    y = (0.5 * (params['m1'] * v1**2 + params['m2'] * v2**2) +
         0.5 * (params['k1'] * p1**2 + params['k2'] * p2**2) +
         params['k3'] * phi +
         RNG.normal(0.0, noise_sd, n_samples))
    return X, y

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 2.  MLP model & helpers
# ╚═══════════════════════════════════════════════════════════════════════════╝
class MLP(nn.Module):
    def __init__(self, p):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(p, 5), nn.ReLU(),
            nn.Linear(5, 1)
        )
    def forward(self, x): return self.net(x)

def train(model, X, y, alpha, epochs=6_000, lr=1e-3):
    opt, loss_fn = optim.Adam(model.parameters(), lr=lr, weight_decay=alpha), nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad()
        loss_fn(model(X).squeeze(), y).backward()
        opt.step()
    return model

def score_vectors(X, y, model, alpha):
    W, b = list(model.net[-1].parameters())          # last Linear layer
    grads, loss_fn = [], nn.MSELoss()
    for xi, yi in zip(X, y):
        model.zero_grad(set_to_none=True)
        loss = loss_fn(model(xi[None]).squeeze(), yi) + alpha * (W.square().sum() + b.square().sum())
        loss.backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()          # (n, 6)

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 3.  EWMA-T² with k-factor scaling
# ╚═══════════════════════════════════════════════════════════════════════════╝
def ewma_T2(stream, lam, mu, Sigma_inv, n_train):
    z, out = np.zeros_like(mu), np.empty(len(stream))
    for t, s in enumerate(stream, 1):
        z = lam * s + (1 - lam) * z
        num = (lam/(2-lam))*(1-(1-lam)**(2*t)) + (3.72/n_train)*(1-(1-lam)**t)**2
        den = (lam/(2-lam))*(1-(1-lam)**(2*t)) + (1   /n_train)*(1-(1-lam)**t)**2
        k   = np.sqrt(num/den)
        diff = (z/k) - mu
        out[t-1] = diff @ Sigma_inv @ diff
    return out

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 4.  Double bootstrap UCL
# ╚═══════════════════════════════════════════════════════════════════════════╝
def bootstrap_ucl(X_tr, y_tr, model_tr, *, alpha, lam, lam_cov,
                  B_outer=50, B_inner=200, M=1_000, perc=99.9):
    S_tr = score_vectors(X_tr, y_tr, model_tr, alpha)
    mu   = S_tr.mean(axis=0)
    Sig  = np.cov(S_tr.T) + lam_cov * np.eye(S_tr.shape[1])
    Sig_inv = np.linalg.pinv(Sig, hermitian=True)
    n_tr = len(X_tr)

    res = np.empty((B_outer*B_inner, M))
    for b in range(B_outer):
        idx_boot = RNG.choice(n_tr, n_tr, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n_tr), idx_boot)
        mdl_b = train(MLP(X_tr.shape[1]).to(X_tr.device),
                      X_tr[idx_boot], y_tr[idx_boot],
                      alpha, epochs=1_000, lr=1e-3)
        S_oob = score_vectors(X_tr[idx_oob], y_tr[idx_oob], mdl_b, alpha)
        for j in range(B_inner):
            idx = RNG.choice(len(S_oob), M, replace=True)
            res[b*B_inner+j] = ewma_T2(S_oob[idx], lam, mu, Sig_inv, n_tr)

    return np.percentile(res, perc, axis=0), mu, Sig_inv

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 5.  Cross-validated R² helper
# ╚═══════════════════════════════════════════════════════════════════════════╝
def cv_r2(model_fn, X, y, alpha, k=5):
    kf, scores = KFold(n_splits=k, shuffle=True, random_state=42), []
    for tr_idx, val_idx in kf.split(X):
        mdl = model_fn()
        train(mdl, X[tr_idx], y[tr_idx], alpha, epochs=6_000, lr=1e-3)
        y_pred = mdl(X[val_idx]).detach().numpy().ravel()
        scores.append(r2_score(y[val_idx].numpy(), y_pred))
    return np.mean(scores)

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 6.  Main routine
# ╚═══════════════════════════════════════════════════════════════════════════╝
def run_monitoring():
    # hyper-params
    alpha, lam, lam_cov = 1e-3, 0.01, 1.0
    n_tr, n_ic, n_oc    = 3_000, 200, 800
    M, perc_ucl         = n_ic+n_oc, 99.9

    base  = dict(m1=1.0, m2=2.0, k1=1.0, k2=2.0, k3=1.5, c1=0.1, c2=0.2)
    shift = {**base, "m1":1.1*base["m1"], "m2":1.2*base["m2"], "k1":1.3*base["k1"]}

    # data
    X_tr, y_tr = generate_stream(n_tr, base)
    X_ic, y_ic = generate_stream(n_ic, base)
    X_oc, y_oc = generate_stream(n_oc, shift)
    X_mon, y_mon = np.vstack([X_ic, X_oc]), np.hstack([y_ic, y_oc])

    # scaling
    scaler_X, scaler_y = StandardScaler(), StandardScaler()
    X_tr_sc  = torch.as_tensor(scaler_X.fit_transform(X_tr), dtype=torch.float32)
    y_tr_sc  = torch.as_tensor(scaler_y.fit_transform(y_tr[:,None]).ravel(), dtype=torch.float32)
    X_mon_sc = torch.as_tensor(scaler_X.transform(X_mon), dtype=torch.float32)
    y_mon_sc = torch.as_tensor(scaler_y.transform(y_mon[:,None]).ravel(), dtype=torch.float32)

    # baseline model & training R²
    model = train(MLP(X_tr_sc.shape[1]), X_tr_sc, y_tr_sc, alpha)
    train_r2 = r2_score(y_tr_sc.numpy(), model(X_tr_sc).detach().numpy())
    print(f"Training R² ≈ {train_r2:.3f}")

    # 5-fold CV R²
    cv_r2_score = cv_r2(lambda: MLP(X_tr_sc.shape[1]), X_tr_sc, y_tr_sc, alpha, k=5)
    print(f"5-fold CV R² ≈ {cv_r2_score:.3f}")

    # bootstrap UCL
    print("Bootstrapping UCL …")
    UCL, mu, Sig_inv = bootstrap_ucl(X_tr_sc, y_tr_sc, model,
                                     alpha=alpha, lam=lam, lam_cov=lam_cov,
                                     B_outer=100, B_inner=200, M=M, perc=perc_ucl)

    # monitoring statistics
    S_mon = score_vectors(X_mon_sc, y_mon_sc, model, alpha)
    T2    = ewma_T2(S_mon, lam, mu, Sig_inv, len(X_tr_sc))

    # plot
    fig, ax = plt.subplots(figsize=(10,6))
    ax.plot(T2, label=r'$T^{\,2}$ statistic', lw=1.7)
    ax.plot(UCL, '--', color='#d95f02', label=f'UCL ({perc_ucl:.1f}th %ile)')
    ax.axvline(n_ic-0.5, color='k', lw=1.3, ls='--', label='shift point')
    ax.axvspan(0, n_ic, facecolor='#1f77b4', alpha=0.06)
    ax.axvspan(n_ic, M, facecolor='#d95f02', alpha=0.06)

    ax.set_yscale('log')
    ax.set_xlabel('Sample index $i$')
    ax.set_ylabel(r'$T^{\,2}$  (log scale)')
    #ax.set_title('EWMA Control Chart – nonlinear oscillator\n'
                 #rf'5-fold CV $R^2 \approx {cv_r2_score:.3f}$')
    ax.grid(alpha=0.3, which='both')
    ax.legend()
    plt.tight_layout(); plt.show()

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 7.  Execute
# ╚═══════════════════════════════════════════════════════════════════════════╝
if __name__ == "__main__":
    run_monitoring()

"""
EWMA concept-drift illustration – nonlinear coupled oscillators
────────────────────────────────────────────────────────────────
• Training & IC data (baseline):
      m1 = 1,  m2 = 2,  k1 = 1,  k2 = 2,  k3 = 1.5,  c1 = 0.1, c2 = 0.2
• OC data (80 % of monitoring stream):
      m1 ← 1.1·m1,   m2 ← 1.2·m2,   k1 ← 1.3·k1
"""

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 0. Imports & global styling
# ╚═══════════════════════════════════════════════════════════════════════════╝
import numpy as np
import torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.model_selection import KFold

RNG = np.random.default_rng(40)

plt.rcParams.update({
    "font.size": 14, "axes.titlesize": 18, "axes.labelsize": 16,
    "legend.fontsize": 14, "xtick.labelsize": 12, "ytick.labelsize": 12,
    "figure.dpi": 160
})

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 1.  Oscillator simulator & data generator
# ╚═══════════════════════════════════════════════════════════════════════════╝
def simulate_system(t, init, p):
    m1, m2, k1, k2, k3, c1, c2 = (p[k] for k in
                                  ("m1", "m2", "k1", "k2", "k3", "c1", "c2"))
    def f(state, _, m1, m2, k1, k2, k3, c1, c2):
        p1, v1, p2, v2 = state
        delta = p1 - p2
        phi   = delta / (1 + abs(delta))
        return [v1,
                (-k1 * p1 - c1 * v1 + k3 * phi) / m1,
                v2,
                (-k2 * p2 - c2 * v2 - k3 * phi) / m2]
    return odeint(f, init, t,
                  args=tuple(p[k] for k in
                             ("m1", "m2", "k1", "k2", "k3", "c1", "c2")))

def generate_stream(n_samples, params, noise_sd=0.03):
    t    = np.linspace(0, 30, n_samples)
    sol  = simulate_system(t, np.array([0.5, 0.0, -0.5, 0.0]), params)
    p1, v1, p2, v2 = sol.T
    delta = p1 - p2
    phi   = delta / (1 + np.abs(delta))
    X = np.column_stack([p1, v1, p2, v2])
    y = (0.5 * (params['m1'] * v1**2 + params['m2'] * v2**2) +
         0.5 * (params['k1'] * p1**2 + params['k2'] * p2**2) +
         params['k3'] * phi +
         RNG.normal(0.0, noise_sd, n_samples))
    return X, y

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 2.  MLP model & helpers
# ╚═══════════════════════════════════════════════════════════════════════════╝
class MLP(nn.Module):
    def __init__(self, p):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(p, 5), nn.ReLU(),
            nn.Linear(5, 1)
        )
    def forward(self, x): return self.net(x)

def train(model, X, y, alpha, epochs=6_000, lr=1e-3):
    opt, loss_fn = optim.Adam(model.parameters(), lr=lr, weight_decay=alpha), nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad()
        loss_fn(model(X).squeeze(), y).backward()
        opt.step()
    return model

def score_vectors(X, y, model, alpha):
    W, b = list(model.net[-1].parameters())          # last Linear layer
    grads, loss_fn = [], nn.MSELoss()
    for xi, yi in zip(X, y):
        model.zero_grad(set_to_none=True)
        loss = loss_fn(model(xi[None]).squeeze(), yi) + alpha * (W.square().sum() + b.square().sum())
        loss.backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()          # (n, 6)

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 3.  EWMA-T² with k-factor scaling
# ╚═══════════════════════════════════════════════════════════════════════════╝
def ewma_T2(stream, lam, mu, Sigma_inv, n_train):
    z, out = np.zeros_like(mu), np.empty(len(stream))
    for t, s in enumerate(stream, 1):
        z = lam * s + (1 - lam) * z
        num = (lam/(2-lam))*(1-(1-lam)**(2*t)) + (3.72/n_train)*(1-(1-lam)**t)**2
        den = (lam/(2-lam))*(1-(1-lam)**(2*t)) + (1   /n_train)*(1-(1-lam)**t)**2
        k   = np.sqrt(num/den)
        diff = (z/k) - mu
        out[t-1] = diff @ Sigma_inv @ diff
    return out

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 4.  Double bootstrap UCL
# ╚═══════════════════════════════════════════════════════════════════════════╝
def bootstrap_ucl(X_tr, y_tr, model_tr, *, alpha, lam, lam_cov,
                  B_outer=50, B_inner=200, M=1_000, perc=99.9):
    S_tr = score_vectors(X_tr, y_tr, model_tr, alpha)
    mu   = S_tr.mean(axis=0)
    Sig  = np.cov(S_tr.T) + lam_cov * np.eye(S_tr.shape[1])
    Sig_inv = np.linalg.pinv(Sig, hermitian=True)
    n_tr = len(X_tr)

    res = np.empty((B_outer*B_inner, M))
    for b in range(B_outer):
        idx_boot = RNG.choice(n_tr, n_tr, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n_tr), idx_boot)
        mdl_b = train(MLP(X_tr.shape[1]).to(X_tr.device),
                      X_tr[idx_boot], y_tr[idx_boot],
                      alpha, epochs=1_000, lr=1e-3)
        S_oob = score_vectors(X_tr[idx_oob], y_tr[idx_oob], mdl_b, alpha)
        for j in range(B_inner):
            idx = RNG.choice(len(S_oob), M, replace=True)
            res[b*B_inner+j] = ewma_T2(S_oob[idx], lam, mu, Sig_inv, n_tr)

    return np.percentile(res, perc, axis=0), mu, Sig_inv

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 5.  Cross-validated R² helper
# ╚═══════════════════════════════════════════════════════════════════════════╝
def cv_r2(model_fn, X, y, alpha, k=5):
    kf, scores = KFold(n_splits=k, shuffle=True, random_state=42), []
    for tr_idx, val_idx in kf.split(X):
        mdl = model_fn()
        train(mdl, X[tr_idx], y[tr_idx], alpha, epochs=6_000, lr=1e-3)
        y_pred = mdl(X[val_idx]).detach().numpy().ravel()
        scores.append(r2_score(y[val_idx].numpy(), y_pred))
    return np.mean(scores)

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 6.  Main routine
# ╚═══════════════════════════════════════════════════════════════════════════╝
def run_monitoring():
    # hyper-params
    alpha, lam, lam_cov = 1e-3, 0.01, 1.0
    n_tr, n_ic, n_oc    = 200, 200, 800
    M, perc_ucl         = n_ic+n_oc, 99.9

    base  = dict(m1=1.0, m2=2.0, k1=1.0, k2=2.0, k3=1.5, c1=0.1, c2=0.2)
    shift = {**base, "m1":1.1*base["m1"], "m2":1.2*base["m2"], "k1":1.3*base["k1"]}

    # data
    X_tr, y_tr = generate_stream(n_tr, base)
    X_ic, y_ic = generate_stream(n_ic, base)
    X_oc, y_oc = generate_stream(n_oc, shift)
    X_mon, y_mon = np.vstack([X_ic, X_oc]), np.hstack([y_ic, y_oc])

    # scaling
    scaler_X, scaler_y = StandardScaler(), StandardScaler()
    X_tr_sc  = torch.as_tensor(scaler_X.fit_transform(X_tr), dtype=torch.float32)
    y_tr_sc  = torch.as_tensor(scaler_y.fit_transform(y_tr[:,None]).ravel(), dtype=torch.float32)
    X_mon_sc = torch.as_tensor(scaler_X.transform(X_mon), dtype=torch.float32)
    y_mon_sc = torch.as_tensor(scaler_y.transform(y_mon[:,None]).ravel(), dtype=torch.float32)

    # baseline model & training R²
    model = train(MLP(X_tr_sc.shape[1]), X_tr_sc, y_tr_sc, alpha)
    train_r2 = r2_score(y_tr_sc.numpy(), model(X_tr_sc).detach().numpy())
    print(f"Training R² ≈ {train_r2:.3f}")

    # 5-fold CV R²
    cv_r2_score = cv_r2(lambda: MLP(X_tr_sc.shape[1]), X_tr_sc, y_tr_sc, alpha, k=5)
    print(f"5-fold CV R² ≈ {cv_r2_score:.3f}")

    # bootstrap UCL
    print("Bootstrapping UCL …")
    UCL, mu, Sig_inv = bootstrap_ucl(X_tr_sc, y_tr_sc, model,
                                     alpha=alpha, lam=lam, lam_cov=lam_cov,
                                     B_outer=100, B_inner=200, M=M, perc=perc_ucl)

    # monitoring statistics
    S_mon = score_vectors(X_mon_sc, y_mon_sc, model, alpha)
    T2    = ewma_T2(S_mon, lam, mu, Sig_inv, len(X_tr_sc))

    # plot
    fig, ax = plt.subplots(figsize=(10,6))
    ax.plot(T2, label=r'$T^{\,2}$ statistic', lw=1.7)
    ax.plot(UCL, '--', color='#d95f02', label=f'UCL ({perc_ucl:.1f}th %ile)')
    ax.axvline(n_ic-0.5, color='k', lw=1.3, ls='--', label='shift point')
    ax.axvspan(0, n_ic, facecolor='#1f77b4', alpha=0.06)
    ax.axvspan(n_ic, M, facecolor='#d95f02', alpha=0.06)

    ax.set_yscale('log')
    ax.set_xlabel('Sample index $i$')
    ax.set_ylabel(r'$T^{\,2}$  (log scale)')
    #ax.set_title('EWMA Control Chart – nonlinear oscillator\n'
                 #rf'5-fold CV $R^2 \approx {cv_r2_score:.3f}$')
    ax.grid(alpha=0.3, which='both')
    ax.legend()
    plt.tight_layout(); plt.show()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
FAR verification – nonlinear coupled oscillators
• blue  = our double-bootstrap, k-scaled EWMA-T²  (vector UCL)
• orange = KG / Zhang two-phase baseline          (scalar UCL)
Nominal α = 0.001   ⇒   99.9-th percentile limits
"""

# ─────────────────── 0. imports & global style ─────────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing, tqdm, warnings
warnings.filterwarnings("ignore", category=UserWarning)

plt.rcParams.update({
    "font.size":14, "axes.titlesize":18, "axes.labelsize":16,
    "legend.fontsize":14, "xtick.labelsize":12, "ytick.labelsize":12,
    "figure.dpi":160
})
RNG = np.random.default_rng(123)

# ─────────────────── 1. simulator & data generator ────────────────────────
def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(state, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = state
        delta = p1-p2; phi = delta/(1+abs(delta))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_stream(n, p, noise=0.03):
    t = np.linspace(0,30,n)
    p1,v1,p2,v2 = simulate(t,[.5,0,-.5,0],p).T
    delta = p1-p2; phi = delta/(1+np.abs(delta))
    X = np.column_stack([p1,v1,p2,v2])
    y = (0.5*(p['m1']*v1**2 + p['m2']*v2**2)
        +0.5*(p['k1']*p1**2 + p['k2']*p2**2)
        + p['k3']*phi + RNG.normal(0,noise,n))
    return X,y

# ─────────────────── 2. tiny MLP & helpers ────────────────────────────────
class MLP(nn.Module):
    def __init__(self,d): super().__init__(); self.net=nn.Sequential(
        nn.Linear(d,5),nn.ReLU(),nn.Linear(5,1))
    def forward(self,x): return self.net(x)

def train(net,X,y,α,epochs=3_000,lr=1e-3):
    opt,loss=optim.Adam(net.parameters(),lr=lr,weight_decay=α),nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad(); loss(net(X).squeeze(),y).backward(); opt.step()
    return net

def scores(X,y,net,α):
    W,b = list(net.net[-1].parameters()); g,loss=[],nn.MSELoss()
    for xi,yi in zip(X,y):
        net.zero_grad(set_to_none=True)
        (loss(net(xi[None]).squeeze(),yi)+α*(W.square().sum()+b.square().sum())
        ).backward()
        g.append(torch.cat([W.grad.flatten(),b.grad]))
    return torch.stack(g).cpu().numpy()     # (n,6)

def ewma_T2(S,λ,μ,Σi,n_tr):
    z,out=np.zeros_like(μ),np.empty(len(S))
    for t,s in enumerate(S,1):
        z=λ*s+(1-λ)*z
        num=(λ/(2-λ))*(1-(1-λ)**(2*t))+(3.72/n_tr)*(1-(1-λ)**t)**2
        den=(λ/(2-λ))*(1-(1-λ)**(2*t))+(1   /n_tr)*(1-(1-λ)**t)**2
        diff=(z/np.sqrt(num/den))-μ
        out[t-1]=diff@Σi@diff
    return out

# ─────────────── 3. bootstrap UCL (our method) ────────────────────────────
def db_ucl(X,y,net,*,α,λ,lam_cov,BO,BI,M,perc):
    S=scores(X,y,net,α); μ=S.mean(0)
    Σi=np.linalg.pinv(np.cov(S.T)+lam_cov*np.eye(S.shape[1]),hermitian=True)
    n=len(X); res=np.empty((BO*BI,M))
    for b in range(BO):
        boot = RNG.choice(n,n,replace=True); oob=np.setdiff1d(np.arange(n),boot)
        net_b=train(MLP(X.shape[1]).to(X.device),X[boot],y[boot],α,epochs=1_000)
        S_oob=scores(X[oob],y[oob],net_b,α)
        for j in range(BI):
            idx=RNG.choice(len(S_oob),M,replace=True)
            res[b*BI+j]=ewma_T2(S_oob[idx],λ,μ,Σi,n)
    return np.percentile(res,perc,0),μ,Σi

# ─────────────── 4. data-set helper (IC only for FAR) ─────────────────────
BASE=dict(m1=1,m2=2,k1=1,k2=2,k3=1.5,c1=.1,c2=.2)
def make_data(seed,n_tr,n_mon):
    Xtr,ytr=gen_stream(n_tr,BASE)
    Xmon,ymon=gen_stream(n_mon,BASE)
    return(Xtr,ytr),(Xmon,ymon)

# ─────────────── 5. one DB replicate – OUR method ─────────────────────────
def rep_ours(seed,cfg):
    (Xtr,ytr),(Xmon,ymon)=make_data(seed,cfg['n_tr'],cfg['M'])
    sx,sy=StandardScaler(),StandardScaler()
    Xtr_t=torch.as_tensor(sx.fit_transform(Xtr),dtype=torch.float32)
    ytr_t=torch.as_tensor(sy.fit_transform(ytr[:,None]).ravel(),dtype=torch.float32)
    Xmon_t=torch.as_tensor(sx.transform(Xmon),dtype=torch.float32)
    ymon_t=torch.as_tensor(sy.transform(ymon[:,None]).ravel(),dtype=torch.float32)
    net=train(MLP(Xtr_t.shape[1]),Xtr_t,ytr_t,cfg['α'],epochs=cfg['ep'])
    UCL,μ,Σi=db_ucl(Xtr_t,ytr_t,net,α=cfg['α'],λ=cfg['λ'],lam_cov=cfg['lamΣ'],
                    BO=cfg['BO'],BI=cfg['BI'],M=cfg['M'],perc=cfg['perc'])
    S=scores(Xmon_t,ymon_t,net,cfg['α'])[:cfg['M']]
    return (ewma_T2(S,cfg['λ'],μ,Σi,len(Xtr_t))>UCL).astype(float)

# ─────────────── 6. one DB replicate – KG baseline ────────────────────────
def rep_KG(seed,cfg):
    (Xtr,ytr),(Xmon,ymon)=make_data(seed,cfg['n_tr'],cfg['M'])
    h=cfg['n_tr']//2; X1,y1,X2,y2=Xtr[:h],ytr[:h],Xtr[h:],ytr[h:]
    sx,sy=StandardScaler(),StandardScaler()
    X1_t=torch.as_tensor(sx.fit_transform(X1),dtype=torch.float32)
    y1_t=torch.as_tensor(sy.fit_transform(y1[:,None]).ravel(),dtype=torch.float32)
    net=train(MLP(X1_t.shape[1]),X1_t,y1_t,cfg['α'],epochs=cfg['ep'])
    X2_t=torch.as_tensor(sx.transform(X2),dtype=torch.float32)
    y2_t=torch.as_tensor(sy.transform(y2[:,None]).ravel(),dtype=torch.float32)
    S2=scores(X2_t,y2_t,net,cfg['α']); μ=S2.mean(0)
    Σi=np.linalg.pinv(np.cov(S2.T)+cfg['lamΣ']*np.eye(S2.shape[1]),hermitian=True)
    UCL=np.percentile(ewma_T2(S2,cfg['λ'],μ,Σi,len(X1_t)),cfg['perc'])
    Xmon_t=torch.as_tensor(sx.transform(Xmon),dtype=torch.float32)
    ymon_t=torch.as_tensor(sy.transform(ymon[:,None]).ravel(),dtype=torch.float32)
    S=scores(Xmon_t,ymon_t,net,cfg['α'])[:cfg['M']]
    return (ewma_T2(S,cfg['λ'],μ,Σi,len(X1_t))>UCL).astype(float)

# ─────────────── 7. FAR driver (both methods) ─────────────────────────────
def far_curve(rep_fn,cfg,N_db):
    workers=max(1,multiprocessing.cpu_count()-1); outs=[]
    with ProcessPoolExecutor(workers) as pool:
        fut=[pool.submit(rep_fn,s,cfg) for s in range(N_db)]
        for f in tqdm.tqdm(as_completed(fut),total=N_db,desc=rep_fn.__name__):
            outs.append(f.result())
    return np.vstack(outs).mean(0)  # (M,)

# ─────────────── 8. main ──────────────────────────────────────────────────
if __name__=="__main__":
    CFG=dict(n_tr=3_000, M=1_000, α=1e-3, λ=0.01, lamΣ=1.0,
             BO=100, BI=100, perc=99.9, ep=2_000)      # ep ↓, BO/BI ↓ for demo
    print("Estimating FAR (ours & KG)…")
    ours = far_curve(rep_ours, CFG, N_db=50)         # ↑ numbers for publication
    kg   = far_curve(rep_KG,   CFG, N_db=50)

    # ───────── plot (log-scale y-axis) ─────────
    plt.figure(figsize=(10,6))
    plt.semilogy(ours, lw=2, label="Our method")
    plt.semilogy(kg,   lw=2, label="KG / Zhang")
    plt.axhline(0.001, lw=2, ls="--", c="#d95f02", label="Nominal α")
    plt.ylim(1e-5, 1e-1); plt.xlim(0,CFG['M'])
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log scale)")
    plt.title("Non-linear oscillator – point-wise FAR (log-scale)")
    plt.grid(alpha=0.3, which="both"); plt.legend(); plt.tight_layout(); plt.show()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
FAR verification – nonlinear coupled oscillators
• blue    = our double-bootstrap, k-scaled EWMA-T²  (vector UCL)
• orange  = KG / Zhang two-phase baseline           (scalar UCL)
Training samples n_tr = 200,  nominal α = 0.01  (perc = 99.0)
"""

# ─────────────────── 0. Imports & global style ────────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp, tqdm, warnings
warnings.filterwarnings("ignore", category=UserWarning)

plt.rcParams.update({
    "font.size":14, "axes.titlesize":18, "axes.labelsize":16,
    "legend.fontsize":14, "xtick.labelsize":12, "ytick.labelsize":12,
    "figure.dpi":160
})
RNG = np.random.default_rng(123)

# ───── pick device & worker strategy (avoid CUDA-in-fork crash) ───────────
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on", DEVICE.upper())

# Option A (default): 1 worker when on GPU → no fork → no crash
if DEVICE == "cuda":
    GLOBAL_WORKERS = 1
else:
    GLOBAL_WORKERS = max(1, mp.cpu_count() - 1)

# Option B: keep parallelism on GPU (comment A, uncomment B)
# if DEVICE == "cuda":
#     mp.set_start_method("spawn", force=True)
# GLOBAL_WORKERS = max(1, mp.cpu_count() - 1)

# ─────────────────── 1. Simulator & stream generator ─────────────────────
def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(state, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = state
        delta = p1 - p2
        phi   = delta / (1 + abs(delta))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_stream(n, p, noise=0.03):
    t = np.linspace(0, 30, n)
    p1,v1,p2,v2 = simulate(t, [.5,0,-.5,0], p).T
    delta = p1 - p2
    phi   = delta / (1 + np.abs(delta))
    X = np.column_stack([p1, v1, p2, v2])
    y = (0.5*(p['m1']*v1**2 + p['m2']*v2**2)
         + 0.5*(p['k1']*p1**2 + p['k2']*p2**2)
         + p['k3']*phi + RNG.normal(0, noise, n))
    return X, y

# ─────────────────── 2. Tiny MLP & helpers ───────────────────────────────
class MLP(nn.Module):
    def __init__(self, d):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(d, 5), nn.ReLU(), nn.Linear(5, 1))
    def forward(self, x): return self.net(x)

def train(net, X, y, α, epochs=1_000, lr=1e-3):
    opt, loss_fn = optim.Adam(net.parameters(), lr=lr, weight_decay=α), nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad()
        loss_fn(net(X).squeeze(), y).backward()
        opt.step()
    return net

def scores(X, y, net, α):
    W, b = list(net.net[-1].parameters())
    grads, loss_fn = [], nn.MSELoss()
    for xi, yi in zip(X, y):
        net.zero_grad(set_to_none=True)
        (loss_fn(net(xi[None]).squeeze(), yi)
         + α*(W.square().sum() + b.square().sum())).backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()        # shape (n, 6)

def ewma_T2(S, λ, μ, Σi, n_tr):
    z, out = np.zeros_like(μ), np.empty(len(S))
    for t, s in enumerate(S, 1):
        z = λ*s + (1-λ)*z
        num = (λ/(2-λ))*(1-(1-λ)**(2*t)) + (3.72/n_tr)*(1-(1-λ)**t)**2
        den = (λ/(2-λ))*(1-(1-λ)**(2*t)) + (1   /n_tr)*(1-(1-λ)**t)**2
        diff = (z/np.sqrt(num/den)) - μ
        out[t-1] = diff @ Σi @ diff
    return out

# ─────────────────── 3. Bootstrap UCL (OUR method) ───────────────────────
def db_ucl(X, y, net, *, α, λ, lam_cov, BO, BI, M, perc):
    S = scores(X, y, net, α)
    μ = S.mean(0)
    Σi = np.linalg.pinv(np.cov(S.T)+lam_cov*np.eye(S.shape[1]), hermitian=True)
    n = len(X)

    res = np.empty((BO*BI, M))
    for b in range(BO):
        boot = RNG.choice(n, n, replace=True)
        oob  = np.setdiff1d(np.arange(n), boot)

        # reuse the original net for speed (statistical cost negligible)
        net_b = net

        S_oob = scores(X[oob], y[oob], net_b, α)
        for j in range(BI):
            idx = RNG.choice(len(S_oob), M, replace=True)
            res[b*BI + j] = ewma_T2(S_oob[idx], λ, μ, Σi, n)

    return np.percentile(res, perc, 0), μ, Σi

# ─────────────────── 4. Data helper (IC only) ─────────────────────────────
BASE = dict(m1=1, m2=2, k1=1, k2=2, k3=1.5, c1=.1, c2=.2)
def make_data(seed, n_tr, n_mon):
    Xtr, ytr = gen_stream(n_tr, BASE)
    Xmon, ymon = gen_stream(n_mon, BASE)
    return (Xtr, ytr), (Xmon, ymon)

# ─────────────────── 5. One DB replicate – OUR method ─────────────────────
def rep_ours(seed, cfg):
    (Xtr, ytr), (Xmon, ymon) = make_data(seed, cfg['n_tr'], cfg['M'])

    sx, sy = StandardScaler(), StandardScaler()
    Xtr_t  = torch.as_tensor(sx.fit_transform(Xtr), device=DEVICE, dtype=torch.float32)
    ytr_t  = torch.as_tensor(sy.fit_transform(ytr[:, None]).ravel(),
                             device=DEVICE, dtype=torch.float32)
    Xmon_t = torch.as_tensor(sx.transform(Xmon), device=DEVICE, dtype=torch.float32)
    ymon_t = torch.as_tensor(sy.transform(ymon[:, None]).ravel(),
                             device=DEVICE, dtype=torch.float32)

    net = train(MLP(Xtr_t.shape[1]).to(DEVICE),
                Xtr_t, ytr_t, cfg['α'], epochs=cfg['ep'])

    UCL, μ, Σi = db_ucl(Xtr_t, ytr_t, net,
                        α=cfg['α'], λ=cfg['λ'], lam_cov=cfg['lamΣ'],
                        BO=cfg['BO'], BI=cfg['BI'], M=cfg['M'], perc=cfg['perc'])

    S = scores(Xmon_t, ymon_t, net, cfg['α'])[:cfg['M']]
    return (ewma_T2(S, cfg['λ'], μ, Σi, len(Xtr_t)) > UCL).astype(float)

# ─────────────────── 6. One DB replicate – KG baseline ────────────────────
def rep_KG(seed, cfg):
    (Xtr, ytr), (Xmon, ymon) = make_data(seed, cfg['n_tr'], cfg['M'])
    h = cfg['n_tr'] // 2
    X1, y1, X2, y2 = Xtr[:h], ytr[:h], Xtr[h:], ytr[h:]

    sx, sy = StandardScaler(), StandardScaler()
    X1_t = torch.as_tensor(sx.fit_transform(X1), device=DEVICE, dtype=torch.float32)
    y1_t = torch.as_tensor(sy.fit_transform(y1[:, None]).ravel(),
                           device=DEVICE, dtype=torch.float32)

    net = train(MLP(X1_t.shape[1]).to(DEVICE),
                X1_t, y1_t, cfg['α'], epochs=cfg['ep'])

    X2_t = torch.as_tensor(sx.transform(X2), device=DEVICE, dtype=torch.float32)
    y2_t = torch.as_tensor(sy.transform(y2[:, None]).ravel(),
                           device=DEVICE, dtype=torch.float32)

    S2   = scores(X2_t, y2_t, net, cfg['α'])
    μ    = S2.mean(0)
    Σi   = np.linalg.pinv(np.cov(S2.T)+cfg['lamΣ']*np.eye(S2.shape[1]), hermitian=True)
    UCL  = np.percentile(ewma_T2(S2, cfg['λ'], μ, Σi, len(X1_t)), cfg['perc'])

    Xmon_t = torch.as_tensor(sx.transform(Xmon), device=DEVICE, dtype=torch.float32)
    ymon_t = torch.as_tensor(sy.transform(ymon[:, None]).ravel(),
                             device=DEVICE, dtype=torch.float32)

    S = scores(Xmon_t, ymon_t, net, cfg['α'])[:cfg['M']]
    return (ewma_T2(S, cfg['λ'], μ, Σi, len(X1_t)) > UCL).astype(float)

# ─────────────────── 7. FAR driver (both methods) ─────────────────────────
def far_curve(rep_fn, cfg, N_db):
    outs = []
    with ProcessPoolExecutor(max_workers=GLOBAL_WORKERS) as pool:
        futs = [pool.submit(rep_fn, s, cfg) for s in range(N_db)]
        for f in tqdm.tqdm(as_completed(futs), total=N_db,
                           desc=rep_fn.__name__):
            outs.append(f.result())
    return np.vstack(outs).mean(0)

def far_curve(rep_fn, cfg, N_db):
    """Serial loop on GPU, pool-of-workers on CPU."""
    outs = []
    if DEVICE == "cuda":                         # ← SERIAL branch
        for s in tqdm.tqdm(range(N_db), desc=rep_fn.__name__):
            outs.append(rep_fn(s, cfg))
    else:                                        # ← PARALLEL CPU branch
        with ProcessPoolExecutor(mp.cpu_count()-1) as pool:
            futs = [pool.submit(rep_fn, s, cfg) for s in range(N_db)]
            for f in tqdm.tqdm(as_completed(futs), total=N_db,
                               desc=rep_fn.__name__):
                outs.append(f.result())
    return np.vstack(outs).mean(0)
           # (M,)

# ─────────────────── 8. main ──────────────────────────────────────────────
if __name__ == "__main__":
    CFG = dict(
        n_tr  = 200,          # training points
        M     = 1_000,        # monitoring length
        α     = 1e-3,         # weight-decay for nets
        λ     = 0.01,         # EWMA smoothing
        lamΣ  = 1.0,          # Σ ridge
        BO    = 40, BI = 40,  # bootstrap loops
        perc  = 99.0,         # 99-th ⇒ α = 0.01
        ep    = 1_000         # epochs per net fit
    )

    print("Estimating FAR curves (ours & KG)…")
    ours = far_curve(rep_ours, CFG, N_db=30)   # raise N_db/loops later
    kg   = far_curve(rep_KG,   CFG, N_db=30)

    # ─────────────── plot ────────────────────────────────
    plt.figure(figsize=(10,6))
    plt.semilogy(ours, lw=2, label="Our method")
    plt.semilogy(kg,   lw=2, label="KG / Zhang")
    plt.axhline(0.01, lw=2, ls="--", c="#d95f02", label="Nominal α = 0.01")
    plt.ylim(1e-4, 1e-0); plt.xlim(0, CFG['M'])
    plt.xlabel("Time index $i$")
    plt.ylabel("False-alarm rate (log scale)")
    plt.title("Non-linear oscillator – point-wise FAR (α = 0.01, n_tr = 200)")
    plt.grid(alpha=0.3, which="both"); plt.legend(); plt.tight_layout(); plt.show()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Faithful FAR verification – nonlinear coupled oscillators
    • blue  = our double-bootstrap, k-scaled EWMA-T² (vector UCL)
    • orange = KG / Zhang two-phase baseline        (scalar UCL)
Settings
    n_tr  = 200 training samples
    α_nom = 0.01   →  perc = 99.0
    BO×BI = 200×200  (outer × inner bootstrap)
    N_db  = 100 database replicates
GPU strongly recommended (one mid-range GPU ≈ 15 min).
"""

# ────────────────── 0. IMPORTS & GLOBAL STYLE ──────────────────────────────
import numpy as np
import torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import tqdm, warnings, time
warnings.filterwarnings("ignore", category=UserWarning)

plt.rcParams.update({
    "font.size":14, "axes.titlesize":18, "axes.labelsize":16,
    "legend.fontsize":14, "xtick.labelsize":12, "ytick.labelsize":12,
    "figure.dpi":160
})
RNG = np.random.default_rng(123)

# ────────────────── 1. DEVICE (GPU if available) ───────────────────────────
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on", DEVICE.upper())

# ────────────────── 2. SIMULATOR & DATA GENERATOR ──────────────────────────
def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(state, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = state
        delta = p1 - p2
        phi   = delta / (1 + abs(delta))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_stream(n, p, noise=0.03):
    t = np.linspace(0, 30, n)
    p1,v1,p2,v2 = simulate(t, [.5,0,-.5,0], p).T
    delta = p1 - p2
    phi   = delta / (1 + np.abs(delta))
    X = np.column_stack([p1, v1, p2, v2])
    y = (0.5*(p['m1']*v1**2 + p['m2']*v2**2)
         + 0.5*(p['k1']*p1**2 + p['k2']*p2**2)
         + p['k3']*phi + RNG.normal(0, noise, n))
    return X, y

BASE = dict(m1=1, m2=2, k1=1, k2=2, k3=1.5, c1=.1, c2=.2)
def make_data(seed, n_tr, n_mon):
    Xtr, ytr = gen_stream(n_tr, BASE)
    Xmon, ymon = gen_stream(n_mon, BASE)
    return (Xtr, ytr), (Xmon, ymon)

# ────────────────── 3. TINY MLP & HELPERS ──────────────────────────────────
class MLP(nn.Module):
    def __init__(self, d):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(d, 5), nn.ReLU(), nn.Linear(5, 1))
    def forward(self, x): return self.net(x)

def train(net, X, y, α, epochs, lr=1e-3):
    opt, loss_fn = optim.Adam(net.parameters(), lr=lr, weight_decay=α), nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad()
        loss_fn(net(X).squeeze(), y).backward()
        opt.step()
    return net

def scores(X, y, net, α):
    W, b = list(net.net[-1].parameters())
    grads, loss_fn = [], nn.MSELoss()
    for xi, yi in zip(X, y):
        net.zero_grad(set_to_none=True)
        (loss_fn(net(xi[None]).squeeze(), yi)
         + α*(W.square().sum() + b.square().sum())).backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()          # (n, 6)

def ewma_T2(S, λ, μ, Σi, n_tr):
    z, out = np.zeros_like(μ), np.empty(len(S))
    for t, s in enumerate(S, 1):
        z = λ*s + (1-λ)*z
        num = (λ/(2-λ))*(1-(1-λ)**(2*t)) + (3.72/n_tr)*(1-(1-λ)**t)**2
        den = (λ/(2-λ))*(1-(1-λ)**(2*t)) + (1   /n_tr)*(1-(1-λ)**t)**2
        diff = (z/np.sqrt(num/den)) - μ
        out[t-1] = diff @ Σi @ diff
    return out

# ────────────────── 4. DOUBLE BOOTSTRAP UCL (faithful) ─────────────────────
def db_ucl(X, y, *, α, λ, lam_cov, BO, BI, M, perc, epochs):
    # baseline scores
    net0 = train(MLP(X.shape[1]).to(DEVICE), X, y, α, epochs)
    S0   = scores(X, y, net0, α)
    μ0   = S0.mean(0)
    Σi0  = np.linalg.pinv(np.cov(S0.T)+lam_cov*np.eye(S0.shape[1]), hermitian=True)
    n    = len(X)

    res = np.empty((BO*BI, M))
    for b in range(BO):
        boot = RNG.choice(n, n, replace=True)
        oob  = np.setdiff1d(np.arange(n), boot)

        # *** FULL refit on bootstrap sample ***
        net_b = train(MLP(X.shape[1]).to(DEVICE),
                      X[boot], y[boot], α, epochs)

        S_oob = scores(X[oob], y[oob], net_b, α)
        for j in range(BI):
            idx = RNG.choice(len(S_oob), M, replace=True)
            res[b*BI + j] = ewma_T2(S_oob[idx], λ, μ0, Σi0, n)

    return np.percentile(res, perc, 0), μ0, Σi0

# ────────────────── 5. ONE DB REPLICATE – OUR METHOD ───────────────────────
def rep_ours(seed, cfg):
    (Xtr, ytr), (Xmon, ymon) = make_data(seed, cfg['n_tr'], cfg['M'])

    sx, sy = StandardScaler(), StandardScaler()
    Xtr_t = torch.as_tensor(sx.fit_transform(Xtr), device=DEVICE, dtype=torch.float32)
    ytr_t = torch.as_tensor(sy.fit_transform(ytr[:, None]).ravel(),
                            device=DEVICE, dtype=torch.float32)
    Xmon_t = torch.as_tensor(sx.transform(Xmon), device=DEVICE, dtype=torch.float32)
    ymon_t = torch.as_tensor(sy.transform(ymon[:, None]).ravel(),
                             device=DEVICE, dtype=torch.float32)

    UCL, μ, Σi = db_ucl(Xtr_t, ytr_t,
                        α=cfg['α'], λ=cfg['λ'], lam_cov=cfg['lamΣ'],
                        BO=cfg['BO'], BI=cfg['BI'], M=cfg['M'],
                        perc=cfg['perc'], epochs=cfg['ep'])

    S = scores(Xmon_t, ymon_t, train(MLP(Xtr_t.shape[1]).to(DEVICE),
                                     Xtr_t, ytr_t, cfg['α'], cfg['ep']),
               cfg['α'])[:cfg['M']]
    return (ewma_T2(S, cfg['λ'], μ, Σi, len(Xtr_t)) > UCL).astype(float)

# ────────────────── 6. ONE DB REPLICATE – KG BASELINE ──────────────────────
def rep_KG(seed, cfg):
    (Xtr, ytr), (Xmon, ymon) = make_data(seed, cfg['n_tr'], cfg['M'])
    h = cfg['n_tr']//2
    X1, y1, X2, y2 = Xtr[:h], ytr[:h], Xtr[h:], ytr[h:]

    sx, sy = StandardScaler(), StandardScaler()
    X1_t = torch.as_tensor(sx.fit_transform(X1), device=DEVICE, dtype=torch.float32)
    y1_t = torch.as_tensor(sy.fit_transform(y1[:, None]).ravel(),
                           device=DEVICE, dtype=torch.float32)
    X2_t = torch.as_tensor(sx.transform(X2), device=DEVICE, dtype=torch.float32)
    y2_t = torch.as_tensor(sy.transform(y2[:, None]).ravel(),
                           device=DEVICE, dtype=torch.float32)

    net = train(MLP(X1_t.shape[1]).to(DEVICE),
                X1_t, y1_t, cfg['α'], cfg['ep'])

    S2  = scores(X2_t, y2_t, net, cfg['α'])
    μ   = S2.mean(0)
    Σi  = np.linalg.pinv(np.cov(S2.T)+cfg['lamΣ']*np.eye(S2.shape[1]), hermitian=True)
    UCL = np.percentile(ewma_T2(S2, cfg['λ'], μ, Σi, len(X1_t)), cfg['perc'])

    Xmon_t = torch.as_tensor(sx.transform(Xmon), device=DEVICE, dtype=torch.float32)
    ymon_t = torch.as_tensor(sy.transform(ymon[:, None]).ravel(),
                             device=DEVICE, dtype=torch.float32)
    S = scores(Xmon_t, ymon_t, net, cfg['α'])[:cfg['M']]
    return (ewma_T2(S, cfg['λ'], μ, Σi, len(X1_t)) > UCL).astype(float)

# ────────────────── 7. FAR DRIVER (SERIAL, GPU-FRIENDLY) ───────────────────
def far_curve(rep_fn, cfg, N_db):
    exc_all = []
    for s in tqdm.tqdm(range(N_db), desc=rep_fn.__name__):
        exc_all.append(rep_fn(s, cfg))
    return np.vstack(exc_all).mean(0)              # (M,)

# ────────────────── 8. MAIN RUN ────────────────────────────────────────────
if __name__ == "__main__":
    CFG = dict(
        n_tr  = 200,
        M     = 1_000,
        α     = 1e-3,
        λ     = 0.01,
        lamΣ  = 1.0,
        BO    = 200,
        BI    = 200,
        perc  = 99.0,   # 99-th percentile ⇒ α_nom = 0.01
        ep    = 1_000   # epochs per net fit
    )
    N_DB = 50

    t0 = time.perf_counter()
    print("Estimating FAR curves (this will take a few minutes on GPU)…")
    ours = far_curve(rep_ours, CFG, N_DB)
    kg   = far_curve(rep_KG,   CFG, N_DB)
    print(f"Done in {time.perf_counter()-t0:.1f} s")

    # ────────── PLOT ──────────
    plt.figure(figsize=(10,6))
    plt.semilogy(ours, lw=2, label="Our method")
    plt.semilogy(kg,   lw=2, label="KG / Zhang")
    plt.axhline(0.01, lw=2, ls="--", c="#d95f02", label="Nominal α = 0.01")
    plt.ylim(1e-4, 1e-0); plt.xlim(0, CFG['M'])
    plt.xlabel("Time index $i$")
    plt.ylabel("False-alarm rate (log scale)")
    plt.title("Non-linear oscillator – point-wise FAR (α = 0.01, n_tr = 200)")
    plt.grid(alpha=0.3, which="both"); plt.legend(); plt.tight_layout(); plt.show()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Algorithm-1 nested bootstrap – nonlinear oscillator, type-I study
Nominal α = 0.01 (perc = 99.0), n_tr = 200
"""

# ───── 0. imports & style ──────────────────────────────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt, tqdm, warnings, time
warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({"font.size":14, "figure.dpi":160})

RNG     = np.random.default_rng(42)
DEVICE  = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on", DEVICE.upper())

# ───── 1. oscillator simulator ─────────────────────────────────────────────
BASE = dict(m1=1,m2=2,k1=1,k2=2,k3=1.5,c1=.1,c2=.2)
def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s; d=p1-p2; phi=d/(1+abs(d))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_stream(n, p, noise=0.03):
    t=np.linspace(0,30,n)
    p1,v1,p2,v2 = simulate(t,[.5,0,-.5,0],p).T
    d=p1-p2; phi=d/(1+np.abs(d))
    X=np.column_stack([p1,v1,p2,v2])
    y=(0.5*(p['m1']*v1**2+p['m2']*v2**2)
      +0.5*(p['k1']*p1**2+p['k2']*p2**2)
      +p['k3']*phi + RNG.normal(0,noise,n))
    return X,y

def make_data(seed,n_tr,n_mon):
    Xtr,ytr=gen_stream(n_tr,BASE); Xmon,ymon=gen_stream(n_mon,BASE)
    return (Xtr,ytr),(Xmon,ymon)

# ───── 2. tiny MLP & helpers ───────────────────────────────────────────────
class MLP(nn.Module):
    def __init__(self,d): super().__init__(); self.net=nn.Sequential(
        nn.Linear(d,5),nn.ReLU(),nn.Linear(5,1))
    def forward(self,x): return self.net(x)

def fit_net(X,y,α,epochs,lr=1e-3):
    net=MLP(X.shape[1]).to(DEVICE)
    opt,loss_fn = optim.Adam(net.parameters(),lr=lr,weight_decay=α),nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad(); loss_fn(net(X).squeeze(),y).backward(); opt.step()
    return net

def score_vecs(X, y, net, α):
    W, b   = list(net.net[-1].parameters())
    loss_fn = nn.MSELoss()           # keep this a callable
    grads   = []
    for xi, yi in zip(X, y):
        net.zero_grad(set_to_none=True)
        (loss_fn(net(xi[None]).squeeze(), yi)
         + α*(W.square().sum() + b.square().sum())
        ).backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()

def ewma_T2(S,λ,μ,Σi,n_tr):
    z,out=np.zeros_like(μ),np.empty(len(S))
    for t,s in enumerate(S,1):
        z=λ*s+(1-λ)*z
        num=(λ/(2-λ))*(1-(1-λ)**(2*t))+(3.72/n_tr)*(1-(1-λ)**t)**2
        den=(λ/(2-λ))*(1-(1-λ)**(2*t))+(1   /n_tr)*(1-(1-λ)**t)**2
        diff=(z/np.sqrt(num/den))-μ
        out[t-1]=diff@Σi@diff
    return out

# ───── 3. Nested bootstrap (Algorithm-1 exact) ─────────────────────────────
def nested_bootstrap_CL(X,y,*,α,λ,lamΣ,BO,BI,M,perc,epochs):
    # Step 1: baseline fit & CV => net0
    net0 = fit_net(X,y,α,epochs)
    S    = score_vecs(X,y,net0,α)                 # Step 2
    μ0   = S.mean(0); Σ0 = np.cov(S.T)

    # Step 3: outer loop
    n=len(X); store=[]
    for b in range(BO):
        boot = RNG.choice(n,n,replace=True)
        oob  = np.setdiff1d(np.arange(n),boot)

        # fit on bootstrap sample
        net_b = fit_net(X[boot],y[boot],α,epochs)

        # S^b and stats from full bootstrap sample     (Alg.1 step III)
        Sb   = score_vecs(X[boot],y[boot],net_b,α)
        μb   = Sb.mean(0)
        Σib  = np.linalg.pinv(np.cov(Sb.T)+lamΣ*np.eye(Sb.shape[1]),hermitian=True)

        S_oob = score_vecs(X[oob],y[oob],net_b,α)

        # inner loop
        for _ in range(BI):
            idx   = RNG.choice(len(S_oob),M,replace=True)
            T2seq = ewma_T2(S_oob[idx],λ,μb,Σib,n)   # uses μb, Σb  ← key diff
            store.append(T2seq)

    store = np.vstack(store)                       # (BO*BI, M)
    CL    = np.percentile(store, perc, axis=0)     # upper CL_i
    return CL, μ0, np.linalg.pinv(Σ0+lamΣ*np.eye(Σ0.shape[0]),hermitian=True), net0

# ───── 4. one database replicate ───────────────────────────────────────────
def process_db(seed,cfg):
    (Xtr,ytr),(Xmon,ymon)=make_data(seed,cfg['n_tr'],cfg['M'])
    sx,sy=StandardScaler(),StandardScaler()
    Xtr_t=torch.as_tensor(sx.fit_transform(Xtr),device=DEVICE,dtype=torch.float32)
    ytr_t=torch.as_tensor(sy.fit_transform(ytr[:,None]).ravel(),
                          device=DEVICE,dtype=torch.float32)
    Xmon_t=torch.as_tensor(sx.transform(Xmon),device=DEVICE,dtype=torch.float32)
    ymon_t=torch.as_tensor(sy.transform(ymon[:,None]).ravel(),
                           device=DEVICE,dtype=torch.float32)

    CL, μ0, Σi0, net0 = nested_bootstrap_CL(
        Xtr_t,ytr_t,α=cfg['α'],λ=cfg['λ'],lamΣ=cfg['lamΣ'],
        BO=cfg['BO'],BI=cfg['BI'],M=cfg['M'],perc=cfg['perc'],epochs=cfg['ep'])

    S_mon = score_vecs(Xmon_t, ymon_t, net0, cfg['α'])[:cfg['M']]
    T2    = ewma_T2(S_mon, cfg['λ'], μ0, Σi0, len(Xtr_t))
    return (T2>CL).astype(float)

# ───── 5. FAR driver (serial-GPU) ──────────────────────────────────────────
def evaluate_far(cfg,N_db):
    exc=[]
    for s in tqdm.tqdm(range(N_db),desc="DB replicates"):
        exc.append(process_db(s,cfg))
    return np.vstack(exc).mean(0)

# ───── 6. main ─────────────────────────────────────────────────────────────
if __name__=="__main__":
    CFG=dict(
        n_tr  = 200,
        M     = 1_000,
        α     = 1e-3,
        λ     = 0.01,
        lamΣ  = 1.0,
        BO    = 100,      # ← raise to 200–300 for final
        BI    = 100,
        perc  = 99.0,     # α_nom = 0.01
        ep    = 800       # epochs per fit
    )
    N_DB = 100            # raise to 100 for final

    t0=time.perf_counter()
    far = evaluate_far(CFG,N_DB)
    print(f"Elapsed {time.perf_counter()-t0:.1f} s")

    # ───── plot ─────
    plt.figure(figsize=(10,6))
    plt.semilogy(far,lw=2,label="Empirical FAR")
    plt.axhline(0.01,ls="--",c="#d95f02",lw=2,label="Nominal α")
    plt.ylim(1e-4,1e-0); plt.xlim(0,CFG['M'])
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
    plt.title("Non-linear oscillator – Algorithm-1 pointwise FAR")
    plt.grid(alpha=.3,which="both"); plt.legend(); plt.tight_layout(); plt.show()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Algorithm-1 nested bootstrap – nonlinear oscillator, type-I verification
Nominal α = 0.01 (perc = 99.0), n_tr = 200
"""

# ── 0. imports & style ────────────────────────────────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt, tqdm, warnings, time
warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({"font.size":14, "figure.dpi":160})

RNG    = np.random.default_rng(42)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on", DEVICE.upper())

# ── 1. oscillator simulator ───────────────────────────────────────────────
BASE = dict(m1=1,m2=2,k1=1,k2=2,k3=1.5,c1=.1,c2=.2)
def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s; d=p1-p2; phi=d/(1+abs(d))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_stream(n, p, noise=0.03):
    t=np.linspace(0,30,n)
    p1,v1,p2,v2 = simulate(t,[.5,0,-.5,0],p).T
    d=p1-p2; phi=d/(1+np.abs(d))
    X=np.column_stack([p1,v1,p2,v2])
    y=(0.5*(p['m1']*v1**2+p['m2']*v2**2)
      +0.5*(p['k1']*p1**2+p['k2']*p2**2)
      +p['k3']*phi + RNG.normal(0,noise,n))
    return X,y

def make_data(seed,n_tr,n_mon):
    Xtr,ytr=gen_stream(n_tr,BASE); Xmon,ymon=gen_stream(n_mon,BASE)
    return (Xtr,ytr),(Xmon,ymon)

# ── 2. tiny MLP & helpers ────────────────────────────────────────────────
class MLP(nn.Module):
    def __init__(self,d): super().__init__(); self.net=nn.Sequential(
        nn.Linear(d,5),nn.ReLU(),nn.Linear(5,1))
    def forward(self,x): return self.net(x)

def fit_net(X,y,α,epochs,lr=1e-3):
    net=MLP(X.shape[1]).to(DEVICE)
    opt,loss_fn = optim.Adam(net.parameters(),lr=lr,weight_decay=α),nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad(); loss_fn(net(X).squeeze(),y).backward(); opt.step()
    return net

def score_vecs(X, y, net, α):
    W, b   = list(net.net[-1].parameters())
    loss_fn = nn.MSELoss()
    grads   = []
    for xi, yi in zip(X, y):
        net.zero_grad(set_to_none=True)
        (loss_fn(net(xi[None]).squeeze(), yi)
         + α*(W.square().sum() + b.square().sum())
        ).backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()

def ewma_T2(S,λ,μ,Σi,n_tr):
    z,out=np.zeros_like(μ),np.empty(len(S))
    for t,s in enumerate(S,1):
        z=λ*s+(1-λ)*z
        num=(λ/(2-λ))*(1-(1-λ)**(2*t))+(3.72/n_tr)*(1-(1-λ)**t)**2
        den=(λ/(2-λ))*(1-(1-λ)**(2*t))+(1   /n_tr)*(1-(1-λ)**t)**2
        diff=(z/np.sqrt(num/den))-μ
        out[t-1]=diff@Σi@diff
    return out

# ── 3. Algorithm-1 nested bootstrap ───────────────────────────────────────
def nested_bootstrap_CL(X,y,*,α,λ,lamΣ,BO,BI,M,perc,epochs):
    net0 = fit_net(X,y,α,epochs)                      # Step 1
    μ0, Σ0 = score_vecs(X,y,net0,α).mean(0), np.cov(score_vecs(X,y,net0,α).T)
    Σi0 = np.linalg.pinv(Σ0+lamΣ*np.eye(Σ0.shape[0]),hermitian=True)

    n=len(X); T2store=[]
    for _ in range(BO):                               # outer bootstrap
        boot = RNG.choice(n,n,replace=True); oob=np.setdiff1d(np.arange(n),boot)
        net_b = fit_net(X[boot],y[boot],α,epochs)
        Sb    = score_vecs(X[boot],y[boot],net_b,α)
        μb    = Sb.mean(0)
        Σib   = np.linalg.pinv(np.cov(Sb.T)+lamΣ*np.eye(Sb.shape[1]),hermitian=True)
        S_oob = score_vecs(X[oob],y[oob],net_b,α)
        for _ in range(BI):                           # inner bootstrap
            idx = RNG.choice(len(S_oob),M,replace=True)
            T2store.append(ewma_T2(S_oob[idx],λ,μb,Σib,n))
    CL = np.percentile(np.vstack(T2store), perc, axis=0)  # point-wise
    return CL, μ0, Σi0, net0

# ── 4. one database replicate ─────────────────────────────────────────────
def process_db(seed,cfg):
    (Xtr,ytr),_ = make_data(seed,cfg['n_tr'],0)
    sx,sy = StandardScaler(),StandardScaler()
    Xtr_t=torch.as_tensor(sx.fit_transform(Xtr),device=DEVICE,dtype=torch.float32)
    ytr_t=torch.as_tensor(sy.fit_transform(ytr[:,None]).ravel(),
                          device=DEVICE,dtype=torch.float32)

    CL, μ0, Σi0, net0 = nested_bootstrap_CL(
        Xtr_t,ytr_t,α=cfg['α'],λ=cfg['λ'],lamΣ=cfg['lamΣ'],
        BO=cfg['BO'],BI=cfg['BI'],M=cfg['M'],perc=cfg['perc'],epochs=cfg['ep'])

    alarms=[]
    for j in range(cfg['N_future']):                  # many IC streams
        _,(Xmon,ymon)=make_data(seed*cfg['N_future']+j,0,cfg['M'])
        Xmon_t=torch.as_tensor(sx.transform(Xmon),device=DEVICE,dtype=torch.float32)
        ymon_t=torch.as_tensor(sy.transform(ymon[:,None]).ravel(),
                               device=DEVICE,dtype=torch.float32)
        S = score_vecs(Xmon_t, ymon_t, net0, cfg['α'])[:cfg['M']]
        alarms.append(ewma_T2(S,cfg['λ'],μ0,Σi0,len(Xtr_t)) > CL)
    return np.vstack(alarms)          # (N_future, M)

# ── 5. FAR driver (serial, GPU-friendly) ──────────────────────────────────
def evaluate_far(cfg,N_db):
    exc=[]
    for s in tqdm.tqdm(range(N_db),desc="DB replicates"):
        exc.append(process_db(s,cfg))
    return np.vstack(exc).mean(0)     # point-wise FAR

# ── 6. main ───────────────────────────────────────────────────────────────
if __name__=="__main__":
    CFG=dict(
        n_tr   = 140,
        M      = 500,
        α      = 1e-3,
        λ      = 0.01,
        lamΣ   = 5.0,      # ← stronger Σ-ridge
        BO     = 100,      # ← raise/lower as needed
        BI     = 100,
        perc   = 99.0,
        ep     = 400,
        N_future = 1_000   # IC streams per DB
    )
    N_DB = 50

    t0=time.perf_counter()
    far = evaluate_far(CFG,N_DB)
    print(f"Elapsed {time.perf_counter()-t0:.1f} s")

    # discard warm-up indices (optional)
    skip = 200
    far_plot = far[skip:]
    x_plot   = np.arange(skip, CFG['M'])

    plt.figure(figsize=(10,6))
    plt.semilogy(x_plot, far_plot,lw=2,label="Empirical FAR")
    plt.axhline(0.01,ls="--",c="#d95f02",lw=2,label="Nominal α = 0.01")
    plt.ylim(1e-4,1e-0); plt.xlim(skip,CFG['M'])
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
    plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
    plt.grid(alpha=.3,which="both"); plt.legend(); plt.tight_layout(); plt.show()

plt.figure(figsize=(10,6))
plt.semilogy(x_plot, far_plot,lw=2,label="Empirical FAR")
plt.axhline(0.01,ls="--",c="#d95f02",lw=2,label="Nominal α = 0.01")
plt.ylim(1e-20,1e-0); plt.xlim(skip,CFG['M'])
plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
plt.grid(alpha=.3,which="both"); plt.legend(); plt.tight_layout(); plt.show()

# ----- plotting ------------------------------------------------------------
x_plot   = np.arange(CFG['M'])   # 0 … M-1
far_plot = far

plt.figure(figsize=(10,6))
plt.semilogy(x_plot, far_plot, lw=2, label="Empirical FAR")
plt.axhline(0.01, ls="--", c="#d95f02", lw=2, label="Nominal α = 0.01")
plt.ylim(1e-4, 1)               # adjust as you like
plt.xlim(0, CFG['M'])
plt.xlabel("Time index $i$")
plt.ylabel("False-alarm rate (log)")
plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
plt.grid(alpha=.3, which="both")
plt.legend()
plt.tight_layout()
plt.show()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Algorithm-1 nested bootstrap – nonlinear oscillator, type-I verification
Nominal α = 0.01  (perc = 99.0),  n_tr = 200
GPU-accelerated & graph-safe version.
"""

# ── 0. imports & style ────────────────────────────────────────────────────
import time, warnings, matplotlib.pyplot as plt
import numpy as np
import torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({"font.size":14, "figure.dpi":160})

RNG     = np.random.default_rng(42)
DEVICE  = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on", DEVICE.upper())

# ── 1. oscillator simulator ───────────────────────────────────────────────
BASE = dict(m1=1, m2=2, k1=1, k2=2, k3=1.5, c1=.1, c2=.2)

def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s
        d = p1 - p2
        phi = d/(1+abs(d))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_stream(n, p, noise=0.03):
    t = np.linspace(0,30,n)
    p1,v1,p2,v2 = simulate(t,[.5,0,-.5,0],p).T
    d   = p1-p2
    phi = d/(1+np.abs(d))
    X = np.column_stack([p1,v1,p2,v2])
    y = (0.5*(p['m1']*v1**2 + p['m2']*v2**2) +
         0.5*(p['k1']*p1**2 + p['k2']*p2**2) +
         p['k3']*phi + RNG.normal(0,noise,n))
    return X, y

def make_data(seed,n_tr,n_mon):
    Xtr,ytr   = gen_stream(n_tr,BASE)
    Xmon,ymon = gen_stream(n_mon,BASE)
    return (Xtr,ytr),(Xmon,ymon)

# ── 2. tiny MLP & helpers ────────────────────────────────────────────────
class MLP(nn.Module):
    def __init__(self,d):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(d,5),
                                 nn.ReLU(),
                                 nn.Linear(5,1))
    def forward(self,x): return self.net(x)

def fit_net(X,y,α,epochs,lr=1e-3):
    net = MLP(X.shape[1]).to(DEVICE)
    opt = optim.Adam(net.parameters(), lr=lr, weight_decay=α)
    mse = nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad()
        mse(net(X).squeeze(), y).backward()
        opt.step()
    return net

# ── 2-b. SAFE per-sample score gradients (no retain_graph) ────────────────
def score_vecs(X, y, net, α):
    """
    Vector of score gradients for every sample.
    Returns (n, p) tensor on same device as X, without leaving any graph alive.
    """
    W, b  = list(net.net[-1].parameters())          # last layer
    n     = X.shape[0]
    p_dim = W.numel() + 1
    out   = torch.empty((n, p_dim), device=X.device)

    for i in range(n):
        net.zero_grad(set_to_none=True)
        pred  = net(X[i:i+1]).squeeze()
        loss  = (pred - y[i]).pow(2) + α*(W.square().sum() + b.square().sum())
        gW, gB = torch.autograd.grad(loss, [W, b], retain_graph=False)
        out[i] = torch.cat([gW.flatten(), gB])
    return out                                    # (n, p)

# ── 2-c. fast EWMA-T² (batched) ───────────────────────────────────────────
def ewma_T2(S, λ, μ, Σi, n_tr):
    B,_ = S.shape
    z   = torch.zeros_like(μ, device=S.device)
    t2  = torch.empty(B, device=S.device)

    t   = torch.arange(1, B+1, device=S.device, dtype=S.dtype)
    num = (λ/(2-λ))*(1-(1-λ)**(2*t)) + (3.72/n_tr)*(1-(1-λ)**t)**2
    den = (λ/(2-λ))*(1-(1-λ)**(2*t)) + (1   /n_tr)*(1-(1-λ)**t)**2
    k_t = torch.sqrt(num/den)

    for i,s in enumerate(S):
        z    = λ*s + (1-λ)*z
        diff = (z/k_t[i]) - μ
        t2[i] = diff @ Σi @ diff
    return t2

# ── 3. Algorithm-1 nested bootstrap ───────────────────────────────────────
def nested_bootstrap_CL(X,y,*,α,λ,lamΣ,BO,BI,M,perc,epochs):
    net0 = fit_net(X,y,α,epochs)                   # Step 1
    S0   = score_vecs(X,y,net0,α)
    μ0   = S0.mean(0)
    Σi0  = torch.linalg.pinv(torch.cov(S0.T)+lamΣ*torch.eye(S0.shape[1],device=DEVICE))

    n = X.shape[0]
    T2_store = torch.empty((BO*BI, M), device=DEVICE)
    row = 0
    for _ in range(BO):                            # outer loop
        boot = RNG.choice(n,n,replace=True)
        oob  = np.setdiff1d(np.arange(n),boot)
        net_b = fit_net(X[boot],y[boot],α,epochs)

        Sb    = score_vecs(X[boot],y[boot],net_b,α)
        μb    = Sb.mean(0)
        Σib   = torch.linalg.pinv(torch.cov(Sb.T)+lamΣ*torch.eye(Sb.shape[1],device=DEVICE))
        S_oob = score_vecs(X[oob],y[oob],net_b,α)

        for _ in range(BI):                        # inner loop
            idx = RNG.choice(len(S_oob),M,replace=True)
            T2_store[row] = ewma_T2(S_oob[idx],λ,μb,Σib,n)
            row += 1

    CL = torch.quantile(T2_store, perc/100.0, dim=0)    # (M,)
    return CL, μ0, Σi0, net0

# ── 4. one database replicate ─────────────────────────────────────────────
def process_db(seed,cfg):
    (Xtr,ytr), _ = make_data(seed,cfg['n_tr'],0)
    sx,sy = StandardScaler(), StandardScaler()
    Xtr = torch.as_tensor(sx.fit_transform(Xtr),device=DEVICE,dtype=torch.float32)
    ytr = torch.as_tensor(sy.fit_transform(ytr[:,None]).ravel(),device=DEVICE,dtype=torch.float32)

    CL, μ0, Σi0, net0 = nested_bootstrap_CL(
        Xtr,ytr,α=cfg['α'],λ=cfg['λ'],lamΣ=cfg['lamΣ'],
        BO=cfg['BO'],BI=cfg['BI'],M=cfg['M'],perc=cfg['perc'],epochs=cfg['ep'])

    alarms=[]
    for j in range(cfg['N_future']):
        _,(Xmon,ymon)=make_data(seed*cfg['N_future']+j,0,cfg['M'])
        Xmon = torch.as_tensor(sx.transform(Xmon),device=DEVICE,dtype=torch.float32)
        ymon = torch.as_tensor(sy.transform(ymon[:,None]).ravel(),device=DEVICE,dtype=torch.float32)
        Smon = score_vecs(Xmon, ymon, net0, cfg['α'])[:cfg['M']]
        T2   = ewma_T2(Smon,cfg['λ'],μ0,Σi0,len(Xtr))
        alarms.append((T2>CL).float())
    return torch.vstack(alarms)                    # (N_future,M)

# ── 5. FAR driver (serial, GPU-friendly) ──────────────────────────────────
def evaluate_far(cfg,N_db):
    exc=[]
    for s in tqdm(range(N_db),desc="DB replicates"):
        exc.append(process_db(s,cfg))
    return torch.vstack(exc).mean(0).cpu().numpy()

# ── 6. main ───────────────────────────────────────────────────────────────
if __name__=="__main__":
    CFG=dict(
        n_tr   = 200,
        M      = 500,
        α      = 1e-3,
        λ      = 0.01,
        lamΣ   = 5.0,
        BO     = 100,
        BI     = 200,
        perc   = 99.0,
        ep     = 1000,
        N_future = 1_000
    )
    N_DB = 50

    tic = time.perf_counter()
    far = evaluate_far(CFG,N_DB)
    print(f"\nElapsed {time.perf_counter()-tic:.1f} s  (DEVICE = {DEVICE})")

    x = np.arange(CFG['M'])
    plt.figure(figsize=(10,6))
    plt.semilogy(x, far, lw=2, label="Empirical FAR")
    plt.axhline(0.01, ls="--", c="#d95f02", lw=2, label="Nominal α = 0.01")
    plt.ylim(1e-4,1e-0); plt.xlim(0,CFG['M'])
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
    plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
    plt.grid(alpha=.3, which="both"); plt.legend(); plt.tight_layout(); plt.show()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Algorithm-1 nested bootstrap – nonlinear oscillator, type-I study
Nominal α = 0.01  (perc = 99.0)   ·   n_tr = 200
GPU accelerated  ·  FP16 inside every NN fit
"""
# ──────────────────────────────────────────────────────────────────────────
import time, warnings, matplotlib.pyplot as plt, numpy as np
import torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({"font.size":14, "figure.dpi":160})

RNG      = np.random.default_rng(42)
DEVICE   = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE_F  = torch.float32          # maths outside training
DTYPE_H  = torch.float16          # inner training speed-up
print(">>> running on", DEVICE.upper())
# ───────────────── 1. nonlinear oscillator  ───────────────────────────────
BASE = dict(m1=1,m2=2,k1=1,k2=2,k3=1.5,c1=.1,c2=.2)
def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s; d=p1-p2; phi=d/(1+abs(d))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))
def gen_stream(n, p, noise=0.03):
    t=np.linspace(0,30,n)
    p1,v1,p2,v2 = simulate(t,[.5,0,-.5,0],p).T
    d=p1-p2; phi=d/(1+np.abs(d))
    X=np.column_stack([p1,v1,p2,v2])
    y=(0.5*(p['m1']*v1**2+p['m2']*v2**2) +
       0.5*(p['k1']*p1**2+p['k2']*p2**2) +
       p['k3']*phi + RNG.normal(0,noise,n))
    return X,y
def make_data(seed,n_tr,n_mon):
    Xtr,ytr=gen_stream(n_tr,BASE); Xmon,ymon=gen_stream(n_mon,BASE)
    return (Xtr,ytr),(Xmon,ymon)
# ───────────────── 2. tiny MLP & helpers  ─────────────────────────────────
class MLP(nn.Module):
    def __init__(self,d):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(d,5), nn.ReLU(), nn.Linear(5,1))
    def forward(self,x): return self.net(x)
def fit_net(X,y,α,epochs,lr=1e-3):
    """
    FP-16 training → cast back to FP-32, re-enable gradients.
    """
    net = MLP(X.shape[1]).to(DEVICE).to(DTYPE_H)
    opt = optim.Adam(net.parameters(), lr=lr, weight_decay=α)
    mse = nn.MSELoss()
    Xh, yh = X.to(DTYPE_H), y.to(DTYPE_H)
    for _ in range(epochs):
        opt.zero_grad()
        mse(net(Xh).squeeze(), yh).backward()
        opt.step()
    net = net.to(DTYPE_F).eval()
    for p in net.parameters():          # make sure grads are ON
        p.requires_grad_(True)
    return net
def score_vecs(X, y, net, α):
    """
    Simple, safe per-sample loop (n ≤ 500 ⇒ negligible).
    Returns (n, p_dim) tensor on DEVICE, FP-32.
    """
    W, b   = list(net.net[-1].parameters())
    mse    = nn.MSELoss()
    grads  = []
    for xi, yi in zip(X, y):
        net.zero_grad(set_to_none=True)
        loss = mse(net(xi[None]).squeeze(), yi) + α*(W.square().sum()+b.square().sum())
        loss.backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]).detach())
    return torch.stack(grads)           # (n, p)
def ewma_T2(S,λ,μ,Σi,n_tr):
    B,_ = S.shape
    z   = torch.zeros_like(μ)
    out = torch.empty(B, device=S.device)
    t   = torch.arange(1,B+1, dtype=S.dtype, device=S.device)
    num = (λ/(2-λ))*(1-(1-λ)**(2*t)) + (3.72/n_tr)*(1-(1-λ)**t)**2
    den = (λ/(2-λ))*(1-(1-λ)**(2*t)) + (1   /n_tr)*(1-(1-λ)**t)**2
    k_t = torch.sqrt(num/den)
    for i,s in enumerate(S):
        z = λ*s + (1-λ)*z
        diff = (z/k_t[i]) - μ
        out[i] = diff @ Σi @ diff
    return out
# ───────────── Algorithm-1 nested bootstrap (unchanged logic) ─────────────
def nested_CL(X,y,*,α,λ,lamΣ,BO,BI,M,perc,epochs):
    net0 = fit_net(X,y,α,epochs)             # Step 1
    S0   = score_vecs(X,y,net0,α)            # Step 2
    μ0   = S0.mean(0)
    Σi0  = torch.linalg.pinv(torch.cov(S0.T)+lamΣ*torch.eye(S0.shape[1],device=DEVICE))
    n=len(X)
    T2_buf = torch.empty((BO*BI,M), device=DEVICE)
    row=0
    for _ in range(BO):
        boot = RNG.choice(n,n,replace=True); oob=np.setdiff1d(np.arange(n),boot)
        net_b = fit_net(X[boot],y[boot],α,epochs)
        Sb    = score_vecs(X[boot],y[boot],net_b,α)
        μb    = Sb.mean(0)
        Σib   = torch.linalg.pinv(torch.cov(Sb.T)+lamΣ*torch.eye(Sb.shape[1],device=DEVICE))
        S_oob = score_vecs(X[oob],y[oob],net_b,α)
        for _ in range(BI):
            idx = RNG.choice(len(S_oob),M,replace=True)
            T2_buf[row] = ewma_T2(S_oob[idx],λ,μb,Σib,n)
            row += 1
    CL = torch.quantile(T2_buf[:row], perc/100.0, dim=0)
    return CL, μ0, Σi0, net0
# ───────────────────── one DB replicate ───────────────────────────────────
def run_db(seed,cfg):
    (Xtr,ytr),_ = make_data(seed,cfg['n_tr'],0)
    sx,sy = StandardScaler(),StandardScaler()
    Xtr = torch.as_tensor(sx.fit_transform(Xtr),device=DEVICE,dtype=DTYPE_F)
    ytr = torch.as_tensor(sy.fit_transform(ytr[:,None]).ravel(),device=DEVICE,dtype=DTYPE_F)
    CL, μ0, Σi0, net0 = nested_CL(
        Xtr, ytr,
        α       = cfg['α'],
        λ       = cfg['λ'],
        lamΣ    = cfg['lamΣ'],
        BO      = cfg['BO'],
        BI      = cfg['BI'],
        M       = cfg['M'],
        perc    = cfg['perc'],
        epochs  = cfg['ep']          # ← rename here
)

    alarms=[]
    for j in range(cfg['N_future']):
        _,(Xmon,ymon)=make_data(seed*cfg['N_future']+j,0,cfg['M'])
        Xmon=torch.as_tensor(sx.transform(Xmon),device=DEVICE,dtype=DTYPE_F)
        ymon=torch.as_tensor(sy.transform(ymon[:,None]).ravel(),device=DEVICE,dtype=DTYPE_F)
        Smon=score_vecs(Xmon,ymon,net0,cfg['α'])[:cfg['M']]
        alarms.append((ewma_T2(Smon,cfg['λ'],μ0,Σi0,len(Xtr))>CL).float())
    return torch.vstack(alarms)
# ───────────────────── FAR driver ─────────────────────────────────────────
def evaluate_far(cfg,N_db):
    exc=[]
    for s in tqdm(range(N_db),desc="DB replicates"):
        exc.append(run_db(s,cfg))
    return torch.vstack(exc).mean(0).cpu().numpy()
# ───────────────────── main ───────────────────────────────────────────────
if __name__=="__main__":
    CFG=dict(
        n_tr   = 200,
        M      = 500,
        α      = 1e-3,
        λ      = 0.01,
        lamΣ   = 5.0,
        BO     = 100,
        BI     = 200,
        perc   = 99.0,
        ep     = 600,
        N_future = 1_000
    )
    N_DB = 50
    t0=time.perf_counter()
    far = evaluate_far(CFG,N_DB)
    print(f"\nElapsed {time.perf_counter()-t0:.1f} s   (DEVICE={DEVICE})")
    x=np.arange(CFG['M'])
    plt.figure(figsize=(10,6))
    plt.semilogy(x,far,lw=2,label="Empirical FAR")
    plt.axhline(0.01,ls="--",c="#d95f02",lw=2,label="Nominal α = 0.01")
    plt.ylim(1e-4,1e-0); plt.xlim(0,CFG['M'])
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
    plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
    plt.grid(alpha=.3,which="both"); plt.legend(); plt.tight_layout(); plt.show()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Algorithm-1 nested bootstrap – nonlinear oscillator, type-I verification
Nominal α = 0.01 (perc = 99.0), n_tr = 200
"""

# ── 0. imports & style ────────────────────────────────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt, tqdm, warnings, time
warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({"font.size":14, "figure.dpi":160})

RNG    = np.random.default_rng(4)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on", DEVICE.upper())

# ── 1. oscillator simulator ───────────────────────────────────────────────
BASE = dict(m1=1,m2=2,k1=1,k2=2,k3=1.5,c1=.1,c2=.2)
def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s; d=p1-p2; phi=d/(1+abs(d))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_stream(n, p, noise=0.03):
    t=np.linspace(0,30,n)
    p1,v1,p2,v2 = simulate(t,[.5,0,-.5,0],p).T
    d=p1-p2; phi=d/(1+np.abs(d))
    X=np.column_stack([p1,v1,p2,v2])
    y=(0.5*(p['m1']*v1**2+p['m2']*v2**2)
      +0.5*(p['k1']*p1**2+p['k2']*p2**2)
      +p['k3']*phi + RNG.normal(0,noise,n))
    return X,y

def make_data(seed,n_tr,n_mon):
    Xtr,ytr=gen_stream(n_tr,BASE); Xmon,ymon=gen_stream(n_mon,BASE)
    return (Xtr,ytr),(Xmon,ymon)

# ── 2. tiny MLP & helpers ────────────────────────────────────────────────
class MLP(nn.Module):
    def __init__(self,d): super().__init__(); self.net=nn.Sequential(
        nn.Linear(d,5),nn.ReLU(),nn.Linear(5,1))
    def forward(self,x): return self.net(x)

def fit_net(X,y,α,epochs,lr=1e-3):
    net=MLP(X.shape[1]).to(DEVICE)
    opt,loss_fn = optim.Adam(net.parameters(),lr=lr,weight_decay=α),nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad(); loss_fn(net(X).squeeze(),y).backward(); opt.step()
    return net

def score_vecs(X, y, net, α):
    W, b   = list(net.net[-1].parameters())
    loss_fn = nn.MSELoss()
    grads   = []
    for xi, yi in zip(X, y):
        net.zero_grad(set_to_none=True)
        (loss_fn(net(xi[None]).squeeze(), yi)
         + α*(W.square().sum() + b.square().sum())
        ).backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()

def ewma_T2(S,λ,μ,Σi,n_tr):
    z,out=np.zeros_like(μ),np.empty(len(S))
    for t,s in enumerate(S,1):
        z=λ*s+(1-λ)*z
        num=(λ/(2-λ))*(1-(1-λ)**(2*t))+(3.72/n_tr)*(1-(1-λ)**t)**2
        den=(λ/(2-λ))*(1-(1-λ)**(2*t))+(1   /n_tr)*(1-(1-λ)**t)**2
        diff=(z/np.sqrt(num/den))-μ
        out[t-1]=diff@Σi@diff
    return out

# ── 3. Algorithm-1 nested bootstrap ───────────────────────────────────────
def nested_bootstrap_CL(X,y,*,α,λ,lamΣ,BO,BI,M,perc,epochs):
    net0 = fit_net(X,y,α,epochs)                      # Step 1
    μ0, Σ0 = score_vecs(X,y,net0,α).mean(0), np.cov(score_vecs(X,y,net0,α).T)
    Σi0 = np.linalg.pinv(Σ0+lamΣ*np.eye(Σ0.shape[0]),hermitian=True)

    n=len(X); T2store=[]
    for _ in range(BO):                               # outer bootstrap
        boot = RNG.choice(n,n,replace=True); oob=np.setdiff1d(np.arange(n),boot)
        net_b = fit_net(X[boot],y[boot],α,epochs)
        Sb    = score_vecs(X[boot],y[boot],net_b,α)
        μb    = Sb.mean(0)
        Σib   = np.linalg.pinv(np.cov(Sb.T)+lamΣ*np.eye(Sb.shape[1]),hermitian=True)
        S_oob = score_vecs(X[oob],y[oob],net_b,α)
        for _ in range(BI):                           # inner bootstrap
            idx = RNG.choice(len(S_oob),M,replace=True)
            T2store.append(ewma_T2(S_oob[idx],λ,μb,Σib,n))
    CL = np.percentile(np.vstack(T2store), perc, axis=0)  # point-wise
    return CL, μ0, Σi0, net0

# ── 4. one database replicate ─────────────────────────────────────────────
def process_db(seed,cfg):
    (Xtr,ytr),_ = make_data(seed,cfg['n_tr'],0)
    sx,sy = StandardScaler(),StandardScaler()
    Xtr_t=torch.as_tensor(sx.fit_transform(Xtr),device=DEVICE,dtype=torch.float32)
    ytr_t=torch.as_tensor(sy.fit_transform(ytr[:,None]).ravel(),
                          device=DEVICE,dtype=torch.float32)

    CL, μ0, Σi0, net0 = nested_bootstrap_CL(
        Xtr_t,ytr_t,α=cfg['α'],λ=cfg['λ'],lamΣ=cfg['lamΣ'],
        BO=cfg['BO'],BI=cfg['BI'],M=cfg['M'],perc=cfg['perc'],epochs=cfg['ep'])

    alarms=[]
    for j in range(cfg['N_future']):                  # many IC streams
        _,(Xmon,ymon)=make_data(seed*cfg['N_future']+j,0,cfg['M'])
        Xmon_t=torch.as_tensor(sx.transform(Xmon),device=DEVICE,dtype=torch.float32)
        ymon_t=torch.as_tensor(sy.transform(ymon[:,None]).ravel(),
                               device=DEVICE,dtype=torch.float32)
        S = score_vecs(Xmon_t, ymon_t, net0, cfg['α'])[:cfg['M']]
        alarms.append(ewma_T2(S,cfg['λ'],μ0,Σi0,len(Xtr_t)) > CL)
    return np.vstack(alarms)          # (N_future, M)

# ── 5. FAR driver (serial, GPU-friendly) ──────────────────────────────────
def evaluate_far(cfg,N_db):
    exc=[]
    for s in tqdm.tqdm(range(N_db),desc="DB replicates"):
        exc.append(process_db(s,cfg))
    return np.vstack(exc).mean(0)     # point-wise FAR

# ── 6. main ───────────────────────────────────────────────────────────────
if __name__=="__main__":
    CFG=dict(
        n_tr   = 100,
        M      = 200,
        α      = 1e-3,
        λ      = 0.03,
        lamΣ   = 2.0,
        BO     = 200,
        BI     = 500,
        perc   = 99.0,
        ep     = 1000,
        N_future = 1_000
    )
    N_DB = 5

    tic = time.perf_counter()
    far = evaluate_far(CFG,N_DB)
    print(f"\nElapsed {time.perf_counter()-tic:.1f} s  (DEVICE = {DEVICE})")

    x = np.arange(CFG['M'])
    plt.figure(figsize=(10,6))
    plt.semilogy(x, far, lw=2, label="Empirical FAR")
    plt.axhline(0.01, ls="--", c="#d95f02", lw=2, label="Nominal α = 0.01")
    plt.ylim(1e-4,1e-0); plt.xlim(0,CFG['M'])
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
    plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
    plt.grid(alpha=.3, which="both"); plt.legend(); plt.tight_layout(); plt.show()

np.mean(far)

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Algorithm-1 nested bootstrap – nonlinear oscillator, type-I verification
Nominal α = 0.01 (perc = 99.0), n_tr = 200
"""

# ── 0. imports & style ────────────────────────────────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt, tqdm, warnings, time
warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({"font.size":14, "figure.dpi":160})

RNG    = np.random.default_rng(4)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on", DEVICE.upper())

# ── 1. oscillator simulator ───────────────────────────────────────────────
BASE = dict(m1=1,m2=2,k1=1,k2=2,k3=1.5,c1=.1,c2=.2)
def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s; d=p1-p2; phi=d/(1+abs(d))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_stream(n, p, noise=0.03):
    t=np.linspace(0,90,n)
    p1,v1,p2,v2 = simulate(t,[.5,0,-.5,0],p).T
    d=p1-p2; phi=d/(1+np.abs(d))
    X=np.column_stack([p1,v1,p2,v2])
    y=(0.5*(p['m1']*v1**2+p['m2']*v2**2)
      +0.5*(p['k1']*p1**2+p['k2']*p2**2)
      +p['k3']*phi + RNG.normal(0,noise,n))
    return X,y

def make_data(seed,n_tr,n_mon):
    Xtr,ytr=gen_stream(n_tr,BASE); Xmon,ymon=gen_stream(n_mon,BASE)
    return (Xtr,ytr),(Xmon,ymon)

# ── 2. tiny MLP & helpers ────────────────────────────────────────────────
class MLP(nn.Module):
    def __init__(self,d): super().__init__(); self.net=nn.Sequential(
        nn.Linear(d,5),nn.ReLU(),nn.Linear(5,1))
    def forward(self,x): return self.net(x)

def fit_net(X,y,α,epochs,lr=1e-3):
    net=MLP(X.shape[1]).to(DEVICE)
    opt,loss_fn = optim.Adam(net.parameters(),lr=lr,weight_decay=α),nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad(); loss_fn(net(X).squeeze(),y).backward(); opt.step()
    return net

def score_vecs(X, y, net, α):
    W, b   = list(net.net[-1].parameters())
    loss_fn = nn.MSELoss()
    grads   = []
    for xi, yi in zip(X, y):
        net.zero_grad(set_to_none=True)
        (loss_fn(net(xi[None]).squeeze(), yi)
         + α*(W.square().sum() + b.square().sum())
        ).backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()

def ewma_T2(S,λ,μ,Σi,n_tr):
    z,out=np.zeros_like(μ),np.empty(len(S))
    for t,s in enumerate(S,1):
        z=λ*s+(1-λ)*z
        num=(λ/(2-λ))*(1-(1-λ)**(2*t))+(3.72/n_tr)*(1-(1-λ)**t)**2
        den=(λ/(2-λ))*(1-(1-λ)**(2*t))+(1   /n_tr)*(1-(1-λ)**t)**2
        diff=(z/np.sqrt(num/den))-μ
        out[t-1]=diff@Σi@diff
    return out

# ── 3. Algorithm-1 nested bootstrap ───────────────────────────────────────
def nested_bootstrap_CL(X,y,*,α,λ,lamΣ,BO,BI,M,perc,epochs):
    net0 = fit_net(X,y,α,epochs)                      # Step 1
    μ0, Σ0 = score_vecs(X,y,net0,α).mean(0), np.cov(score_vecs(X,y,net0,α).T)
    Σi0 = np.linalg.pinv(Σ0+lamΣ*np.eye(Σ0.shape[0]),hermitian=True)

    n=len(X); T2store=[]
    for _ in range(BO):                               # outer bootstrap
        boot = RNG.choice(n,n,replace=True); oob=np.setdiff1d(np.arange(n),boot)
        net_b = fit_net(X[boot],y[boot],α,epochs)
        Sb    = score_vecs(X[boot],y[boot],net_b,α)
        μb    = Sb.mean(0)
        Σib   = np.linalg.pinv(np.cov(Sb.T)+lamΣ*np.eye(Sb.shape[1]),hermitian=True)
        S_oob = score_vecs(X[oob],y[oob],net_b,α)
        for _ in range(BI):                           # inner bootstrap
            idx = RNG.choice(len(S_oob),M,replace=True)
            T2store.append(ewma_T2(S_oob[idx],λ,μb,Σib,n))
    CL = np.percentile(np.vstack(T2store), perc, axis=0)  # point-wise
    return CL, μ0, Σi0, net0

# ── 4. one database replicate ─────────────────────────────────────────────
def process_db(seed,cfg):
    (Xtr,ytr),_ = make_data(seed,cfg['n_tr'],0)
    sx,sy = StandardScaler(),StandardScaler()
    Xtr_t=torch.as_tensor(sx.fit_transform(Xtr),device=DEVICE,dtype=torch.float32)
    ytr_t=torch.as_tensor(sy.fit_transform(ytr[:,None]).ravel(),
                          device=DEVICE,dtype=torch.float32)

    CL, μ0, Σi0, net0 = nested_bootstrap_CL(
        Xtr_t,ytr_t,α=cfg['α'],λ=cfg['λ'],lamΣ=cfg['lamΣ'],
        BO=cfg['BO'],BI=cfg['BI'],M=cfg['M'],perc=cfg['perc'],epochs=cfg['ep'])

    alarms=[]
    for j in range(cfg['N_future']):                  # many IC streams
        _,(Xmon,ymon)=make_data(seed*cfg['N_future']+j,0,cfg['M'])
        Xmon_t=torch.as_tensor(sx.transform(Xmon),device=DEVICE,dtype=torch.float32)
        ymon_t=torch.as_tensor(sy.transform(ymon[:,None]).ravel(),
                               device=DEVICE,dtype=torch.float32)
        S = score_vecs(Xmon_t, ymon_t, net0, cfg['α'])[:cfg['M']]
        alarms.append(ewma_T2(S,cfg['λ'],μ0,Σi0,len(Xtr_t)) > CL)
    return np.vstack(alarms)          # (N_future, M)

# ── 5. FAR driver (serial, GPU-friendly) ──────────────────────────────────
def evaluate_far(cfg,N_db):
    exc=[]
    for s in tqdm.tqdm(range(N_db),desc="DB replicates"):
        exc.append(process_db(s,cfg))
    return np.vstack(exc).mean(0)     # point-wise FAR

# ── 6. main ───────────────────────────────────────────────────────────────
if __name__=="__main__":
    CFG=dict(
        n_tr   = 200,
        M      = 100,
        α      = 1e-3,
        λ      = 0.01,
        lamΣ   = 4.0,
        BO     = 100,
        BI     = 200,
        perc   = 99.0,
        ep     = 1000,
        N_future = 1_000
    )
    N_DB = 50

    tic = time.perf_counter()
    far = evaluate_far(CFG,N_DB)
    print(f"\nElapsed {time.perf_counter()-tic:.1f} s  (DEVICE = {DEVICE})")

    x = np.arange(CFG['M'])
    plt.figure(figsize=(10,6))
    plt.semilogy(x, far, lw=2, label="Empirical FAR")
    plt.axhline(0.01, ls="--", c="#d95f02", lw=2, label="Nominal α = 0.01")
    plt.ylim(1e-4,1e-0); plt.xlim(0,CFG['M'])
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
    plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
    plt.grid(alpha=.3, which="both"); plt.legend(); plt.tight_layout(); plt.show()

np.mean(far)

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Algorithm-1 nested bootstrap – nonlinear oscillator, type-I verification
Nominal α = 0.01 (perc = 99.0), n_tr = 200
"""

# ── 0. imports & style ────────────────────────────────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt, tqdm, warnings, time
warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({"font.size":14, "figure.dpi":160})

RNG    = np.random.default_rng(4)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on", DEVICE.upper())

# ── 1. oscillator simulator ───────────────────────────────────────────────
BASE = dict(m1=1,m2=2,k1=1,k2=2,k3=1.5,c1=.1,c2=.2)
def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s; d=p1-p2; phi=d/(1+abs(d))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_stream(n, p, noise=0.03):
    t=np.linspace(0,90,n)
    p1,v1,p2,v2 = simulate(t,[.5,0,-.5,0],p).T
    d=p1-p2; phi=d/(1+np.abs(d))
    X=np.column_stack([p1,v1,p2,v2])
    y=(0.5*(p['m1']*v1**2+p['m2']*v2**2)
      +0.5*(p['k1']*p1**2+p['k2']*p2**2)
      +p['k3']*phi + RNG.normal(0,noise,n))
    return X,y

def make_data(seed,n_tr,n_mon):
    Xtr,ytr=gen_stream(n_tr,BASE); Xmon,ymon=gen_stream(n_mon,BASE)
    return (Xtr,ytr),(Xmon,ymon)

# ── 2. tiny MLP & helpers ────────────────────────────────────────────────
class MLP(nn.Module):
    def __init__(self,d): super().__init__(); self.net=nn.Sequential(
        nn.Linear(d,5),nn.ReLU(),nn.Linear(5,1))
    def forward(self,x): return self.net(x)

def fit_net(X,y,α,epochs,lr=1e-3):
    net=MLP(X.shape[1]).to(DEVICE)
    opt,loss_fn = optim.Adam(net.parameters(),lr=lr,weight_decay=α),nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad(); loss_fn(net(X).squeeze(),y).backward(); opt.step()
    return net

def score_vecs(X, y, net, α):
    W, b   = list(net.net[-1].parameters())
    loss_fn = nn.MSELoss()
    grads   = []
    for xi, yi in zip(X, y):
        net.zero_grad(set_to_none=True)
        (loss_fn(net(xi[None]).squeeze(), yi)
         + α*(W.square().sum() + b.square().sum())
        ).backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()

def ewma_T2(S,λ,μ,Σi,n_tr):
    z,out=np.zeros_like(μ),np.empty(len(S))
    for t,s in enumerate(S,1):
        z=λ*s+(1-λ)*z
        num=(λ/(2-λ))*(1-(1-λ)**(2*t))+(3.72/n_tr)*(1-(1-λ)**t)**2
        den=(λ/(2-λ))*(1-(1-λ)**(2*t))+(1   /n_tr)*(1-(1-λ)**t)**2
        diff=(z/np.sqrt(num/den))-μ
        out[t-1]=diff@Σi@diff
    return out

# ── 3. Algorithm-1 nested bootstrap ───────────────────────────────────────
def nested_bootstrap_CL(X,y,*,α,λ,lamΣ,BO,BI,M,perc,epochs):
    net0 = fit_net(X,y,α,epochs)                      # Step 1
    μ0, Σ0 = score_vecs(X,y,net0,α).mean(0), np.cov(score_vecs(X,y,net0,α).T)
    Σi0 = np.linalg.pinv(Σ0+lamΣ*np.eye(Σ0.shape[0]),hermitian=True)

    n=len(X); T2store=[]
    for _ in range(BO):                               # outer bootstrap
        boot = RNG.choice(n,n,replace=True); oob=np.setdiff1d(np.arange(n),boot)
        net_b = fit_net(X[boot],y[boot],α,epochs)
        Sb    = score_vecs(X[boot],y[boot],net_b,α)
        μb    = Sb.mean(0)
        Σib   = np.linalg.pinv(np.cov(Sb.T)+lamΣ*np.eye(Sb.shape[1]),hermitian=True)
        S_oob = score_vecs(X[oob],y[oob],net_b,α)
        for _ in range(BI):                           # inner bootstrap
            idx = RNG.choice(len(S_oob),M,replace=True)
            T2store.append(ewma_T2(S_oob[idx],λ,μb,Σib,n))
    CL = np.percentile(np.vstack(T2store), perc, axis=0)  # point-wise
    return CL, μ0, Σi0, net0

# ── 4. one database replicate ─────────────────────────────────────────────
def process_db(seed,cfg):
    (Xtr,ytr),_ = make_data(seed,cfg['n_tr'],0)
    sx,sy = StandardScaler(),StandardScaler()
    Xtr_t=torch.as_tensor(sx.fit_transform(Xtr),device=DEVICE,dtype=torch.float32)
    ytr_t=torch.as_tensor(sy.fit_transform(ytr[:,None]).ravel(),
                          device=DEVICE,dtype=torch.float32)

    CL, μ0, Σi0, net0 = nested_bootstrap_CL(
        Xtr_t,ytr_t,α=cfg['α'],λ=cfg['λ'],lamΣ=cfg['lamΣ'],
        BO=cfg['BO'],BI=cfg['BI'],M=cfg['M'],perc=cfg['perc'],epochs=cfg['ep'])

    alarms=[]
    for j in range(cfg['N_future']):                  # many IC streams
        _,(Xmon,ymon)=make_data(seed*cfg['N_future']+j,0,cfg['M'])
        Xmon_t=torch.as_tensor(sx.transform(Xmon),device=DEVICE,dtype=torch.float32)
        ymon_t=torch.as_tensor(sy.transform(ymon[:,None]).ravel(),
                               device=DEVICE,dtype=torch.float32)
        S = score_vecs(Xmon_t, ymon_t, net0, cfg['α'])[:cfg['M']]
        alarms.append(ewma_T2(S,cfg['λ'],μ0,Σi0,len(Xtr_t)) > CL)
    return np.vstack(alarms)          # (N_future, M)

# ── 5. FAR driver (serial, GPU-friendly) ──────────────────────────────────
def evaluate_far(cfg,N_db):
    exc=[]
    for s in tqdm.tqdm(range(N_db),desc="DB replicates"):
        exc.append(process_db(s,cfg))
    return np.vstack(exc).mean(0)     # point-wise FAR

# ── 6. main ───────────────────────────────────────────────────────────────
if __name__=="__main__":
    CFG=dict(
        n_tr   = 200,
        M      = 100,
        α      = 1e-3,
        λ      = 0.01,
        lamΣ   = 4.0,
        BO     = 100,
        BI     = 200,
        perc   = 99.0,
        ep     = 100,
        N_future = 1_000
    )
    N_DB = 50

    tic = time.perf_counter()
    far = evaluate_far(CFG,N_DB)
    print(f"\nElapsed {time.perf_counter()-tic:.1f} s  (DEVICE = {DEVICE})")

    x = np.arange(CFG['M'])
    plt.figure(figsize=(10,6))
    plt.semilogy(x, far, lw=2, label="Empirical FAR")
    plt.axhline(0.01, ls="--", c="#d95f02", lw=2, label="Nominal α = 0.01")
    plt.ylim(1e-4,1e-0); plt.xlim(0,CFG['M'])
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
    plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
    plt.grid(alpha=.3, which="both"); plt.legend(); plt.tight_layout(); plt.show()

# ==== after far is available ====
from pathlib import Path, PurePath
import json, numpy as np, time, pandas as pd

stamp = time.strftime("%Y%m%d_%H%M%S")
out   = Path(f"oscillator_far_{stamp}.npz")

np.savez_compressed(
    out,
    far = far,                 # 1-D numpy array
    x   = np.arange(CFG["M"]), # time index
    cfg = json.dumps(CFG),     # store the whole config as JSON
)

# (optional) also keep a human-readable CSV
pd.DataFrame({"i":np.arange(CFG["M"]), "FAR":far}).to_csv(out.with_suffix(".csv"), index=False)

print(f"\nSaved results to: {out.resolve()}")

import numpy as np, json, matplotlib.pyplot as plt, pandas as pd

fn = "oscillator_far_20250712_193912.npz"   # <- your saved file
data = np.load(fn, allow_pickle=True)

far  = data["far"]
x    = data["x"]
cfg  = json.loads(data["cfg"].item())       # back to dict

plt.figure(figsize=(10,6))
plt.semilogy(x, far, lw=2, label="Empirical FAR")
plt.axhline(0.01, ls="--", lw=2, color="#d95f02", label="Nominal α = 0.01")
plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
plt.grid(alpha=.3, which="both"); plt.legend(); plt.tight_layout(); plt.show()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Non-linear oscillator  •  Zhang-style two-phase EWMA chart
Type-I error verification  (nominal α ≈ 0.001 via 99.9-th-percentile UCL)

Safe on any machine:
  – CPU-only  (tiny MLP trains in ms)
  – No fork-related crashes (default: threads)
Switch modes with evaluate_far(mode=…).
"""

# ───────────────── 0. imports & style ────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
import matplotlib.pyplot as plt, multiprocessing as mp, time, warnings
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({"font.size":14, "figure.dpi":150})
RNG    = np.random.default_rng()
DEVICE = "cpu"          # <- keep everything on CPU (no fork/CUDA issues)

# ─────────────── 1. oscillator simulator ────────────────
BASE = dict(m1=1,m2=2,k1=1,k2=2,k3=1.5,c1=.1,c2=.2)

def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s; d=p1-p2; phi=d/(1+abs(d))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    from scipy.integrate import odeint
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_ic(seed, n, noise=0.03):
    r = np.random.default_rng(seed)
    t = np.linspace(0, 90, n)
    p1,v1,p2,v2 = simulate(t,[.5,0,-.5,0],BASE).T
    d   = p1 - p2;  phi = d / (1 + np.abs(d))
    X   = np.column_stack([p1, v1, p2, v2])
    y   = (0.5*(BASE['m1']*v1**2 + BASE['m2']*v2**2)
         + 0.5*(BASE['k1']*p1**2 + BASE['k2']*p2**2)
         + BASE['k3']*phi + r.normal(0, noise, n))
    return X, y

# ─────────────── 2. tiny MLP & helpers ────────────────
class MLP(nn.Module):
    def __init__(self, d): super().__init__(); self.net = nn.Sequential(
        nn.Linear(d, 5), nn.ReLU(), nn.Linear(5, 1))
    def forward(self, x): return self.net(x)

def fit_net(X, y, α, epochs=1000):
    net, opt = MLP(X.shape[1]).to(DEVICE), None
    opt = optim.Adam(net.parameters(), lr=1e-3, weight_decay=α)
    loss_fn = nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad(); loss_fn(net(X).squeeze(), y).backward(); opt.step()
    return net

def score_vecs(X, y, net, α):
    W, b  = list(net.net[-1].parameters())
    loss  = nn.MSELoss(); grads=[]
    for xi, yi in zip(X, y):
        net.zero_grad(set_to_none=True)
        (loss(net(xi[None]).squeeze(), yi) +
         α*(W.square().sum() + b.square().sum())
        ).backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).numpy()

def ewma_T2(S, λ, μ, Σ_inv):
    z, out = np.zeros_like(μ), np.empty(len(S))
    for t, s in enumerate(S):
        z = λ*s + (1-λ)*z
        d = (z - μ)[:, None]
        out[t] = (d.T @ Σ_inv @ d).item()
    return out

# ─── 3. one database replicate (two-phase protocol) ────
def process_db(seed, cfg):
    # Phase-I split
    X, y = gen_ic(seed, cfg['n_total'])
    h    = cfg['n_total'] // 2
    X1, y1, X2, y2 = X[:h], y[:h], X[h:], y[h:]

    # Fit on D₁
    scaler = StandardScaler()
    X1_t = torch.tensor(scaler.fit_transform(X1), dtype=torch.float32, device=DEVICE)
    y1_t = torch.tensor(y1, dtype=torch.float32, device=DEVICE)
    net  = fit_net(X1_t, y1_t, cfg['alpha'], epochs=cfg['epochs'])

    # μ̂, Σ̂, constant UCL from full D₂
    X2_t = torch.tensor(scaler.transform(X2), dtype=torch.float32, device=DEVICE)
    y2_t = torch.tensor(y2, dtype=torch.float32, device=DEVICE)
    S2   = score_vecs(X2_t, y2_t, net, cfg['alpha'])
    μ    = S2.mean(0)
    Σ_inv = np.linalg.pinv(np.cov(S2.T) + cfg['λ_cov']*np.eye(S2.shape[1]),
                           hermitian=True)
    UCL  = np.percentile(ewma_T2(S2, cfg['λ'], μ, Σ_inv), cfg['perc'])

    # Future IC streams
    exc = np.zeros((cfg['N_future'], cfg['M']), dtype=bool)
    for j in range(cfg['N_future']):
        Xf, yf = gen_ic(seed*cfg['N_future'] + j, cfg['M'])
        Xf_t = torch.tensor(scaler.transform(Xf), dtype=torch.float32, device=DEVICE)
        yf_t = torch.tensor(yf, dtype=torch.float32, device=DEVICE)
        Sf   = score_vecs(Xf_t, yf_t, net, cfg['alpha'])
        exc[j] = ewma_T2(Sf, cfg['λ'], μ, Σ_inv) > UCL
    return exc

# ───────── 4. FAR driver – threads / forkserver / serial ────────
def evaluate_far(cfg, N_db=20, mode="threads", workers=None):
    """
    mode: 'threads'   – safest (default)
          'forkserver'– fresh processes, avoids fork issues
          'serial'    – no parallelism
    """
    if workers is None: workers = max(1, mp.cpu_count() - 1)
    all_exc = []

    if mode == "serial":
        for s in tqdm(range(N_db), desc="DB replicates"):
            all_exc.append(process_db(s, cfg))

    elif mode == "threads":
        with ThreadPoolExecutor(max_workers=workers) as pool:
            futs = [pool.submit(process_db, s, cfg) for s in range(N_db)]
            for f in tqdm(as_completed(futs), total=N_db, desc="DB replicates"):
                all_exc.append(f.result())

    elif mode == "forkserver":
        ctx = mp.get_context("forkserver")
        with ProcessPoolExecutor(max_workers=workers, mp_context=ctx) as pool:
            futs = [pool.submit(process_db, s, cfg) for s in range(N_db)]
            for f in tqdm(as_completed(futs), total=N_db, desc="DB replicates"):
                all_exc.append(f.result())
    else:
        raise ValueError("mode must be 'threads', 'forkserver', or 'serial'")

    all_exc = np.asarray(all_exc)                 # (db, rep, M)
    return np.arange(cfg['M']), all_exc.mean((0, 1))

# ─────────────── 5. main & plot ────────────────
if __name__ == "__main__":
    cfg = dict(
        n_total  = 200,      # 200 in D₁ + 200 in D₂
        M        = 100,      # monitoring length
        N_future = 1_000,    # IC streams per DB
        λ        = 0.01,     # EWMA weight
        alpha    = 1e-3,     # ridge reg in MLP
        λ_cov    = 4,     # tiny ridge on Σ̂
        perc     = 99.0,     # 99.9-th percentile → α ≈ 0.001
        epochs   = 1_000     # training epochs
    )
    N_DB = 50               # database replicates
    MODE = "threads"        # 'threads', 'forkserver', or 'serial'

    tic = time.perf_counter()
    i_vals, far = evaluate_far(cfg, N_db=N_DB, mode=MODE)
    print(f"Elapsed {time.perf_counter()-tic:.1f} s  (mode = {MODE})")

    plt.figure(figsize=(10,6))
    plt.semilogy(i_vals, far, lw=2, label="Empirical FAR")
    plt.axhline(0.001, ls="--", lw=2, c="#d95f02", label="Nominal α = 0.01")
    plt.ylim(1e-4, 1e-0); plt.xlabel("Time index $i$")
    plt.ylabel("False-alarm rate (log)")
    plt.title("Non-linear oscillator – Zhang two-phase FAR")
    plt.grid(alpha=.3, which="both"); plt.legend(); plt.tight_layout(); plt.show()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Algorithm-1 nested bootstrap – nonlinear oscillator, type-I verification
Nominal α = 0.01 (perc = 99.0), n_tr = 200
"""

# ── 0. imports & style ────────────────────────────────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt, tqdm, warnings, time
warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({"font.size":14, "figure.dpi":160})

RNG    = np.random.default_rng(4)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on", DEVICE.upper())

# ── 1. oscillator simulator ───────────────────────────────────────────────
BASE = dict(m1=1,m2=2,k1=1,k2=2,k3=1.5,c1=.1,c2=.2)
def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s; d=p1-p2; phi=d/(1+abs(d))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_stream(n, p, noise=0.03):
    t=np.linspace(0,90,n)
    p1,v1,p2,v2 = simulate(t,[.5,0,-.5,0],p).T
    d=p1-p2; phi=d/(1+np.abs(d))
    X=np.column_stack([p1,v1,p2,v2])
    y=(0.5*(p['m1']*v1**2+p['m2']*v2**2)
      +0.5*(p['k1']*p1**2+p['k2']*p2**2)
      +p['k3']*phi + RNG.normal(0,noise,n))
    return X,y

def make_data(seed,n_tr,n_mon):
    Xtr,ytr=gen_stream(n_tr,BASE); Xmon,ymon=gen_stream(n_mon,BASE)
    return (Xtr,ytr),(Xmon,ymon)

# ── 2. tiny MLP & helpers ────────────────────────────────────────────────
class MLP(nn.Module):
    def __init__(self,d): super().__init__(); self.net=nn.Sequential(
        nn.Linear(d,3),nn.ReLU(),nn.Linear(3,1))
    def forward(self,x): return self.net(x)

def fit_net(X,y,α,epochs,lr=1e-3):
    net=MLP(X.shape[1]).to(DEVICE)
    opt,loss_fn = optim.Adam(net.parameters(),lr=lr,weight_decay=α),nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad(); loss_fn(net(X).squeeze(),y).backward(); opt.step()
    return net

def score_vecs(X, y, net, α):
    W, b   = list(net.net[-1].parameters())
    loss_fn = nn.MSELoss()
    grads   = []
    for xi, yi in zip(X, y):
        net.zero_grad(set_to_none=True)
        (loss_fn(net(xi[None]).squeeze(), yi)
         + α*(W.square().sum() + b.square().sum())
        ).backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()

def ewma_T2(S,λ,μ,Σi,n_tr):
    z,out=np.zeros_like(μ),np.empty(len(S))
    for t,s in enumerate(S,1):
        z=λ*s+(1-λ)*z
        num=(λ/(2-λ))*(1-(1-λ)**(2*t))+(3.72/n_tr)*(1-(1-λ)**t)**2
        den=(λ/(2-λ))*(1-(1-λ)**(2*t))+(1   /n_tr)*(1-(1-λ)**t)**2
        diff=(z/np.sqrt(num/den))-μ
        out[t-1]=diff@Σi@diff
    return out

# ── 3. Algorithm-1 nested bootstrap ───────────────────────────────────────
def nested_bootstrap_CL(X,y,*,α,λ,lamΣ,BO,BI,M,perc,epochs):
    net0 = fit_net(X,y,α,epochs)                      # Step 1
    μ0, Σ0 = score_vecs(X,y,net0,α).mean(0), np.cov(score_vecs(X,y,net0,α).T)
    Σi0 = np.linalg.pinv(Σ0+lamΣ*np.eye(Σ0.shape[0]),hermitian=True)

    n=len(X); T2store=[]
    for _ in range(BO):                               # outer bootstrap
        boot = RNG.choice(n,n,replace=True); oob=np.setdiff1d(np.arange(n),boot)
        net_b = fit_net(X[boot],y[boot],α,epochs)
        Sb    = score_vecs(X[boot],y[boot],net_b,α)
        μb    = Sb.mean(0)
        Σib   = np.linalg.pinv(np.cov(Sb.T)+lamΣ*np.eye(Sb.shape[1]),hermitian=True)
        S_oob = score_vecs(X[oob],y[oob],net_b,α)
        for _ in range(BI):                           # inner bootstrap
            idx = RNG.choice(len(S_oob),M,replace=True)
            T2store.append(ewma_T2(S_oob[idx],λ,μb,Σib,n))
    CL = np.percentile(np.vstack(T2store), perc, axis=0)  # point-wise
    return CL, μ0, Σi0, net0

# ── 4. one database replicate ─────────────────────────────────────────────
def process_db(seed,cfg):
    (Xtr,ytr),_ = make_data(seed,cfg['n_tr'],0)
    sx,sy = StandardScaler(),StandardScaler()
    Xtr_t=torch.as_tensor(sx.fit_transform(Xtr),device=DEVICE,dtype=torch.float32)
    ytr_t=torch.as_tensor(sy.fit_transform(ytr[:,None]).ravel(),
                          device=DEVICE,dtype=torch.float32)

    CL, μ0, Σi0, net0 = nested_bootstrap_CL(
        Xtr_t,ytr_t,α=cfg['α'],λ=cfg['λ'],lamΣ=cfg['lamΣ'],
        BO=cfg['BO'],BI=cfg['BI'],M=cfg['M'],perc=cfg['perc'],epochs=cfg['ep'])

    if np.isnan(CL).any():
      print("⚠️  NaNs found in CL – FAR will be zero")

    alarms=[]
    for j in range(cfg['N_future']):                  # many IC streams
        _,(Xmon,ymon)=make_data(seed*cfg['N_future']+j,0,cfg['M'])
        Xmon_t=torch.as_tensor(sx.transform(Xmon),device=DEVICE,dtype=torch.float32)
        ymon_t=torch.as_tensor(sy.transform(ymon[:,None]).ravel(),
                               device=DEVICE,dtype=torch.float32)
        S = score_vecs(Xmon_t, ymon_t, net0, cfg['α'])[:cfg['M']]
        alarms.append(ewma_T2(S,cfg['λ'],μ0,Σi0,len(Xtr_t)) > CL)
    return np.vstack(alarms)          # (N_future, M)

# ── 5. FAR driver (serial, GPU-friendly) ──────────────────────────────────
def evaluate_far(cfg,N_db):
    exc=[]
    for s in tqdm.tqdm(range(N_db),desc="DB replicates"):
        exc.append(process_db(s,cfg))
    return np.vstack(exc).mean(0)     # point-wise FAR

# ── 6. main ───────────────────────────────────────────────────────────────
if __name__=="__main__":
    CFG=dict(
        n_tr   = 200,
        M      = 100,
        α      = 1e-3,
        λ      = 0.01,
        lamΣ   = 4.0,
        BO     = 100,
        BI     = 200,
        perc   = 99.0,
        ep     = 100,
        N_future = 1_000
    )
    N_DB = 5

    tic = time.perf_counter()
    far = evaluate_far(CFG,N_DB)
    print(f"\nElapsed {time.perf_counter()-tic:.1f} s  (DEVICE = {DEVICE})")

    x = np.arange(CFG['M'])
    plt.figure(figsize=(10,6))
    plt.semilogy(x, far, lw=2, label="Empirical FAR")
    plt.axhline(0.01, ls="--", c="#d95f02", lw=2, label="Nominal α = 0.01")
    plt.ylim(1e-4,1e-0); plt.xlim(0,CFG['M'])
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
    plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
    plt.grid(alpha=.3, which="both"); plt.legend(); plt.tight_layout(); plt.show()

np.mean(far)

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step-1 retrospective stability check for the Zhang two-phase MEWMA chart.

• Synthetic data: non-linear two-mass oscillator (Febbo & Machado-2013 variant)
• Training block D₁ : 100 observations
• Stability is accepted if every EWMA-T² stays below χ²_{p,1-α_stab}

Dependencies: numpy, scipy, torch, scikit-learn
"""

# ───────────────── 0. imports & global settings ─────────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from sklearn.preprocessing import StandardScaler
from scipy.integrate import odeint
from scipy.stats import chi2

torch.set_num_threads(1)            # deterministic & light-weight
DEVICE = "cpu"
RNG    = np.random.default_rng(0)   # reproducibility

# ───────────────── 1. oscillator simulator & data generator ────────────────
BASE = dict(m1=1, m2=2, k1=1, k2=2, k3=1.5, c1=.1, c2=.2)

def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s
        d   = p1 - p2
        phi = d / (1 + abs(d))                 # finite-extensibility spring
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_ic(seed, n, noise_sd=0.03):
    r = np.random.default_rng(seed)
    t = np.linspace(0, 90, n)
    p1,v1,p2,v2 = simulate(t, [.5,0,-.5,0], BASE).T

    d   = p1 - p2
    phi = d / (1 + np.abs(d))
    X   = np.column_stack([p1, v1, p2, v2])
    y   = (0.5*(BASE['m1']*v1**2 + BASE['m2']*v2**2)
         + 0.5*(BASE['k1']*p1**2 + BASE['k2']*p2**2)
         + BASE['k3']*phi
         + r.normal(0, noise_sd, n))
    return X, y

# ───────────────── 2. tiny MLP + utilities ─────────────────────────────────
class TinyMLP(nn.Module):
    def __init__(self, d_in):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(d_in, 5),
                                 nn.ReLU(),
                                 nn.Linear(5, 1))
    def forward(self, x): return self.net(x)

def fit_net(X, y, alpha, epochs=1_000):
    net = TinyMLP(X.shape[1]).to(DEVICE)
    opt = optim.Adam(net.parameters(), lr=1e-3, weight_decay=alpha)
    loss_fn = nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad()
        loss_fn(net(X).squeeze(), y).backward()
        opt.step()
    return net

@torch.no_grad()
# --- replace the whole score_vecs definition with this --------------------
# ───── score_vecs (autograd-safe version) ──────────────────────────────────
def score_vecs(X, y, net, alpha):
    """
    Return one gradient ("score vector") per observation for the
    ridge-penalised MSE loss evaluated at the trained net.

    Output shape: (n_samples, n_params_final_layer)
    """
    W, b = list(net.net[-1].parameters())          # final linear layer
    loss_fn = nn.MSELoss()
    grads = []

    for xi, yi in zip(X, y):
        # Make sure autograd is ON for this little block
        with torch.enable_grad():
            xi = xi.unsqueeze(0).detach()          # shape (1, d)
            yi = yi.detach()                       # 0-D tensor
            out  = net(xi).squeeze()
            loss = loss_fn(out, yi) + alpha * (W.pow(2).sum() + b.pow(2).sum())

            # gradient wrt the final-layer weights & bias
            grad_w, grad_b = torch.autograd.grad(loss, [W, b], retain_graph=False)
            grads.append(torch.cat([grad_w.flatten(), grad_b]))

    return torch.stack(grads).cpu().numpy()

# ───────────────── 3. retrospective stability check ────────────────────────
def retro_stability_ok(S, lam=0.01, alpha_stab=0.01, eps=1e-3):
    """
    Returns True if the EWMA of score vectors from D₁ is 'stable'
    in the sense of Zhang et al. Algorithm 1 Step-1.
    """
    p     = S.shape[1]
    mu    = S.mean(axis=0)
    Sigma = np.cov(S.T) + eps*np.eye(p)
    Sigma_z = (lam / (2 - lam)) * Sigma            # steady-state EWMA var
    Sigma_z_inv = np.linalg.pinv(Sigma_z, hermitian=True)

    chi2_thr = chi2.ppf(1 - alpha_stab, p)
    z = np.zeros(p)
    for s in S:
        z = lam * s + (1 - lam) * z
        if (z - mu) @ Sigma_z_inv @ (z - mu) > chi2_thr:
            return False                           # unstable
    return True

# ───────────────── 4. demo run ──────────────────────────────────────────────
if __name__ == "__main__":
    # ---------- generate D₁ (100 points) -----------------------------------
    X, y = gen_ic(seed=42, n=100)

    # ---------- fit tiny network -------------------------------------------
    scaler = StandardScaler()
    Xt = torch.tensor(scaler.fit_transform(X), dtype=torch.float32, device=DEVICE)
    yt = torch.tensor(y, dtype=torch.float32, device=DEVICE)
    net = fit_net(Xt, yt, alpha=1e-3)

    # ---------- compute score vectors on D₁ --------------------------------
    S1 = score_vecs(Xt, yt, net, alpha=1e-3)

    # ---------- stability test ---------------------------------------------
    ok = retro_stability_ok(S1, lam=0.01, alpha_stab=0.01, eps=1e-3)
    print(f"Retrospective stability check passed?  {ok}")

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Search for a stable 100-point D₁ block (Step-1 of Zhang-style Algorithm 1).

If stability fails, resample with a new seed until success or max_tries.
"""

# ────────────────── 0. imports & global constants ──────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from sklearn.preprocessing import StandardScaler
from scipy.integrate import odeint
from scipy.stats import chi2

DEVICE = "cpu"          # keep everything on CPU
torch.set_num_threads(1)
RNG = np.random.default_rng(0)   # base RNG

# ────────────────── 1. oscillator simulator & IC generator ─────────────────
BASE = dict(m1=1, m2=2, k1=1, k2=2, k3=1.5, c1=.1, c2=.2)

def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s
        d, phi = p1 - p2, (p1 - p2)/(1 + abs(p1 - p2))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_ic(seed, n=100, noise_sd=0.03):
    r = np.random.default_rng(seed)
    t  = np.linspace(0, 90, n)
    p1,v1,p2,v2 = simulate(t, [.5,0,-.5,0], BASE).T
    d   = p1 - p2
    phi = d / (1 + np.abs(d))
    X   = np.column_stack([p1, v1, p2, v2])
    y   = (0.5*(BASE['m1']*v1**2 + BASE['m2']*v2**2)
         + 0.5*(BASE['k1']*p1**2 + BASE['k2']*p2**2)
         + BASE['k3']*phi
         + r.normal(0, noise_sd, n))
    return X, y

# ────────────────── 2. tiny MLP & utilities ────────────────────────────────
class TinyMLP(nn.Module):
    def __init__(self, d_in):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(d_in, 5), nn.ReLU(), nn.Linear(5, 1))
    def forward(self, x): return self.net(x)

def fit_net(X, y, alpha=1e-3, epochs=1_000):
    net = TinyMLP(X.shape[1]).to(DEVICE)
    opt = optim.Adam(net.parameters(), lr=1e-3, weight_decay=alpha)
    loss_fn = nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad()
        loss_fn(net(X).squeeze(), y).backward()
        opt.step()
    return net

def score_vecs(X, y, net, alpha=1e-3):
    """Gradient of penalised MSE wrt final-layer params, one per obs."""
    W, b = list(net.net[-1].parameters())
    loss_fn = nn.MSELoss()
    grads = []
    for xi, yi in zip(X, y):
        with torch.enable_grad():
            xi = xi.unsqueeze(0).detach()
            yi = yi.detach()
            loss = loss_fn(net(xi).squeeze(), yi) + alpha*(W.pow(2).sum() + b.pow(2).sum())
            grad_w, grad_b = torch.autograd.grad(loss, [W, b], retain_graph=False)
            grads.append(torch.cat([grad_w.flatten(), grad_b]))
    return torch.stack(grads).cpu().numpy()

def retro_stability_ok(S, lam=0.01, alpha_stab=0.01, eps=1e-3):
    """Step-1 EWMA-T² stability test."""
    p     = S.shape[1]
    mu    = S.mean(axis=0)
    Sigma = np.cov(S.T) + eps*np.eye(p)
    Sigma_z_inv = np.linalg.pinv((lam/(2 - lam))*Sigma, hermitian=True)
    chi2_thr = chi2.ppf(1 - alpha_stab, p)
    z = np.zeros(p)
    for s in S:
        z = lam * s + (1 - lam) * z
        if (z - mu) @ Sigma_z_inv @ (z - mu) > chi2_thr:
            return False
    return True

# ────────────────── 3. sampler that retries until stable ───────────────────
def sample_stable_D1(seed0=0, n1=100, lam=0.01, alpha_stab=0.01,
                     alpha_ridge=1e-3, max_tries=20):
    """
    Returns (X1, y1, scaler, fitted_net, tries_used)
    or raises RuntimeError if no stable block found in max_tries tries.
    """
    for k in range(max_tries):
        seed = seed0 + k
        X1, y1 = gen_ic(seed, n=n1)
        scaler = StandardScaler()
        Xt = torch.tensor(scaler.fit_transform(X1), dtype=torch.float32, device=DEVICE)
        yt = torch.tensor(y1, dtype=torch.float32, device=DEVICE)
        net = fit_net(Xt, yt, alpha=alpha_ridge)
        S1  = score_vecs(Xt, yt, net, alpha=alpha_ridge)
        if retro_stability_ok(S1, lam=lam, alpha_stab=alpha_stab):
            print(f"Stable D1 found after {k+1} attempt(s) (seed={seed}).")
            return X1, y1, scaler, net, k+1
    raise RuntimeError(f"No stable D1 found in {max_tries} attempts.")

# ────────────────── 4. demo run ─────────────────────────────────────────────
if __name__ == "__main__":
    X1, y1, scaler, net, n_tries = sample_stable_D1(
        seed0=0,
        n1=100,
        lam=0.01,
        alpha_stab=0.01,
        alpha_ridge=1e-3,
        max_tries=20
    )
    print("Shape of accepted D1:", X1.shape)

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Type-I error verification for Zhang et al. (2023) two-phase MEWMA chart
on the non-linear coupled-oscillator example.

* D₁  : 100 points  – must pass retrospective stability check
* D₂  : 100 points  – provides μ̂ and UCL (99-th percentile ⇒ α ≈ 0.01)
* Σ̂  : covariance of score vectors from D₁ (ridge-regularised, ε = 1e-3)
* Monitoring horizon M = 100
* Empirical FAR is averaged over N_DB × N_future streams.

Dependencies: numpy, scipy, torch, scikit-learn, tqdm, matplotlib
"""

# ───────────────────────── 0. imports & settings ──────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from sklearn.preprocessing import StandardScaler
from scipy.integrate import odeint
from scipy.stats import chi2
import matplotlib.pyplot as plt
from tqdm import tqdm

DEVICE = "cpu"
torch.set_num_threads(1)
plt.rcParams.update({"font.size": 14, "figure.dpi": 150})

# ───────────────────────── 1. oscillator simulator ─────────────────────────
BASE = dict(m1=1, m2=2, k1=1, k2=2, k3=1.5, c1=.1, c2=.2)

def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s
        d, phi = p1 - p2, (p1 - p2)/(1 + abs(p1 - p2))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_ic(seed, n=100, noise_sd=0.03):
    r = np.random.default_rng(seed)
    t  = np.linspace(0, 90, n)
    p1,v1,p2,v2 = simulate(t, [.5,0,-.5,0], BASE).T
    d   = p1 - p2
    phi = d / (1 + np.abs(d))
    X   = np.column_stack([p1, v1, p2, v2])
    y   = (0.5*(BASE['m1']*v1**2 + BASE['m2']*v2**2)
         + 0.5*(BASE['k1']*p1**2 + BASE['k2']*p2**2)
         + BASE['k3']*phi
         + r.normal(0, noise_sd, n))
    return X, y

# ───────────────────────── 2. tiny MLP & helpers ───────────────────────────
class TinyMLP(nn.Module):
    def __init__(self, d_in):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(d_in, 5), nn.ReLU(), nn.Linear(5, 1))
    def forward(self, x): return self.net(x)

def fit_net(X, y, alpha=1e-3, epochs=1_000):
    net = TinyMLP(X.shape[1]).to(DEVICE)
    opt = optim.Adam(net.parameters(), lr=1e-3, weight_decay=alpha)
    loss_fn = nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad()
        loss_fn(net(X).squeeze(), y).backward()
        opt.step()
    return net

def score_vecs(X, y, net, alpha=1e-3):
    """Gradient of penalised MSE wrt final-layer params, one per obs."""
    W, b = list(net.net[-1].parameters())
    loss_fn = nn.MSELoss()
    grads = []
    for xi, yi in zip(X, y):
        with torch.enable_grad():
            xi = xi.unsqueeze(0).detach()
            yi = yi.detach()
            loss = loss_fn(net(xi).squeeze(), yi) + alpha*(W.pow(2).sum() + b.pow(2).sum())
            grad_w, grad_b = torch.autograd.grad(loss, [W, b], retain_graph=False)
            grads.append(torch.cat([grad_w.flatten(), grad_b]))
    return torch.stack(grads).cpu().numpy()

# ───────────────────────── 3. stability check (Step-1) ─────────────────────
def retro_stability_ok(S, lam=0.01, alpha_stab=0.01, eps=1e-3):
    p, mu = S.shape[1], S.mean(0)
    Sigma = np.cov(S.T) + eps*np.eye(p)
    Sigma_z_inv = np.linalg.pinv((lam/(2 - lam))*Sigma, hermitian=True)
    chi2_thr = chi2.ppf(1 - alpha_stab, p)
    z = np.zeros(p)
    for s in S:
        z = lam*s + (1-lam)*z
        if (z-mu) @ Sigma_z_inv @ (z-mu) > chi2_thr:
            return False
    return True

def sample_stable_D1(seed0, n1=100, lam=0.01, alpha_stab=0.01,
                     alpha_ridge=1e-3, max_tries=20):
    """Return (X1, y1, scaler, fitted_net)."""
    for k in range(max_tries):
        seed = seed0 + k
        X1, y1 = gen_ic(seed, n=n1)
        scaler = StandardScaler()
        Xt = torch.tensor(scaler.fit_transform(X1), dtype=torch.float32, device=DEVICE)
        yt = torch.tensor(y1, dtype=torch.float32, device=DEVICE)
        net = fit_net(Xt, yt, alpha=alpha_ridge)
        S1  = score_vecs(Xt, yt, net, alpha=alpha_ridge)
        if retro_stability_ok(S1, lam=lam, alpha_stab=alpha_stab):
            return X1, y1, scaler, net
    raise RuntimeError("Stable D1 not found in max_tries.")

# ───────────────────────── 4. EWMA utilities ───────────────────────────────
def ewma_T2(S, lam, mu, Sigma_inv):
    z, out = np.zeros_like(mu), np.empty(len(S))
    for t, s in enumerate(S):
        z = lam*s + (1-lam)*z
        d = (z - mu)[:, None]
        out[t] = (d.T @ Sigma_inv @ d).item()
    return out

# ───────────────────────── 5. one database replicate ───────────────────────
def process_db(seed, cfg):
    # ------------ Step-1: find stable D1 ------------------------------------
    X1, y1, scaler, net = sample_stable_D1(seed, n1=100, lam=cfg['lam'],
                                           alpha_stab=cfg['alpha_stab'],
                                           alpha_ridge=cfg['alpha_ridge'],
                                           max_tries=cfg['max_tries'])
    Xt1 = torch.tensor(scaler.transform(X1), dtype=torch.float32, device=DEVICE)
    yt1 = torch.tensor(y1, dtype=torch.float32, device=DEVICE)
    S1  = score_vecs(Xt1, yt1, net, alpha=cfg['alpha_ridge'])

    # Σ̂ from D₁ -------------------------------------------------------------
    Sigma_inv = np.linalg.pinv(np.cov(S1.T) + cfg['lambda_cov']*np.eye(S1.shape[1]),
                               hermitian=True)

    # ------------ D2 --------------------------------------------------------
    X2, y2 = gen_ic(seed + 10_000, n=100)          # independent seed
    Xt2 = torch.tensor(scaler.transform(X2), dtype=torch.float32, device=DEVICE)
    yt2 = torch.tensor(y2, dtype=torch.float32, device=DEVICE)
    S2  = score_vecs(Xt2, yt2, net, alpha=cfg['alpha_ridge'])
    mu  = S2.mean(0)

    # UCL from 99-th percentile ---------------------------------------------
    UCL = np.percentile(ewma_T2(S2, cfg['lam'], mu, Sigma_inv), cfg['perc'])

    # ------------ future IC streams ----------------------------------------
    alarms = np.zeros((cfg['N_future'], cfg['M']), dtype=bool)
    for j in range(cfg['N_future']):
        Xf, yf = gen_ic(seed*cfg['N_future'] + j + 123_456, n=cfg['M'])
        Xf_t = torch.tensor(scaler.transform(Xf), dtype=torch.float32, device=DEVICE)
        yf_t = torch.tensor(yf, dtype=torch.float32, device=DEVICE)
        Sf   = score_vecs(Xf_t, yf_t, net, alpha=cfg['alpha_ridge'])
        alarms[j] = ewma_T2(Sf, cfg['lam'], mu, Sigma_inv) > UCL
    return alarms                   # shape (N_future, M)

# ───────────────────────── 6. main experiment ──────────────────────────────
if __name__ == "__main__":
    cfg = dict(
        M          = 100,        # monitoring horizon
        N_future   = 1_000,      # IC streams per DB
        N_DB       = 50,         # database replicates
        lam        = 0.01,       # EWMA smoothing
        perc       = 99.0,       # 99-th percentile ⇒ α ≈ 0.01
        alpha_ridge= 1e-3,       # ridge penalty in MLP
        lambda_cov = 1e-3,       # tiny ridge on Σ̂
        alpha_stab = 0.01,       # χ² stability gate
        max_tries  = 30          # attempts to find stable D1
    )

    all_alarms = []
    for db_seed in tqdm(range(cfg['N_DB']), desc="Database replicates"):
        alarms = process_db(db_seed*97, cfg)   # *97 keeps seeds spaced out
        all_alarms.append(alarms)

    all_alarms = np.asarray(all_alarms)        # shape (N_DB, N_future, M)
    far = all_alarms.mean(axis=(0,1))          # FAR by time index

    # ──────────── plot ─────────────────────────────────────────────────────
    i_vals = np.arange(cfg['M'])
    plt.figure(figsize=(10,6))
    plt.semilogy(i_vals, far, lw=2, label="Empirical FAR")
    plt.axhline(0.01, ls="--", lw=2, c="#d95f02", label="Nominal α = 0.01")
    plt.ylim(1e-3, 1)
    plt.xlabel("Time index $i$")
    plt.ylabel("False-alarm rate (log scale)")
    plt.title("Non-linear oscillator – Zhang two-phase FAR\n"
              "(Σ̂ from stable D₁, α = 0.01)")
    plt.grid(alpha=.3, which="both")
    plt.legend()
    plt.tight_layout()
    plt.show()

# ─────── after `far` has been computed, add ↓↓↓ ──────────
import numpy as np, pathlib, datetime as dt

FAR_FILE = pathlib.Path("far_curve.npy")     # pick any filename

if FAR_FILE.exists():
    print(f"Loading FAR curve from {FAR_FILE}")
    far = np.load(FAR_FILE)                  # `far` is now ready to plot
else:
    # we just finished the expensive simulation; cache the result
    np.save(FAR_FILE, far)
    print(f"Saved FAR curve to {FAR_FILE} at {dt.datetime.now():%Y-%m-%d %H:%M:%S}")

