# -*- coding: utf-8 -*-
"""mixture_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hSUC0ekKuuGpOdQvhOiqDGsMJB-fADUs
"""

# ==============================================================================
# Concept-drift MEWMA chart   (train on single line → monitor 200 IC + 800 OC)
# ==============================================================================

import numpy as np, matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from time import perf_counter

# --------------------------- reproducibility ----------------------------------
RNG = np.random.default_rng(42)

# --------------------------- 1. data generators -------------------------------
def gen_single_linear(m, c, n, noise_sd=3.0, rng=RNG):
    x = rng.uniform(-np.sqrt(3), np.sqrt(3), n)
    eps = rng.normal(0.0, noise_sd, n)
    return x, m * x + c + eps

def gen_mixture_linear(m1, c1, m2, c2, n_each, noise_sd=3.0, rng=RNG):
    x1, y1 = gen_single_linear(m1, c1, n_each, noise_sd, rng)
    x2, y2 = gen_single_linear(m2, c2, n_each, noise_sd, rng)
    return np.concatenate([x1, x2]), np.concatenate([y1, y2])

# --------------------------- 2. score-vector utilities ------------------------
def score_vectors(x, y, model, alpha):
    X_aug = np.hstack([np.ones((x.size, 1)), x.reshape(-1, 1)])
    beta = np.append(model.intercept_, model.coef_)
    err  = -2 * (y - X_aug @ beta)
    reg  = 2 * alpha * np.array([0.0, model.coef_[0]])
    return err[:, None] * X_aug + reg / x.size

def score_transform(x, y, model, alpha, scaler, n_train):
    x_s  = scaler.transform(x.reshape(-1, 1))
    X_aug = np.hstack([np.ones((x_s.shape[0], 1)), x_s])
    beta = np.append(model.intercept_, model.coef_)
    err  = -2 * (y - X_aug @ beta)
    reg  = 2 * alpha * np.array([0.0, model.coef_[0]])
    return err[:, None] * X_aug + reg / n_train

# --------------------------- 3. bootstrap UCL builder -------------------------
def hotelling_bootstrap(x, y, *, BO=50, BI=200, M=1000,
                        alpha_CL=0.001, lamb=0.01,
                        verbose=True, echo_every=5, ridge_eps=1e-4):
    n = x.size
    scaler = StandardScaler().fit(x.reshape(-1, 1))
    Xs     = scaler.transform(x.reshape(-1, 1))

    grid = GridSearchCV(Ridge(), {'alpha': np.logspace(-5, 5, 13)},
                        cv=5, scoring='neg_mean_squared_error')
    grid.fit(Xs, y)
    best_ridge = grid.best_estimator_
    alpha_r    = grid.best_params_['alpha']

    S_tr   = score_vectors(Xs.ravel(), y, best_ridge, alpha_r)
    mean_S = S_tr.mean(0)
    cov_S  = np.cov(S_tr.T) + ridge_eps * np.eye(2)   # <-- ridge here (fix)

    all_T2 = []
    t0 = perf_counter()
    for b in range(BO):
        idx_boot = RNG.choice(n, n, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n), idx_boot)

        model_b = Ridge(alpha=alpha_r).fit(Xs[idx_boot], y[idx_boot])
        S_oob   = score_vectors(Xs[idx_oob].ravel(), y[idx_oob], model_b, alpha_r)
        mu_b    = S_oob.mean(0)
        cov_b   = np.cov(S_oob.T) + ridge_eps * np.eye(2)  # <-- ridge here (fix)
        cov_b_inv = np.linalg.inv(cov_b)

        for _ in range(BI):
            S_sample = S_oob[RNG.choice(S_oob.shape[0], M, replace=True)]
            Z = np.zeros((M, 2))
            for i in range(M):
                Z[i] = lamb * S_sample[i] + (1 - lamb) * (Z[i-1] if i else 0.0)

            i_idx = np.arange(1, M + 1)
            k_num = lamb/(2 - lamb) * (1 - (1 - lamb)**(2 * i_idx)) \
                    + 3.72/n * (1 - (1 - lamb)**i_idx)**2
            k_den = lamb/(2 - lamb) * (1 - (1 - lamb)**(2 * i_idx)) \
                    + 1.0/n * (1 - (1 - lamb)**i_idx)**2
            k = k_num / k_den

            T2 = np.array([
                i * ((Z[i-1] / np.sqrt(k[i-1]) - mu_b) @ cov_b_inv
                     @ (Z[i-1] / np.sqrt(k[i-1]) - mu_b))
                for i in i_idx
            ])
            all_T2.append(T2)

        if verbose and ((b + 1) % echo_every == 0 or b == BO - 1):
            dt = perf_counter() - t0
            print(f"[{dt:6.1f}s]  finished {b + 1:>3}/{BO} outer bootstraps",
                  end="\r" if (b + 1) < BO else "\n")

    all_T2 = np.vstack(all_T2)                 # shape (BO*BI, M)
    CL = np.quantile(all_T2, 1 - alpha_CL, axis=0)
    return CL, mean_S, cov_S, best_ridge, alpha_r, scaler, n

# --------------------------- 4. experiment parameters -------------------------
TRAIN_N          = 3000
IC_MONITOR_N     = 200   # in-control points
OC_MONITOR_N_EACH= 400   # per component → 800 OC points total
BO, BI, M        = 50, 200, 1000
ALPHA_CL         = 0.001
LAMB             = 0.01

# --------------------------- 5. training data ---------------------------------
x_tr, y_tr = gen_single_linear(m=4, c=3, n=TRAIN_N, noise_sd=3.0)

CL, mu_S, cov_S, ridge, alpha_r, scaler, n_train = hotelling_bootstrap(
        x_tr, y_tr, BO=BO, BI=BI, M=M, alpha_CL=ALPHA_CL,
        lamb=LAMB, verbose=True, echo_every=5)

# --------------------------- 6. monitoring stream -----------------------------
#   first 200 points from same distribution as training  → in-control
x_ic, y_ic = gen_single_linear(m=4, c=3, n=IC_MONITOR_N, noise_sd=3.0)

#   next 800 points from mixture  → out-of-control
x_oc, y_oc = gen_mixture_linear(m1=16, c1=5, m2=12, c2=3,
                                n_each=OC_MONITOR_N_EACH, noise_sd=3.0)

x_mon = np.concatenate([x_ic, x_oc])
y_mon = np.concatenate([y_ic, y_oc])

S_mon = score_transform(x_mon, y_mon, ridge, alpha_r, scaler, n_train)

Z = np.zeros_like(S_mon)
for i in range(S_mon.shape[0]):
    Z[i] = LAMB * S_mon[i] + (1 - LAMB) * (Z[i-1] if i else 0.0)

T2_mon = np.array([
    (i + 1) * ((Z[i] - mu_S) @ np.linalg.inv(cov_S) @ (Z[i] - mu_S))
    for i in range(S_mon.shape[0])
])

# --------------------------- 7. plot ------------------------------------------
plt.figure(figsize=(10, 5))
plt.semilogy(T2_mon[:M], label=r"$T_i^2$ statistic")
plt.semilogy(CL, "r--", label=f"UCL (α={ALPHA_CL})")

plt.axvline(IC_MONITOR_N-1, color="k", ls=":", lw=1.2,
            label="start of mixture (out-of-control)")

plt.xlabel("Observation index $i$")
plt.ylabel(r"$T_i^2$  (log scale)")
plt.title("Concept-drift MEWMA chart:\n200 IC points → 800 OC mixture points")
plt.legend()
plt.tight_layout()
plt.show()

# ==============================================================================
# 0.  Imports & reproducibility
# ==============================================================================
import numpy as np, matplotlib.pyplot as plt, seaborn as sns
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from time import perf_counter

sns.set_theme(style="ticks")                    # clean seaborn look
RNG = np.random.default_rng(42)                 # global RNG

# ==============================================================================
# 1.  Data generator
# ==============================================================================
def gen_single_linear(m, c, n, noise_sd=5.0, rng=RNG):
    """y = m·x + c + ε,  x ∼ U[-√3, √3],  ε ∼ N(0, noise_sd²)."""
    x   = rng.uniform(-np.sqrt(3), np.sqrt(3), n)
    eps = rng.normal(0.0, noise_sd, n)
    return x, m * x + c + eps

# ==============================================================================
# 2.  Score-vector utilities
# ==============================================================================
def score_vectors(x, y, model, alpha):
    X_aug = np.hstack([np.ones((x.size, 1)), x.reshape(-1, 1)])
    beta  = np.append(model.intercept_, model.coef_)
    err   = -2 * (y - X_aug @ beta)
    reg   = 2 * alpha * np.array([0.0, model.coef_[0]])
    return err[:, None] * X_aug + reg / x.size

def score_transform(x, y, model, alpha, scaler, n_train):
    xs    = scaler.transform(x.reshape(-1, 1))
    X_aug = np.hstack([np.ones((xs.shape[0], 1)), xs])
    beta  = np.append(model.intercept_, model.coef_)
    err   = -2 * (y - X_aug @ beta)
    reg   = 2 * alpha * np.array([0.0, model.coef_[0]])
    return err[:, None] * X_aug + reg / n_train

# ==============================================================================
# 3.  Vector control-limit bootstrap  (unchanged)
# ==============================================================================
def hotelling_bootstrap(x, y, *, BO=200, BI=400, M=1000,
                        alpha_CL=0.001, lamb=0.01, ridge_eps=1e-4,
                        verbose=True, echo_every=20):
    """
    Returns CL (length-M), mean_S, cov_S, best_ridge, alpha_r, scaler, n_train.
    """
    n      = x.size
    scaler = StandardScaler().fit(x.reshape(-1, 1))
    Xs     = scaler.transform(x.reshape(-1, 1))

    cv = GridSearchCV(Ridge(), {'alpha': np.logspace(-5, 5, 13)},
                      cv=5, scoring='neg_mean_squared_error')
    cv.fit(Xs, y)
    best_ridge = cv.best_estimator_
    alpha_r    = cv.best_params_['alpha']

    S_tr   = score_vectors(Xs.ravel(), y, best_ridge, alpha_r)
    mean_S = S_tr.mean(0)
    cov_S  = np.cov(S_tr.T) + ridge_eps*np.eye(2)

    all_T2 = []
    t0 = perf_counter()
    for b in range(BO):
        idx_boot = RNG.choice(n, n, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n), idx_boot)

        model_b = Ridge(alpha=alpha_r).fit(Xs[idx_boot], y[idx_boot])
        S_oob   = score_vectors(Xs[idx_oob].ravel(), y[idx_oob],
                                model_b, alpha_r)
        mu_b    = S_oob.mean(0)
        cov_b   = np.cov(S_oob.T) + ridge_eps*np.eye(2)
        inv_b   = np.linalg.inv(cov_b)

        for _ in range(BI):
            S_sample = S_oob[RNG.choice(S_oob.shape[0], M, replace=True)]
            Z = np.zeros((M, 2))
            for i in range(M):
                Z[i] = lamb*S_sample[i] + (1-lamb)*(Z[i-1] if i else 0.)

            i_vec = np.arange(1, M+1)
            k_num = lamb/(2-lamb)*(1-(1-lamb)**(2*i_vec)) \
                    + 3.72/n*(1-(1-lamb)**i_vec)**2
            k_den = lamb/(2-lamb)*(1-(1-lamb)**(2*i_vec)) \
                    + 1.0/n*(1-(1-lamb)**i_vec)**2
            k     = k_num / k_den      # --- finite-sample correction

            T2 = np.array([
                i * ((Z[i-1]/np.sqrt(k[i-1]) - mu_b) @ inv_b
                      @ (Z[i-1]/np.sqrt(k[i-1]) - mu_b))
                for i in i_vec
            ])
            all_T2.append(T2)

        if verbose and ((b+1) % echo_every == 0 or b == BO-1):
            dt = perf_counter() - t0
            print(f"[{dt:6.1f}s]  finished {b+1:>3}/{BO} bootstraps",
                  end="\r")
    print()

    CL = np.quantile(np.vstack(all_T2), 1-alpha_CL, axis=0)
    return CL, mean_S, cov_S, best_ridge, alpha_r, scaler, n

# ==============================================================================
# 4.  PFAR Monte-Carlo   **(√k-scaled statistic)**
# ==============================================================================
def estimate_pfar(CL, mean_S, cov_S, model, alpha_r, scaler, n_train,
                  *, M, N_RUNS, lamb, noise_sd, m, c):
    exceed  = np.zeros(M, dtype=int)
    inv_cov = np.linalg.inv(cov_S)

    # --- pre-compute k-vector once, same formula as in bootstrap ----------
    i_vec = np.arange(1, M+1)
    k_num = lamb/(2-lamb)*(1-(1-lamb)**(2*i_vec)) + 3.72/n_train*(1-(1-lamb)**i_vec)**2
    k_den = lamb/(2-lamb)*(1-(1-lamb)**(2*i_vec)) +             1/n_train*(1-(1-lamb)**i_vec)**2
    k_vec = k_num / k_den

    for run in range(N_RUNS):
        x_ic, y_ic = gen_single_linear(m, c, M, noise_sd, rng=RNG)
        S = score_transform(x_ic, y_ic, model, alpha_r, scaler, n_train)

        Z = np.zeros_like(S)
        for i in range(M):
            Z[i] = lamb*S[i] + (1-lamb)*(Z[i-1] if i else 0.)

        T2 = np.array([
            (i+1) * (((Z[i] / np.sqrt(k_vec[i])) - mean_S) @ inv_cov
                     @ ((Z[i] / np.sqrt(k_vec[i])) - mean_S))
            for i in range(M)
        ])

        exceed += T2 >= CL
        if (run+1) % 500 == 0 or run == N_RUNS-1:
            print(f"PFAR run {run+1:>4}/{N_RUNS}", end="\r")
    print()
    return exceed / N_RUNS

# ==============================================================================
# 5.  Hyper-parameters
# ==============================================================================
M          = 1000
ALPHA_CL   = 0.001
LAMBDA     = 0.01
BO, BI     = 100, 400
TRAIN_N    = 2000
N_RUNS     = 10000
NOISE_SD   = 2.0

# ==============================================================================
# 6.  Train & build control limits
# ==============================================================================
x_tr, y_tr = gen_single_linear(m=4, c=3, n=TRAIN_N, noise_sd=NOISE_SD)
CL, mu_S, cov_S, ridge, alpha_r, scaler, n_train = hotelling_bootstrap(
        x_tr, y_tr, BO=BO, BI=BI, M=M, alpha_CL=ALPHA_CL,
        lamb=LAMBDA, verbose=True, echo_every=10)

# ==============================================================================
# 7.  PFAR estimation
# ==============================================================================
pfar = estimate_pfar(CL, mu_S, cov_S, ridge, alpha_r, scaler, n_train,
                     M=M, N_RUNS=N_RUNS, lamb=LAMBDA,
                     noise_sd=NOISE_SD, m=4, c=3)

# ==============================================================================
# 8.  PFAR plot
# ==============================================================================
fig, ax = plt.subplots(figsize=(10, 5))

ax.plot(np.arange(1, M+1), pfar, lw=1.8, color="#0072B2",
        label="Empirical FAR")
ax.axhline(ALPHA_CL, ls="--", lw=2, color="#D55E00",
           label=fr"Nominal level (α = {ALPHA_CL})")

ax.set_xlim(1, M)
ax.set_ylim(0, ALPHA_CL*2)
ax.set_xlabel("Time index")
ax.set_ylabel("False alarm rate")
ax.set_title("Pointwise False-alarm Rate")
ax.grid(ls="--", lw=0.5, alpha=0.7)
ax.legend(frameon=False)
sns.despine()
fig.tight_layout()
plt.show()

fig, ax = plt.subplots(figsize=(10, 5))

ax.plot(np.arange(1, M+1), pfar, lw=1.8, color="#0072B2",
        label="Empirical FAR")
ax.axhline(ALPHA_CL, ls="--", lw=2, color="#D55E00",
           label=fr"Nominal level (α = {ALPHA_CL})")

ax.set_xlim(1, M)
ax.set_ylim(0, ALPHA_CL*100)
ax.set_xlabel("Time index")
ax.set_ylabel("False-alarm rate")
ax.set_title("Point-wise False-alarm Rate")
ax.grid(ls="--", lw=0.5, alpha=0.7)
ax.legend(frameon=False)
sns.despine()
fig.tight_layout()
plt.show()

# ==============================================================================
# 0.  Imports & global defaults
# ==============================================================================
import numpy as np, seaborn as sns, matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from time import perf_counter

sns.set_theme(style="ticks")
plt.rcParams.update({"axes.titlesize": 18,
                     "axes.labelsize": 14,
                     "legend.fontsize": 12})

RNG = np.random.default_rng(4)            # reproducibility

# ==============================================================================
# 1.  Data-generation helpers
# ==============================================================================
def gen_single_linear(m, c, n, noise_sd=3.0, rng=RNG):
    x = rng.uniform(-np.sqrt(3), np.sqrt(3), n)
    eps = rng.normal(0.0, noise_sd, n)
    return x, m * x + c + eps

def gen_mixture_linear(m1, c1, m2, c2, n_each, noise_sd=3.0, rng=RNG):
    x1, y1 = gen_single_linear(m1, c1, n_each, noise_sd, rng)
    x2, y2 = gen_single_linear(m2, c2, n_each, noise_sd, rng)
    return np.concatenate([x1, x2]), np.concatenate([y1, y2])

# ==============================================================================
# 2.  Score-vector utilities
# ==============================================================================
def score_vectors(x, y, model, alpha):
    X_aug = np.hstack([np.ones((x.size, 1)), x.reshape(-1, 1)])
    beta  = np.append(model.intercept_, model.coef_)
    err   = -2 * (y - X_aug @ beta)
    reg   = 2 * alpha * np.array([0.0, model.coef_[0]])
    return err[:, None] * X_aug + reg / x.size

def score_transform(x, y, model, alpha, scaler, n_train):
    xs = scaler.transform(x.reshape(-1, 1))
    X_aug = np.hstack([np.ones((xs.shape[0], 1)), xs])
    beta  = np.append(model.intercept_, model.coef_)
    err   = -2 * (y - X_aug @ beta)
    reg   = 2 * alpha * np.array([0.0, model.coef_[0]])
    return err[:, None] * X_aug + reg / n_train

# ==============================================================================
# 3.  Bootstrap constructor for CL (vector length M)
# ==============================================================================
def hotelling_bootstrap(x, y, *, BO=100, BI=400, M=1000,
                        alpha_CL=0.001, lamb=0.01, ridge_eps=1e-4,
                        verbose=True, echo_every=20):
    n = x.size
    scaler = StandardScaler().fit(x.reshape(-1, 1))
    Xs     = scaler.transform(x.reshape(-1, 1))

    cv = GridSearchCV(Ridge(), {'alpha': np.logspace(-5, 5, 13)},
                      cv=5, scoring='neg_mean_squared_error')
    cv.fit(Xs, y)
    best_ridge = cv.best_estimator_
    alpha_r    = cv.best_params_['alpha']

    S_tr   = score_vectors(Xs.ravel(), y, best_ridge, alpha_r)
    mean_S = S_tr.mean(0)
    cov_S  = np.cov(S_tr.T) + ridge_eps * np.eye(2)

    all_T2 = []
    t0 = perf_counter()
    for b in range(BO):
        idx_boot = RNG.choice(n, n, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n), idx_boot)

        model_b = Ridge(alpha=alpha_r).fit(Xs[idx_boot], y[idx_boot])
        S_oob   = score_vectors(Xs[idx_oob].ravel(), y[idx_oob], model_b, alpha_r)
        mu_b    = S_oob.mean(0)
        cov_b   = np.cov(S_oob.T) + ridge_eps * np.eye(2)
        cov_b_inv = np.linalg.inv(cov_b)

        for _ in range(BI):
            S_sample = S_oob[RNG.choice(S_oob.shape[0], M, replace=True)]
            Z = np.zeros((M, 2))
            for i in range(M):
                Z[i] = lamb * S_sample[i] + (1 - lamb) * (Z[i-1] if i else 0.0)

            i_idx = np.arange(1, M + 1)
            k_num = lamb/(2 - lamb) * (1 - (1 - lamb)**(2 * i_idx)) \
                    + 3.72/n * (1 - (1 - lamb)**i_idx)**2
            k_den = lamb/(2 - lamb) * (1 - (1 - lamb)**(2 * i_idx)) \
                    + 1.0/n * (1 - (1 - lamb)**i_idx)**2
            k = k_num / k_den

            T2 = np.array([
                i * ((Z[i-1] / np.sqrt(k[i-1]) - mu_b) @ cov_b_inv
                     @ (Z[i-1] / np.sqrt(k[i-1]) - mu_b))
                for i in i_idx
            ])
            all_T2.append(T2)

        if verbose and ((b + 1) % echo_every == 0 or b == BO - 1):
            dt = perf_counter() - t0
            print(f"[{dt:6.1f}s]  finished {b + 1:>3}/{BO} outer bootstraps",
                  end="\r" if (b + 1) < BO else "\n")

    all_T2 = np.vstack(all_T2)
    CL = np.quantile(all_T2, 1 - alpha_CL, axis=0)
    return CL, mean_S, cov_S, best_ridge, alpha_r, scaler, n

# ==============================================================================
# 4.  PARAMETERS
# ==============================================================================
# -- data for Fig. 1 (scatter) ------------------------------------
N_total   = 50_000
N_sample  = 300
noise_sd  = 3.0
m_pre, c_pre = 4, 3
m1, c1, m2, c2 = 16, 5, 12, 3

# -- control-chart settings --------------------------------------
TRAIN_N          = 2000
IC_MONITOR_N     = 200
OC_MONITOR_N_EACH= 400
M                = 1_000
ALPHA_CL         = 0.001
LAMBDA           = 0.01
BO, BI           = 100, 400

# ==============================================================================
# 5-A.  FIGURE 1  (scatter with two ridge fits)
# ==============================================================================
# full populations
x_pre_full,  y_pre_full  = gen_single_linear(m_pre, c_pre, N_total,  noise_sd)
x_post1, y_post1 = gen_single_linear(m1, c1, N_total, noise_sd)
x_post2, y_post2 = gen_single_linear(m2, c2, N_total, noise_sd)
x_post_full = np.concatenate([x_post1, x_post2])
y_post_full = np.concatenate([y_post1, y_post2])

# subsample for display
idx_pre  = RNG.choice(N_total,         N_sample, replace=False)
idx_post = RNG.choice(x_post_full.size, N_sample, replace=False)
x_pre,  y_pre  = x_pre_full[idx_pre],  y_pre_full[idx_pre]
x_post, y_post = x_post_full[idx_post], y_post_full[idx_post]

ridge_pre  = Ridge(alpha=1e-6).fit(x_pre.reshape(-1, 1),  y_pre)
ridge_post = Ridge(alpha=1e-6).fit(x_post.reshape(-1, 1), y_post)

fig1, ax1 = plt.subplots(figsize=(10, 6))
ax1.scatter(x_pre,  y_pre,  s=40, c="#0072B2", alpha=0.55,
            edgecolors="w", linewidth=0.4, label="Pre-shift data")
ax1.scatter(x_post, y_post, s=40, c="#D55E00", alpha=0.55,
            edgecolors="w", linewidth=0.4, label="Post-shift data")

xg = np.linspace(-np.sqrt(3), np.sqrt(3), 500)
ax1.plot(xg, ridge_pre.predict(xg.reshape(-1, 1)),
         color="#0057A8", lw=2.2,
         label=fr"Fitted (pre):  $y={ridge_pre.coef_[0]:.2f}x$ + {ridge_pre.intercept_:5.2f}")
ax1.plot(xg, ridge_post.predict(xg.reshape(-1, 1)),
         color="#B34000", lw=2.2,
         label=fr"Fitted (post): $y={ridge_post.coef_[0]:.2f}x$ + {ridge_post.intercept_:5.2f}")

ax1.set_title("Sampled datasets with fitted Ridge-regression models")
ax1.set_xlabel("$x$")
ax1.set_ylabel("$y$")
ax1.legend(frameon=False)
sns.despine()
fig1.tight_layout()

# ==============================================================================
# 5-B.  CONTROL LIMITS  &  FIGURE 2
# ==============================================================================
x_train, y_train = gen_single_linear(m_pre, c_pre, TRAIN_N, noise_sd)
CL, mu_S, cov_S, ridge, alpha_r, scaler, n_train = hotelling_bootstrap(
        x_train, y_train, BO=BO, BI=BI, M=M, alpha_CL=ALPHA_CL,
        lamb=LAMBDA, verbose=True, echo_every=10)

# monitoring stream
x_ic, y_ic = gen_single_linear(m_pre, c_pre, IC_MONITOR_N, noise_sd)
x_oc, y_oc = gen_mixture_linear(m1, c1, m2, c2, OC_MONITOR_N_EACH, noise_sd)
x_mon = np.concatenate([x_ic, x_oc])
y_mon = np.concatenate([y_ic, y_oc])

S_mon = score_transform(x_mon, y_mon, ridge, alpha_r, scaler, n_train)
Z = np.zeros_like(S_mon)
for i in range(S_mon.shape[0]):
    Z[i] = LAMBDA * S_mon[i] + (1 - LAMBDA) * (Z[i-1] if i else 0.0)

T2_mon = np.array([
    (i + 1) * ((Z[i] - mu_S) @ np.linalg.inv(cov_S) @ (Z[i] - mu_S))
    for i in range(S_mon.shape[0])
])

# --- plotting ---------------------------------------------------------------
fig2, ax = plt.subplots(figsize=(10, 5))

ax.semilogy(T2_mon[:M], color="#0072B2", lw=1.8, label=r"$T_i^2$ statistic")
ax.semilogy(CL,        ls="--", color="#D55E00", lw=2,
            label=fr"UCL  $(\alpha = {ALPHA_CL})$")

# mask & shade alarms
over = T2_mon[:M] > CL
ax.fill_between(np.arange(M), T2_mon[:M], CL,
                where=over, color="#0072B2", alpha=0.10)

# change-point line and region shading
cp = IC_MONITOR_N - 1
ax.axvline(cp, color="#555555", ls="-.", lw=1.5, label="change-point")
ax.axvspan(0, cp,          color="#0072B2", alpha=0.04)
ax.axvspan(cp + 1, M,      color="#D55E00", alpha=0.04)

ax.set_xlabel("Sample index $i$")
ax.set_ylabel(r"$T_i^2$  (log scale)")
ax.set_title("EWMA control chart with predictive-relationship shift")
ax.legend(frameon=False)
ax.grid(which="both", ls=":", lw=0.5, alpha=0.6)
sns.despine()
fig2.tight_layout()

plt.show()

# find first crossing (if any) within the horizon M
crossings = np.flatnonzero(T2_mon[:M] > CL)
alarm_idx = crossings[0] if crossings.size else None
print("First crossing (alarm) at i =", alarm_idx)

"""
Concept-drift illustration
  ▸ Pre-shift / training data  : single line  y = 16 x + 5
  ▸ Post-shift streaming data : 50 % same line  (16 x+5)
                                 50 % new line   (12 x+3)
"""
# ════════════════════════════════════════════════════════════════════════════
# 0. Imports & global style
# ════════════════════════════════════════════════════════════════════════════
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge

RNG = np.random.default_rng(42)

plt.rcParams.update({
    "font.size"       : 14,
    "axes.titlesize"  : 18,
    "axes.labelsize"  : 16,
    "legend.fontsize" : 14,
    "xtick.labelsize" : 12,
    "ytick.labelsize" : 12,
    "figure.dpi"      : 150,
})

# ════════════════════════════════════════════════════════════════════════════
# 1.  Line parameters & noise
# ════════════════════════════════════════════════════════════════════════════
# --- curve (4.1) -------------------------------------------------------------
m1, c1 = 16, 5          # y = 16x + 5   (used for ALL pre-shift data)

# --- curve (4.2) -------------------------------------------------------------
m2, c2 = 12, 3          # y = 12x + 3   (only appears post-shift)

mix_prob      = 0.50    # 50-50 mixture after the shift
noise_sd      = 4.0     # same σ for every observation

N_FULL   = 50_000       # generate large pools, then sample for plotting
N_SAMPLE = 300          # visible points from each regime

# helper: x ∼ U[−√3,√3] so  Var(x)=1
def gen_x(n, rng=RNG):
    return rng.uniform(-np.sqrt(3), np.sqrt(3), n)

# ════════════════════════════════════════════════════════════════════════════
# 2.  Generate populations
# ════════════════════════════════════════════════════════════════════════════
# ----- pre-shift -------------------------------------------------------------
x_pre_full = gen_x(N_FULL)
eps_pre    = RNG.normal(0., noise_sd, N_FULL)
y_pre_full = m1 * x_pre_full + c1 + eps_pre    # only curve (4.1)

# ----- post-shift mixture ----------------------------------------------------
x_post_full = gen_x(N_FULL)
eps_post    = RNG.normal(0., noise_sd, N_FULL)
use_curve1  = RNG.random(N_FULL) < mix_prob       # Bernoulli(0.5)

y_post_full = np.where(
    use_curve1,
    m1 * x_post_full + c1 + eps_post,            # curve (4.1)
    m2 * x_post_full + c2 + eps_post,            # curve (4.2)
)

# ════════════════════════════════════════════════════════════════════════════
# 3.  Sub-sample for illustration
# ════════════════════════════════════════════════════════════════════════════
idx_pre  = RNG.choice(N_FULL, N_SAMPLE, replace=False)
idx_post = RNG.choice(N_FULL, N_SAMPLE, replace=False)

x_pre , y_pre  = x_pre_full [idx_pre ], y_pre_full [idx_pre ]
x_post, y_post = x_post_full[idx_post], y_post_full[idx_post]

# ════════════════════════════════════════════════════════════════════════════
# 4.  Fit separate Ridge models (α≈0 → OLS)
# ════════════════════════════════════════════════════════════════════════════
ridge_pre  = Ridge(alpha=0.0).fit(x_pre.reshape (-1,1), y_pre )
ridge_post = Ridge(alpha=0.0).fit(x_post.reshape(-1,1), y_post)

x_grid = np.linspace(min(x_pre.min(), x_post.min()),
                     max(x_pre.max(), x_post.max()), 500)
y_pred_pre  = ridge_pre .predict(x_grid.reshape(-1,1))
y_pred_post = ridge_post.predict(x_grid.reshape(-1,1))

# ════════════════════════════════════════════════════════════════════════════
# 5.  Plot
# ════════════════════════════════════════════════════════════════════════════
fig, ax = plt.subplots(figsize=(10,6))

# scatter points
ax.scatter(x_pre , y_pre , color="#1f77b4", alpha=0.65, label='Pre-shift data')
ax.scatter(x_post, y_post, color="#d95f02", alpha=0.65, label='Post-shift data')

# fitted lines
ax.plot(x_grid, y_pred_pre , color="#1f77b4", lw=2.5,
        label=fr'Fitted (pre):  $y={ridge_pre.coef_[0]:.2f}x+{ridge_pre.intercept_:.2f}$')


# true underlying lines (dashed)
ax.plot(x_grid, m1*x_grid + c1, ls="--", color="#1f77b4", lw=1.8,
        label=r'True curve (4.1)')


ax.set_title('Sampled Datasets with Fitted Ridge Regression Model on Pre-shift Data')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.grid(alpha=0.3)
ax.legend(loc="upper left")

plt.tight_layout()
plt.show()

"""
EWMA concept-drift illustration
───────────────────────────────
• Training & in-control (IC) data :  y = 16 x + 5  + ε      ← curve (4.1)
• Out-of-control (OC) data (50 / 50 mixture)
        y = 16 x + 5 + ε        or        y = 12 x + 3 + ε  ← curves (4.1) & (4.2)
"""

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 0. Imports & global styling
# ╚═══════════════════════════════════════════════════════════════════════════╝
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

RNG = np.random.default_rng(56)

plt.rcParams.update({
    "font.size"       : 14,
    "axes.titlesize"  : 18,
    "axes.labelsize"  : 16,
    "legend.fontsize" : 14,
    "xtick.labelsize" : 12,
    "ytick.labelsize" : 12,
    "figure.dpi"      : 150,
})


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 1. Helper: score-vector for (ridge) linear regression
# ╚═══════════════════════════════════════════════════════════════════════════╝
def score_vectors(X, y, model, alpha):
    """
    ∂ℓ/∂β for ridge regression: (XᵀX + αI)β̂ = Xᵀy
      s_i = −2 (y_i − x_iᵀ β̂) x_i  + 2α β̂ / n
      (intercept handled by an added 1-column)
    """
    X_aug = np.hstack([np.ones((X.shape[0], 1)), X])
    β_hat = np.append(model.intercept_, model.coef_)
    resid = y - X_aug @ β_hat
    reg_term = 2 * alpha * np.append(0, model.coef_) / X.shape[0]
    return (-2 * resid)[:, None] * X_aug + reg_term


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 2. Data generators
# ╚═══════════════════════════════════════════════════════════════════════════╝
def gen_single_line(m=16, c=5, n=50_000, noise_sd=4.0):     # ← curve (4.1)
    x = RNG.uniform(-np.sqrt(3), np.sqrt(3), n)             # Var(x)=1
    ε = RNG.normal(0.0, noise_sd, n)
    y = m * x + c + ε
    return x, y


def gen_mixture(m1=16, c1=5, m2=12, c2=3, n=50_000, noise_sd=4.0, π=0.5):
    x = RNG.uniform(-np.sqrt(3), np.sqrt(3), n)
    ε = RNG.normal(0.0, noise_sd, n)
    mask = RNG.random(n) < π                                # choose component
    y = np.where(mask, m1 * x + c1 + ε, m2 * x + c2 + ε)
    return x, y


def prepare_monitoring_data(n_train=2_000, n_ic=200, n_oc=800):
    # training & IC from curve (4.1)
    x_train_full, y_train_full = gen_single_line()
    x_ic_full,  y_ic_full      = gen_single_line()

    # OC from 50-50 mixture of curves (4.1) & (4.2)
    x_oc_full, y_oc_full = gen_mixture()

    # slice to requested sizes
    x_train, y_train = x_train_full[:n_train], y_train_full[:n_train]
    x_ic,    y_ic    = x_ic_full [:n_ic   ], y_ic_full [:n_ic]
    x_oc,    y_oc    = x_oc_full [:n_oc   ], y_oc_full [:n_oc]

    # concatenate monitoring stream
    x_monitor = np.concatenate([x_ic, x_oc])
    y_monitor = np.concatenate([y_ic, y_oc])

    return (x_train, y_train), (x_monitor, y_monitor), n_ic


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 3. EWMA utility functions
# ╚═══════════════════════════════════════════════════════════════════════════╝
def ewma_T2(stats, λ, μ, Σ_inv, n_train):
    z = np.zeros_like(μ)
    T2 = np.empty(len(stats))
    for t, s in enumerate(stats):
        z = λ * s + (1 - λ) * z
        diff = (z - μ)[:, None]
        T2[t] = (diff.T @ Σ_inv @ diff).item()
    return T2


def ewma_T2_boot(stats, λ, μ, Σ_inv, n_train):
    z = np.zeros_like(μ)
    T2 = np.empty(len(stats))
    for t, s in enumerate(stats, start=1):  # 1-based index
        z = λ * s + (1 - λ) * z

        num = (λ/(2-λ)) * (1 - (1-λ)**(2*t)) + (3.72 / n_train) * (1 - (1-λ)**t)**2
        den = (λ/(2-λ)) * (1 - (1-λ)**(2*t)) + (1    / n_train) * (1 - (1-λ)**t)**2
        k   = np.sqrt(num / den)

        diff = (z / k - μ)[:, None]
        T2[t-1] = (diff.T @ Σ_inv @ diff).item()
    return T2


def bootstrap_ucl(x_train, y_train, *, α_ridge, λ_EWMA,
                  λ_cov, B_outer, B_inner, M, perc):
    scaler = StandardScaler()
    X_train = scaler.fit_transform(x_train[:, None])
    base_model = Ridge(alpha=α_ridge).fit(X_train, y_train)

    S_train = score_vectors(X_train, y_train, base_model, α_ridge)
    μ  = S_train.mean(axis=0)
    Σ = np.cov(S_train.T) + λ_cov * np.eye(S_train.shape[1])
    Σ_inv = np.linalg.pinv(Σ, hermitian=True)

    n_train = len(x_train)
    res = np.empty((B_outer * B_inner, M))

    for b in range(B_outer):
        idx_boot = RNG.choice(n_train, n_train, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n_train), idx_boot)

        X_b, y_b = x_train[idx_boot], y_train[idx_boot]
        X_oob, y_oob = x_train[idx_oob], y_train[idx_oob]

        X_b_scaled = scaler.fit_transform(X_b[:, None])
        model_b = Ridge(alpha=α_ridge).fit(X_b_scaled, y_b)

        X_oob_scaled = scaler.transform(X_oob[:, None])
        S_oob = score_vectors(X_oob_scaled, y_oob, model_b, α_ridge)

        for j in range(B_inner):
            idx = RNG.choice(len(S_oob), M, replace=True)
            T2 = ewma_T2_boot(S_oob[idx], λ_EWMA, μ, Σ_inv, n_train)
            res[b * B_inner + j] = T2

    return np.percentile(res, perc, axis=0)


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 4. Main routine
# ╚═══════════════════════════════════════════════════════════════════════════╝
def run_monitoring_example():
    # ——— user-tunable ——————————————————————————————
    α_ridge   = 0.01
    λ_EWMA    = 0.003
    λ_cov     = 1e-1
    B_outer   = 100
    B_inner   = 200
    M         = 1_000
    perc_ucl  = 99.9            # ⇒ α = 0.001
    # ————————————————————————————————————————————————

    (x_tr, y_tr), (x_mon, y_mon), n_ic = prepare_monitoring_data()
    scaler = StandardScaler()
    X_tr = scaler.fit_transform(x_tr[:, None])
    model = Ridge(alpha=α_ridge).fit(X_tr, y_tr)

    S_tr = score_vectors(X_tr, y_tr, model, α_ridge)
    μ  = S_tr.mean(axis=0)
    Σ = np.cov(S_tr.T) + λ_cov * np.eye(S_tr.shape[1])
    Σ_inv = np.linalg.pinv(Σ, hermitian=True)

    print("Bootstrapping UCL …")
    UCL = bootstrap_ucl(x_tr, y_tr, α_ridge=α_ridge, λ_EWMA=λ_EWMA,
                        λ_cov=λ_cov, B_outer=B_outer, B_inner=B_inner,
                        M=M, perc=perc_ucl)

    X_mon = scaler.transform(x_mon[:, None])
    S_mon = score_vectors(X_mon, y_mon, model, α_ridge)
    T2    = ewma_T2(S_mon, λ_EWMA, μ, Σ_inv, len(x_tr))

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(T2,  color='#1f77b4', label=r'$T^{2}$ statistic')
    ax.plot(UCL, '-.',  lw=2, color='#d95f02',
        label=r'UCL  ($\alpha = 0.001$)')   # dash–dot–dash style


    ax.axvline(n_ic-0.5, color='k', lw=1.4, ls='--', label='change-point')
    ax.axvspan(0, n_ic,  facecolor='#1f77b4', alpha=0.05)
    ax.axvspan(n_ic, M,  facecolor='#d95f02', alpha=0.05)

    ax.set_yscale('log')
    ax.set_xlabel('Sample index $i$')
    ax.set_ylabel(r'$T^{2}$ (log scale)')
    ax.set_title('EWMA Control Chart with Predictive-relationship Shift')
    ax.grid(alpha=0.3, which='both')
    ax.legend()
    plt.tight_layout()
    plt.show()

    return T2, UCL


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 5. Run
# ╚═══════════════════════════════════════════════════════════════════════════╝
if __name__ == "__main__":
    run_monitoring_example()

# ── first-passage time (FPT)  ────────────────────────────────────────────────
# choose how much of the stream to ignore (e.g. skip warm-up IC observations)
search_start = 0          # put n_ic here if you want to ignore the IC segment

fpt = next(                                                  # first index i …
         (i for i, (t2, ucl)                                 #   … where T2>UCL
          in enumerate(zip(T2_stats[search_start:],
                            UCL_curve[search_start:]),
                       start=search_start)
          if t2 > ucl),
         None)                                               # None ⇒ no alarm

if fpt is None:
    print("No alarm raised within the monitoring horizon.")
else:
    print(f"First alarm (first-passage time) at i = {fpt}")

"""
Linear-model score-based EWMA — type-I error verification
( pointwise False–Alarm Rate ≈ nominal α = 0.001 )
"""

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 0. Imports & global style
# ╚═══════════════════════════════════════════════════════════════════════════╝
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
from tqdm import tqdm

# --- publication fonts / colours -------------------------------------------
plt.rcParams.update({
    "font.size"       : 14,
    "axes.titlesize"  : 18,
    "axes.labelsize"  : 16,
    "legend.fontsize" : 14,
    "xtick.labelsize" : 12,
    "ytick.labelsize" : 12,
    "figure.dpi"      : 150
})

RNG = np.random.default_rng()


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 1.  Data generator
# ╚═══════════════════════════════════════════════════════════════════════════╝
# ─────────────────────────────────────────────────────────────────────────────
#  HEADLINE  (for plots / paper caption)
# ─────────────────────────────────────────────────────────────────────────────
"""
Linear-model score-based EWMA — type-I error verification
( pointwise False-Alarm Rate ≈ nominal α = 0.001 )

Set-up
  • Training & IC data     :  y = 16 x + 5 + ε              ← curve (4.1)
  • OC data for “power”    :  50 %  y = 16 x + 5 + ε
                              50 %  y = 12 x + 3 + ε        ← curve (4.2)
    (For FAR studies we keep case='null', therefore every stream is IC.)
"""

# … imports & plotting style unchanged …


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 1. Data generator  (adapted slopes & intercepts)
# ╚═══════════════════════════════════════════════════════════════════════════╝
def generate_data(seed, n_train, n_ic, n_oc,
                  m_ic=16,  c_ic=5,                 # ← curve (4.1)
                  m1=16, c1=5,                      # component 1   (4.1)
                  m2=12, c2=3,                      # component 2   (4.2)
                  case='null'):
    """
    • In-control (training & IC monitoring):
          y = m_ic · x + c_ic + ε          (here 16x + 5)
    • OC stream for power studies (case!='null'):
          50 %  y = m1 · x + c1 + ε        (16x + 5)
          50 %  y = m2 · x + c2 + ε        (12x + 3)
    • For type-I/FAR runs we pass case='null' so the mixture is *not* used.
    """
    rng = np.random.default_rng(seed)

    # training sample
    x_tr = rng.uniform(-np.sqrt(3), np.sqrt(3), n_train)
    y_tr = m_ic * x_tr + c_ic + rng.normal(0, 1, n_train)

    # in-control monitoring slice
    x_ic = rng.uniform(-np.sqrt(3), np.sqrt(3), n_ic)
    y_ic = m_ic * x_ic + c_ic + rng.normal(0, 1, n_ic)

    # out-of-control slice (only if we later set case!='null')
    x_oc = rng.uniform(-np.sqrt(3), np.sqrt(3), n_oc)
    if case == 'null':
        y_oc = m_ic * x_oc + c_ic + rng.normal(0, 1, n_oc)
    else:
        mask = rng.random(n_oc) < 0.5
        ε    = rng.normal(0, 1, n_oc)
        y_oc = np.where(mask,
                        m1 * x_oc + c1 + ε,   # curve (4.1)
                        m2 * x_oc + c2 + ε)   # curve (4.2)

    # concatenate IC + OC to form full monitoring stream
    x_mon = np.concatenate([x_ic, x_oc])
    y_mon = np.concatenate([y_ic, y_oc])

    return (x_tr, y_tr), (x_mon, y_mon), n_ic



# ╔═══════════════════════════════════════════════════════════════════════════╗
# 2.  Score vectors and EWMA helpers
# ╚═══════════════════════════════════════════════════════════════════════════╝
def score_vectors(X, y, model, alpha):
    X_aug = np.hstack([np.ones((X.shape[0], 1)), X])
    β     = np.append(model.intercept_, model.coef_)
    resid = y - X_aug @ β
    reg   = 2 * alpha * np.append(0, model.coef_) / X.shape[0]
    return (-2 * resid)[:, None] * X_aug + reg


def ewma_stats_no_k(S, lam, μ, Σ_inv):
    z = np.zeros_like(μ)
    out = np.empty(len(S))
    for t, s in enumerate(S):
        z = lam * s + (1-lam) * z
        diff = (z - μ)[:, None]
        out[t] = (diff.T @ Σ_inv @ diff).item()
    return out


def ewma_stats_with_k(S, lam, μ, Σ_inv, n_train):
    z = np.zeros_like(μ)
    out = np.empty(len(S))
    for t, s in enumerate(S, start=1):
        z = lam * s + (1-lam) * z
        num = (lam/(2-lam)) * (1 - (1-lam)**(2*t)) + (3.72/n_train) * (1-(1-lam)**t)**2
        den = (lam/(2-lam)) * (1 - (1-lam)**(2*t)) + (1   /n_train)  * (1-(1-lam)**t)**2
        k   = np.sqrt(num / den)
        diff = (z / k - μ)[:, None]
        out[t-1] = (diff.T @ Σ_inv @ diff).item()
    return out


def bootstrap_ucl(x_tr, y_tr, *, lam, alpha, λ_cov, BO, BI, M, perc):
    scaler = StandardScaler()
    Xtr = scaler.fit_transform(x_tr[:, None])
    model = Ridge(alpha=alpha).fit(Xtr, y_tr)

    S_tr = score_vectors(Xtr, y_tr, model, alpha)
    μ  = S_tr.mean(axis=0)
    Σ  = np.cov(S_tr.T) + λ_cov * np.eye(S_tr.shape[1])
    Σi = np.linalg.pinv(Σ, hermitian=True)

    n = len(x_tr)
    res = np.empty((BO*BI, M))

    for b in range(BO):
        idx_boot = RNG.choice(n, n, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n), idx_boot)

        X_b   = scaler.fit_transform(x_tr[idx_boot, None])
        y_b   = y_tr[idx_boot]
        X_oob = scaler.transform(x_tr[idx_oob, None])
        y_oob = y_tr[idx_oob]

        model_b = Ridge(alpha=alpha).fit(X_b, y_b)
        S_oob   = score_vectors(X_oob, y_oob, model_b, alpha)

        for j in range(BI):
            S_draw = S_oob[RNG.choice(len(S_oob), M, replace=True)]
            res[b*BI + j] = ewma_stats_with_k(S_draw, lam, μ, Σi, n)

    return np.percentile(res, perc, axis=0)


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 3.  Single-database replicate
# ╚═══════════════════════════════════════════════════════════════════════════╝
def process_db(seed, cfg):
    (x_tr, y_tr), _, _ = generate_data(seed,
                                       n_train=cfg['n_train'],
                                       n_ic=cfg['n_ic'],
                                       n_oc=cfg['n_oc'])
    scaler = StandardScaler()
    Xtr = scaler.fit_transform(x_tr[:, None])
    model = Ridge(alpha=cfg['alpha']).fit(Xtr, y_tr)

    S_tr = score_vectors(Xtr, y_tr, model, cfg['alpha'])
    μ  = S_tr.mean(axis=0)
    Σ  = np.cov(S_tr.T) + cfg['λ_cov'] * np.eye(S_tr.shape[1])
    Σi = np.linalg.pinv(Σ, hermitian=True)

    # UCL (once per DB replicate)
    UCL = bootstrap_ucl(x_tr, y_tr, lam=cfg['lam'],
                        alpha=cfg['alpha'], λ_cov=cfg['λ_cov'],
                        BO=cfg['BO'], BI=cfg['BI'], M=cfg['M'], perc=cfg['perc'])

    # --- future replicates under the *null* -------------------------------
    exc = np.zeros((cfg['N_future'], cfg['M']))
    for j in range(cfg['N_future']):
        (_, (x_mon, y_mon), _) = generate_data(seed*cfg['N_future']+j,
                                               n_train=cfg['n_train'],
                                               n_ic=cfg['n_ic'],
                                               n_oc=cfg['n_oc'],
                                               case='null')
        X_mon = scaler.transform(x_mon[:, None])
        S_mon = score_vectors(X_mon, y_mon, model, cfg['alpha'])[:cfg['M']]
        T2    = ewma_stats_no_k(S_mon, cfg['lam'], μ, Σi)
        exc[j] = T2 > UCL
    return exc


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 4.  FAR evaluation driver
# ╚═══════════════════════════════════════════════════════════════════════════╝
def evaluate_far(cfg, N_db=20, workers=None):
    if workers is None:
        workers = max(1, multiprocessing.cpu_count() - 1)

    all_ex = []
    with ProcessPoolExecutor(max_workers=workers) as ex:
        futs = [ex.submit(process_db, seed, cfg) for seed in range(N_db)]
        for _ in tqdm(as_completed(futs), total=N_db, desc="DB replicates"):
            all_ex.append(_.result())

    all_ex = np.array(all_ex)                     # (N_db, N_future, M)
    far    = all_ex.mean(axis=(0, 1))
    return pd.DataFrame({"t": np.arange(cfg['M']), "FAR": far})


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 5.  Plot helper
# ╚═══════════════════════════════════════════════════════════════════════════╝
def plot_far(df, α_nom=0.001):
    plt.figure(figsize=(10, 6))
    plt.plot(df.t, df.FAR, lw=2, label="Empirical FAR", color='#1f77b4')
    plt.axhline(α_nom, ls='--', lw=2, color='#d95f02', label=f"Nominal α={α_nom}")
    plt.ylim(0, α_nom*2)
    plt.xlabel("Time Index $i$")
    plt.ylabel("False Alarm Rate")
    plt.title("Pointwise FAR")
    plt.grid(alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 6.  Main run  (adjust BO/BI downward if you only want a smoke-test)
# ╚═══════════════════════════════════════════════════════════════════════════╝
if __name__ == "__main__":
    cfg = dict(
        # data sizes
        n_train = 2_000,
        n_ic    = 200,
        n_oc    = 800,
        M       = 1_000,

        # EWMA / model
        lam     = 0.01,     # EWMA λ
        alpha   = 0.01,     # ridge L2
        λ_cov   = 1e-2,      # Σ regularisation

        # bootstrap
        BO   = 100,
        BI   = 200,
        perc = 99.9,        # ⇒ nominal α = 0.001

        # future reps per DB
        N_future = 1_000,
    )

    print("\nEvaluating empirical FAR …")
    df_far = evaluate_far(cfg, N_db=100)           # increase N_db for tighter CIs
    plot_far(df_far)

arr_far = df_far.to_numpy()[:,1]

print(arr_far)

"""
Kungang Zhang et al. (2023) – type-I error verification
-------------------------------------------------------
Phase-I split: 50 % for model fitting (D₁) + 50 % for UCL estimation (D₂)

 • In-control process:  y = 16 x + 5 + ε,   ε ∼ N(0,1)
 • EWMA on score vectors, λ = 0.01
 • Constant UCL = 99.9-th percentile of T² sequence on D₂  (α ≈ 0.001)
 • FAR estimated from many future IC replicates
"""

# ─────────────────────────────────── 0. imports ────────────────────────────
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm, trange
import multiprocessing

plt.rcParams.update({
    "font.size":14, "axes.titlesize":18, "axes.labelsize":16,
    "legend.fontsize":14, "xtick.labelsize":12, "ytick.labelsize":12,
    "figure.dpi":150
})
RNG = np.random.default_rng()

# ───────────────────── 1. IC data generator – curve (4.1) ───────────────────
def generate_null_data(seed, n, m=16, c=5):
    rng = np.random.default_rng(seed)
    x = rng.uniform(-np.sqrt(3), np.sqrt(3), n)          # Var(x)=1
    y = m * x + c + rng.normal(0, 1, n)
    return x, y

# ───────────────────── 2. score vectors & MEWMA helpers ─────────────────────
def score_vectors(X, y, model, alpha):
    X_aug = np.hstack([np.ones((X.shape[0], 1)), X])
    β = np.append(model.intercept_, model.coef_)
    resid = y - X_aug @ β
    reg = 2 * alpha * np.append(0, model.coef_) / X.shape[0]
    return (-2 * resid)[:, None] * X_aug + reg

def ewma_T2(S, lam, μ, Σ_inv):
    z = np.zeros_like(μ)
    out = np.empty(len(S))
    for t, s in enumerate(S):
        z = lam * s + (1 - lam) * z
        diff = (z - μ)[:, None]
        out[t] = (diff.T @ Σ_inv @ diff).item()
    return out

# ───── 3. one DB replicate following Zhang’s two-phase (D₁/D₂) protocol ─────
def process_db_kungang(seed, cfg):
    # 3-1. split stable data
    x, y = generate_null_data(seed, cfg['n_total'])
    n_half = cfg['n_total'] // 2
    x_D1, y_D1 = x[:n_half], y[:n_half]   # training
    x_D2, y_D2 = x[n_half:], y[n_half:]   # Phase-I for μ̂ & UCL

    # 3-2. fit model on D₁
    scaler = StandardScaler()
    X_D1 = scaler.fit_transform(x_D1[:, None])
    model = Ridge(alpha=cfg['alpha']).fit(X_D1, y_D1)

    # 3-3. Σ̂ from D₁
    S_D1 = score_vectors(X_D1, y_D1, model, cfg['alpha'])
    Σ = np.cov(S_D1.T) + cfg['λ_cov'] * np.eye(S_D1.shape[1])
    Σ_inv = np.linalg.pinv(Σ, hermitian=True)

    # 3-4. μ̂ and UCL from D₂
    X_D2 = scaler.transform(x_D2[:, None])
    S_D2 = score_vectors(X_D2, y_D2, model, cfg['alpha'])
    μ = S_D2.mean(axis=0)                             # ← mean from D₂
    T2_D2 = ewma_T2(S_D2, cfg['lam'], μ, Σ_inv)
    UCL = np.percentile(T2_D2, cfg['perc'])           # constant limit

    # 3-5. future IC streams to estimate FAR
    exc = np.zeros((cfg['N_future'], cfg['M']))
    for j in range(cfg['N_future']):
        xf, yf = generate_null_data(seed * cfg['N_future'] + j, cfg['M'])
        Xf = scaler.transform(xf[:, None])
        Sf = score_vectors(Xf, yf, model, cfg['alpha'])
        T2_f = ewma_T2(Sf, cfg['lam'], μ, Σ_inv)      # uses μ̂, Σ̂ from Phase-I
        exc[j] = (T2_f > UCL)
    return exc

# ─────────────── 4. FAR driver (unchanged, just calls process_db) ───────────
def evaluate_far_kungang(cfg, N_db=50):
    workers = max(1, multiprocessing.cpu_count() - 1)
    all_exc = []
    with ProcessPoolExecutor(max_workers=workers) as pool:
        futs = [pool.submit(process_db_kungang, s, cfg) for s in range(N_db)]
        for fut in tqdm(as_completed(futs), total=N_db, desc="DB replicates"):
            all_exc.append(fut.result())
    all_exc = np.asarray(all_exc)                     # shape (db, rep, M)
    return pd.DataFrame({
        "i": np.arange(cfg['M']),
        "FAR": all_exc.mean(axis=(0, 1))
    })

# ───────────────────────── 5. simple FAR plot helper ────────────────────────
def plot_far(df, α_nom=0.001):
    plt.figure(figsize=(10, 6))
    plt.plot(df.i, df.FAR, lw=2, label="Empirical FAR", c="#1f77b4")
    plt.axhline(α_nom, ls="--", lw=2, c="#d95f02", label=f"Nominal α={α_nom}")
    plt.ylim(0, α_nom*20)
    plt.xlabel("Time index $i$")
    plt.ylabel("False-Alarm Rate")
    plt.title("Pointwise FAR of Zhang et al. two-phase chart")
    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()

# ─────────────────────────────────── 6. run ─────────────────────────────────
if __name__ == "__main__":
    cfg = dict(
        n_total  = 2_000,   # 1 000 in D₁ + 1 000 in D₂
        M        = 1_000,   # samples per monitoring stream
        N_future = 1_000,   # streams per DB replicate
        lam      = 0.01,    # EWMA λ (as in paper)
        alpha    = 0.01,    # ridge penalty
        λ_cov    = 1e-1,    # Σ regulariser
        perc     = 99.9,    # → α = 0.001
    )

    df_far = evaluate_far_kungang(cfg, N_db=100)   # raise N_db for tighter SEs
    plot_far(df_far)

# ───────────────────────── 5. simple FAR plot helper ────────────────────────
def plot_far(df, α_nom=0.001):
    plt.figure(figsize=(10, 6))
    plt.plot(df.i, df.FAR, lw=2, label="Empirical FAR", c="#1f77b4")
    plt.axhline(α_nom, ls="--", lw=2, c="#d95f02", label=f"Nominal α={α_nom}")
    plt.ylim(0, α_nom*100)
    plt.xlabel("Time index $i$")
    plt.ylabel("False-Alarm Rate")
    plt.title("Pointwise FAR of Zhang et al. Approach")
    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()
plot_far(df_far)

def plot_far(df, α_nom=0.001):
    fig, ax = plt.subplots(figsize=(10,6))
    ax.plot(df.i, df.FAR_mean,   lw=2, label="Empirical FAR")
    #ax.plot(df.i, df.FAR_median, lw=2, ls=':', label="median FAR")
    ax.axhline(α_nom, color="#d95f02", ls='--', lw=2, label=f"Nominal α={α_nom}")
    ax.set_ylim(0, α_nom*200)
    ax.set_xlabel("Time Index $i$")
    ax.set_ylabel("False Alarm Rate")
    ax.set_title("Pointwise FAR of Zhang & Apley's Approach")
    ax.grid(alpha=.3);  ax.legend();  plt.tight_layout();  plt.show()



plot_far(df_far)

"""
Kungang Zhang et al. approach – diagnostics
––––––––––––––––––––––––––––––––––––––––––––
• D1 (50 %)  → fit ridge            • λ = 0.003
• D2 (50 %)  → constant UCL = 99.9 % quantile of T²
• FAR + T² diagnostics over many future streams
"""

# ╔══════════════════════════════════════════════════════════════════════════╗
# 0. Imports & plotting defaults
# ╚══════════════════════════════════════════════════════════════════════════╝
import numpy as np, pandas as pd, matplotlib.pyplot as plt, multiprocessing
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

plt.rcParams.update({
    "font.size":14, "axes.titlesize":18, "axes.labelsize":16,
    "legend.fontsize":14, "xtick.labelsize":12, "ytick.labelsize":12,
    "figure.dpi":150
})

RNG = np.random.default_rng()

# ╔══════════════════════════════════════════════════════════════════════════╗
# 1.  Data generator
# ╚══════════════════════════════════════════════════════════════════════════╝
def generate_null(seed, n, m=4, c=3):
    r = np.random.default_rng(seed)
    x = r.uniform(-np.sqrt(3), np.sqrt(3), n)
    y = m * x + c + r.normal(0, 1, n)
    return x, y

# ╔══════════════════════════════════════════════════════════════════════════╗
# 2.  Score vectors and MEWMA utilities
# ╚══════════════════════════════════════════════════════════════════════════╝
def score_vectors(X, y, mdl, alpha):
    Xaug = np.hstack([np.ones((X.shape[0],1)), X])
    beta = np.append(mdl.intercept_, mdl.coef_)
    resid= y - Xaug @ beta
    reg  = 2*alpha*np.append(0, mdl.coef_) / X.shape[0]
    return (-2*resid)[:,None] * Xaug + reg

def ewma_T2(S, lam, mu, Sig_inv):
    z = np.zeros_like(mu); out = np.empty(len(S))
    for t,s in enumerate(S):
        z = lam*s + (1-lam)*z
        diff = (z-mu)[:,None]
        out[t] = (diff.T @ Sig_inv @ diff).item()
    return out                                            # (len(S),)

# ╔══════════════════════════════════════════════════════════════════════════╗
# 3.  One database replicate – now returns alarms *and* T² values
# ╚══════════════════════════════════════════════════════════════════════════╝
def db_kungang(seed, cfg):
    x, y = generate_null(seed, cfg['n_total'])
    n_half = cfg['n_total']//2
    x_D1, y_D1, x_D2, y_D2 = x[:n_half], y[:n_half], x[n_half:], y[n_half:]

    scaler = StandardScaler()
    X_D1   = scaler.fit_transform(x_D1[:,None])
    mdl    = Ridge(alpha=cfg['alpha']).fit(X_D1, y_D1)

    S_D1 = score_vectors(X_D1, y_D1, mdl, cfg['alpha'])
    mu   = S_D1.mean(axis=0)
    Sig  = np.cov(S_D1.T) + cfg['λ_cov']*np.eye(S_D1.shape[1])
    Sig_i= np.linalg.pinv(Sig, hermitian=True)

    # constant UCL from D2
    T2_D2 = ewma_T2(
        score_vectors(scaler.transform(x_D2[:,None]), y_D2, mdl, cfg['alpha']),
        cfg['lam'], mu, Sig_i)
    UCL = np.percentile(T2_D2, cfg['perc'])

    # future streams
    exc  = np.zeros((cfg['N_future'], cfg['M']))
    T2s  = np.zeros_like(exc)
    for j in range(cfg['N_future']):
        xf,yf = generate_null(seed*cfg['N_future']+j, cfg['M'])
        Sf    = score_vectors(scaler.transform(xf[:,None]), yf, mdl, cfg['alpha'])
        T2_f  = ewma_T2(Sf, cfg['lam'], mu, Sig_i)
        T2s[j] = T2_f
        exc[j]= T2_f > UCL
    return exc, T2s, UCL

# ╔══════════════════════════════════════════════════════════════════════════╗
# 4.  Aggregate over DB replicates
# ╚══════════════════════════════════════════════════════════════════════════╝
def evaluate_kungang(cfg, N_db=50):
    work = max(1, multiprocessing.cpu_count()-1)
    all_exc, all_T2, UCLs = [], [], []

    with ProcessPoolExecutor(max_workers=work) as pool:
        futs = [pool.submit(db_kungang, s, cfg) for s in range(N_db)]
        for fut in tqdm(as_completed(futs), total=N_db, desc="DB replicates"):
            exc, T2s, U = fut.result()
            all_exc.append(exc);  all_T2.append(T2s);  UCLs.append(U)

    exc_arr = np.asarray(all_exc)           # (db,fut,M)
    T2_arr  = np.asarray(all_T2)            # (db,fut,M)
    UCL     = np.mean(UCLs)                 # all UCLs ≈ identical

    df = pd.DataFrame({
        "i"         : np.arange(cfg['M']),
        "FAR_mean"  : exc_arr.mean(axis=(0,1)),
        "FAR_p5"    : np.quantile(exc_arr, 0.05, axis=(0,1)),
        "FAR_p95"   : np.quantile(exc_arr, 0.95, axis=(0,1)),
        "T2_med"    : np.median (T2_arr, axis=(0,1)),
        "T2_p95"    : np.quantile(T2_arr, 0.95 , axis=(0,1)),
        "T2_p995"   : np.quantile(T2_arr, 0.995, axis=(0,1)),
    })
    return df, T2_arr, UCL

# ╔══════════════════════════════════════════════════════════════════════════╗
# 5.  Plotting helpers
# ╚══════════════════════════════════════════════════════════════════════════╝
def plot_far(df, α=0.001):
    fig,ax = plt.subplots(figsize=(10,6))
    ax.plot(df.i, df.FAR_mean, lw=2, label="mean FAR")
    ax.fill_between(df.i, df.FAR_p5, df.FAR_p95,
                    alpha=.25, label="5–95 % band")
    ax.axhline(α, ls='--', lw=2, color="#d95f02", label=f"nominal α={α}")
    ax.set_ylim(0, α*2); ax.set_xlabel("time index"); ax.set_ylabel("FAR")
    ax.set_title("Kungang-split: point-wise FAR"); ax.grid(alpha=.3); ax.legend()
    plt.tight_layout(); plt.show()

def plot_T2_quantiles(df, UCL):
    fig,ax=plt.subplots(figsize=(10,6))
    ax.plot(df.i, df.T2_med , lw=2, label="median $T^{2}$")
    ax.plot(df.i, df.T2_p95 , lw=2, label="95 %")
    ax.plot(df.i, df.T2_p995, lw=2, label="99.5 %")
    ax.axhline(UCL, color='k', ls='--', lw=2, label="constant UCL")
    ax.set_yscale('log'); ax.set_xlabel("time index")
    ax.set_ylabel("$T^{2}$ (log scale)")
    ax.set_title("$T^{2}$ distribution vs. UCL"); ax.grid(alpha=.3,which='both')
    ax.legend(); plt.tight_layout(); plt.show()

def plot_histograms(T2_arr, UCL, early=10, late=500):
    for idx, lbl in [(early,"early"),(late,"late")]:
        vals=T2_arr[:,:,idx].ravel()
        plt.figure(figsize=(6,4))
        plt.hist(vals,bins=60,density=True,alpha=.7)
        plt.axvline(UCL,ls='--',color='k',lw=2,label="UCL")
        plt.xscale('log'); plt.xlabel("$T^{2}$ (log)")
        plt.ylabel("density"); plt.title(f"Histogram at {lbl} (i={idx})")
        plt.legend(); plt.tight_layout(); plt.show()

# ╔══════════════════════════════════════════════════════════════════════════╗
# 6.  Main run
# ╚══════════════════════════════════════════════════════════════════════════╝
if __name__ == "__main__":
    cfg = dict(
        n_total = 2_000,   # 1000 + 1000
        M       = 1_000,
        N_future= 1_000,
        lam     = 0.003,
        alpha   = 0.01,
        λ_cov   = 1e-1,
        perc    = 99.9
    )

    df_far, T2_all, UCL = evaluate_kungang(cfg, N_db=50)

    plot_far(df_far, α=0.001)        # (A) FAR curve
    plot_T2_quantiles(df_far, UCL)   # (B) quantile band vs UCL
    plot_histograms(T2_all, UCL)     # (C) histograms at i=10 and i=500

def plot_far(df, α=0.001):
    fig,ax = plt.subplots(figsize=(10,6))
    ax.plot(df.i, df.FAR_mean, lw=2, label="mean FAR")
    ax.axhline(α, ls='--', lw=2, color="#d95f02", label=f"nominal α={α}")
    ax.set_ylim(0, α*200); ax.set_xlabel("time index"); ax.set_ylabel("FAR")
    ax.set_title("Kungang-split: point-wise FAR"); ax.grid(alpha=.3); ax.legend()
    plt.tight_layout(); plt.show()

plot_far(df_far, α=0.001)

def null_T2_kungang(seed, cfg):
    """
    Generate ONE monitoring trajectory under the null *for Kungang's chart*.
    Returns
        T2 : ndarray shape (M,)
    """
    # 1.  Fit model on D1  (reuse code you already have)
    x_all, y_all = generate_null(seed, cfg['n_total'])
    half = cfg['n_total']//2
    scaler = StandardScaler().fit(x_all[:half,None])
    mdl = Ridge(alpha=cfg['alpha']).fit(
            scaler.transform(x_all[:half,None]), y_all[:half])

    S_train = score_vectors(scaler.transform(x_all[:half,None]), y_all[:half],
                            mdl, cfg['alpha'])
    mu   = S_train.mean(axis=0)
    Sig  = np.cov(S_train.T) + cfg['λ_cov']*np.eye(S_train.shape[1])
    Sig_i= np.linalg.pinv(Sig, hermitian=True)

    # 2.  Null monitoring stream
    x_mon, y_mon = generate_null(seed+10_000, cfg['M'])
    S_mon  = score_vectors(scaler.transform(x_mon[:,None]), y_mon, mdl,
                           cfg['alpha'])
    return ewma_T2(S_mon, cfg['lam'], mu, Sig_i)

def process_single_database_power(seed, params):
    """Process a single database replicate for power testing"""
    M = params['M']  # monitoring length

    # Generate database replicate (always from null case for training)
    (x_train, y_train), _, _ = generate_data_for_testing(
        seed=seed,
        case='null',
        m1=params['m1'],
        c1=params['c1']
    )

    # Standardize data and fit model
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(x_train.reshape(-1, 1))
    model = Ridge(alpha=params['alpha'])
    model.fit(X_train_scaled, y_train)

    # Compute score vectors and statistics for training data
    S_train = score_vectors(X_train_scaled, y_train, model, params['alpha'])
    mean_score = np.mean(S_train, axis=0)
    cov_score = np.cov(S_train.T)
    cov_score_reg = cov_score + params['lambda_reg'] * np.eye(cov_score.shape[0])

    # Compute UCL using bootstrap - now correctly unpacking both return values
    UCL, cov_inv_sqrt = compute_bootstrap_ucl(
        x_train, y_train,
        alpha=params['alpha'],
        BO=params['BO'],
        BI=params['BI'],
        M=M,
        p=params['p'],
        lambda_ewma=params['lambda_ewma'],
        lambda_reg=params['lambda_reg']
    )

    # Process future replicates
    n_future = params['n_future_replicates']
    exceedances = np.zeros((n_future, M))

    print(f"\nProcessing {n_future} future replicates...")
    for future_idx in range(n_future):
        # Generate future data with both IC and OC regions
        _, (x_monitor, y_monitor), n_ic = generate_data_for_testing(
            seed=seed*n_future + future_idx,
            case='alternative',  # Use alternative case for power testing
            m1=params['m1'],
            c1=params['c1'],
            m2=params['m2'],
            c2=params['c2']
        )

        # Monitor and record exceedances
        X_monitor_scaled = scaler.transform(x_monitor.reshape(-1, 1))
        S_monitor = score_vectors(X_monitor_scaled, y_monitor, model, params['alpha'])

        # Use only first M points to match UCL length
        S_monitor = S_monitor[:M]

        # Compute monitoring statistics without k factor
        T2_stats = compute_ewma_stats_monitoring(
            S_monitor,
            params['lambda_ewma'],
            mean_score,
            cov_inv_sqrt  # Now using cov_inv_sqrt from bootstrap
        )

        exceedances[future_idx] = T2_stats > UCL

    return exceedances

def evaluate_power(n_db_replicates=20, n_future_replicates=1000, n_processes=None, **params):
    """Evaluate detection power across multiple database replicates"""
    if n_processes is None:
        n_processes = max(1, multiprocessing.cpu_count() - 1)

    print(f"Starting power evaluation with:")
    print(f"- {n_db_replicates} database replicates")
    print(f"- {n_future_replicates} future replicates per database")
    print(f"- Parameters: {params}")

    # Add additional parameters
    params['n_future_replicates'] = n_future_replicates

    # Process databases in parallel
    all_exceedances = []
    with ProcessPoolExecutor(max_workers=n_processes) as executor:
        futures = [executor.submit(process_single_database_power, i, params)
                  for i in range(n_db_replicates)]

        for i, future in enumerate(tqdm(as_completed(futures),
                                      total=n_db_replicates,
                                      desc="Processing database replicates")):
            all_exceedances.append(future.result())

    # Compute probability of detection
    all_exceedances = np.array(all_exceedances)
    pointwise_pod = np.mean(all_exceedances, axis=(0, 1))

    # Create results DataFrame
    results = pd.DataFrame({
        'time_index': np.arange(len(pointwise_pod)),
        'POD': pointwise_pod
    })

    return results

def plot_power_results(results, n_ic=200, title=None):
    """Plot power evaluation results with IC/OC regions"""
    plt.figure(figsize=(12, 6))

    # Plot probability of detection
    plt.plot(results['time_index'], results['POD'], 'b-', label='Probability of Detection')

    # Add vertical line at change point
    plt.axvline(x=n_ic, color='g', linestyle='--', label='Change point')

    # Add nominal level for reference
    plt.axhline(y=0.001, color='r', linestyle='--', label='Nominal level (α=0.001)')

    # Add region labels
    plt.axvspan(0, n_ic, alpha=0.1, color='g', label='In-control region')
    plt.axvspan(n_ic, results['time_index'].max(), alpha=0.1, color='r', label='Out-of-control region')

    plt.xlabel('Time index')
    plt.ylabel('Probability of Detection')
    if title is None:
        title = 'Pointwise Probability of Detection'
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=12)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.show()

# ╔══════════════════════════════════════════════════════════════════════════╗
# >>>               M I S S I N G   P I E C E S   S T A R T               <<<#
# ╚══════════════════════════════════════════════════════════════════════════╝
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

RNG = np.random.default_rng(48)          # global RNG for reproducibility
# ─────────────────────────────────────────────────────────────────────────────
# 1)  DATA-GENERATING UTILITIES
# ─────────────────────────────────────────────────────────────────────────────
def generate_null_data(seed, n, m=4, c=3):
    """Pure in-control line  y = m·x + c + ε  with Var(x)=1, ε~N(0,1)."""
    rng = np.random.default_rng(seed)
    x = rng.uniform(-np.sqrt(3), np.sqrt(3), n)
    y = m * x + c + rng.normal(0, 1, n)
    return x, y


def generate_data(seed,
                  *,
                  n_train, n_ic, n_oc,
                  m_ic=4, c_ic=3,
                  m1=16, c1=5, m2=12, c2=3,
                  case="null"):
    """
    Wrapper that produces (train , monitoring stream , n_ic)
      • If case='null'  : monitoring stream is all IC
      • If case='alternative' : first n_ic points IC, next n_oc from mixture
    """
    rng = np.random.default_rng(seed)

    # training set (always IC)
    x_tr = rng.uniform(-np.sqrt(3), np.sqrt(3), n_train)
    y_tr = m_ic * x_tr + c_ic + rng.normal(0, 1, n_train)

    # IC segment for monitoring
    x_ic = rng.uniform(-np.sqrt(3), np.sqrt(3), n_ic)
    y_ic = m_ic * x_ic + c_ic + rng.normal(0, 1, n_ic)

    # OC segment if needed
    x_oc = rng.uniform(-np.sqrt(3), np.sqrt(3), n_oc)
    if case == "null":
        y_oc = m_ic * x_oc + c_ic + rng.normal(0, 1, n_oc)
    else:
        mask = rng.random(n_oc) < 0.5
        eps  = rng.normal(0, 1, n_oc)
        y_oc = np.where(mask,
                        m1 * x_oc + c1 + eps,
                        m2 * x_oc + c2 + eps)

    x_mon = np.concatenate([x_ic, x_oc])
    y_mon = np.concatenate([y_ic, y_oc])
    return (x_tr, y_tr), (x_mon, y_mon), n_ic


# Thin shim so old code continues to work -----------------------------
def generate_data_for_testing(seed, case,
                              m1, c1, m2=12, c2=3,
                              n_train=2_000, n_ic=200, n_oc=800):
    """Alias that forwards to `generate_data`."""
    return generate_data(seed,
                         n_train=n_train, n_ic=n_ic, n_oc=n_oc,
                         m_ic=m1, c_ic=c1, m1=m1, c1=c1, m2=m2, c2=c2,
                         case=case)


# ─────────────────────────────────────────────────────────────────────────────
# 2)  STATISTIC BUILDING BLOCKS
# ─────────────────────────────────────────────────────────────────────────────
def score_vectors(X, y, model, alpha):
    """
    Per-observation ridge score  s_i = −2 (y_i − x_iᵀβ̂) x_i + 2αβ̂/n .
    Intercept handled by an augmented 1-column.
    """
    X_aug = np.hstack([np.ones((X.shape[0], 1)), X])
    beta  = np.append(model.intercept_, model.coef_)
    resid = y - X_aug @ beta
    reg   = 2 * alpha * np.append(0, model.coef_) / X.shape[0]
    return (-2 * resid)[:, None] * X_aug + reg


def ewma_T2(S, lam, mu, Sigma_inv):
    """Standard EWMA T² with *constant* Σ⁻¹ (no k-factor)."""
    z   = np.zeros_like(mu)
    out = np.empty(len(S))
    for t, s in enumerate(S):
        z = lam * s + (1 - lam) * z
        diff = (z - mu)[:, None]
        out[t] = (diff.T @ Sigma_inv @ diff).item()
    return out


# ─────────────────────────────────────────────────────────────────────────────
# 3)  BOOTSTRAP UCL  (your original routine, lightly adapted)
# ─────────────────────────────────────────────────────────────────────────────
def _bootstrap_ucl_core(x_train, y_train, *,
                        alpha, BO, BI, M,
                        lambda_ewma, lambda_reg, perc):
    """
    Returns a vector length M of bootstrapped UCLs (your original logic).
    """
    scaler = StandardScaler()
    Xtr    = scaler.fit_transform(x_train.reshape(-1, 1))
    base   = Ridge(alpha=alpha).fit(Xtr, y_train)

    S_tr = score_vectors(Xtr, y_train, base, alpha)
    mu   = S_tr.mean(axis=0)
    Sigma= np.cov(S_tr.T) + lambda_reg * np.eye(S_tr.shape[1])
    Sigma_inv = np.linalg.pinv(Sigma, hermitian=True)

    n   = len(x_train)
    res = np.empty((BO * BI, M))

    for b in range(BO):
        idx_boot = RNG.choice(n, n, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n), idx_boot)

        X_b  = scaler.fit_transform(x_train[idx_boot].reshape(-1, 1))
        y_b  = y_train[idx_boot]
        X_oob= scaler.transform(x_train[idx_oob].reshape(-1, 1))
        y_oob= y_train[idx_oob]

        model_b = Ridge(alpha=alpha).fit(X_b, y_b)
        S_oob   = score_vectors(X_oob, y_oob, model_b, alpha)

        for j in range(BI):
            draw = S_oob[RNG.choice(len(S_oob), M, replace=True)]
            res[b * BI + j] = ewma_T2(draw, lambda_ewma, mu, Sigma_inv)

    return np.percentile(res, perc, axis=0), Sigma_inv


def compute_bootstrap_ucl(x_train, y_train, *,
                          alpha, BO, BI, M, p,
                          lambda_ewma, lambda_reg):
    """
    Wrapper expected by your power code:
      • returns (UCL_vec , Σ⁻¹/²)   where  UCL_vec  has length M
    """
    UCL_vec, Sigma_inv = _bootstrap_ucl_core(
        x_train, y_train,
        alpha=alpha, BO=BO, BI=BI, M=M, perc=p,
        lambda_ewma=lambda_ewma, lambda_reg=lambda_reg)

    # matrix square root of Σ⁻¹  (for pre-whitening)
    eigval, eigvec = np.linalg.eigh(Sigma_inv)
    Sigma_inv_sqrt = eigvec @ np.diag(np.sqrt(eigval)) @ eigvec.T
    return UCL_vec, Sigma_inv_sqrt


# ─────────────────────────────────────────────────────────────────────────────
# 4)  EWMA MONITOR USING Σ⁻¹/²  (matches your call)
# ─────────────────────────────────────────────────────────────────────────────
def compute_ewma_stats_monitoring(S, lam, mu, Sigma_inv_sqrt):
    """
    EWMA T² but written with Σ⁻¹/² so caller can reuse it for any Σ.
    """
    z = np.zeros_like(mu)
    T2 = np.empty(len(S))
    for t, s in enumerate(S):
        z = lam * s + (1 - lam) * z
        diff = Sigma_inv_sqrt @ (z - mu)
        T2[t] = diff @ diff          # same as (z-mu)ᵀ Σ⁻¹ (z-mu)
    return T2
# ╔══════════════════════════════════════════════════════════════════════════╗
# >>>                M I S S I N G   P I E C E S   E N D                  <<<#
# ╚══════════════════════════════════════════════════════════════════════════╝

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
from tqdm import tqdm

# ╔══════════════════════════════════════════════════════════════════════════╗
# ORACLE-STYLE CALIBRATION  &  POWER FOR KUNGANG METHOD
# ╚══════════════════════════════════════════════════════════════════════════╝
# 0.  Oracle calibration: pick ONE scalar UCL so PFAR ≈ α under the null
def calibrate_oracle_ucl_kungang(cfg,
                                 n_calib_db=200,        # # training DB replicates
                                 n_streams_per_db=500,  # null streams per DB
                                 rng=np.random.default_rng()):
    """
    Return a single 'oracle' control limit that forces the point-wise FAR of
    Kungang's T²–EWMA to equal cfg['alpha'] under an i.i.d. null.

    The routine repeatedly:
      – draws a training DB, fits KG's ridge model (D1)
      – produces many pure-null monitoring streams
      – records *all* T² values
    The (1-α) empirical quantile of those T²'s is the oracle UCL.
    """
    T2_vals = []

    for db_seed in range(n_calib_db):
        # --- Step 1: simulate one training database & split -----------------
        x, y = generate_null_data(db_seed, cfg['n_total'])
        n_half = cfg['n_total'] // 2
        x_D1, y_D1 = x[:n_half], y[:n_half]                # model-fitting part

        scaler = StandardScaler()
        X_D1   = scaler.fit_transform(x_D1[:, None])
        model  = Ridge(alpha=cfg['alpha']).fit(X_D1, y_D1)

        # score stats for EWMA
        S_D1 = score_vectors(X_D1, y_D1, model, cfg['alpha'])
        μ    = S_D1.mean(axis=0)
        Σi   = np.linalg.pinv(np.cov(S_D1.T)
                              + cfg['λ_cov'] * np.eye(S_D1.shape[1]),
                              hermitian=True)

        # --- Step 2: many pure-null monitoring streams ----------------------
        for s in range(n_streams_per_db):
            xf, yf = generate_null_data(db_seed * 10_000 + s, cfg['M'])
            Xf     = scaler.transform(xf[:, None])
            Sf     = score_vectors(Xf, yf, model, cfg['alpha'])
            T2     = ewma_T2(Sf, cfg['lam'], μ, Σi)        # no k-factor
            T2_vals.extend(T2)

    T2_vals = np.asarray(T2_vals)
    oracle_cl = np.quantile(T2_vals, 1.0 - cfg['alpha'])
    return float(oracle_cl)


# 1.  Single-DB replicate for KG power *with a fixed CL* (no cheating later!)
def process_db_kungang_power_oracle(seed, cfg, UCL_const):
    """
    Like process_db_kungang(), but:
      – uses one *global* constant UCL_const (oracle-tuned beforehand)
      – generates ALT monitoring streams (mixture shift) for power.
    Returns an array (N_future, M) of exceedance indicators.
    """
    # -- 1. fit model on D1 ---------------------------------------------------
    x, y = generate_null_data(seed, cfg['n_total'])
    n_half = cfg['n_total'] // 2
    x_D1, y_D1 = x[:n_half], y[:n_half]

    scaler = StandardScaler()
    X_D1   = scaler.fit_transform(x_D1[:, None])
    model  = Ridge(alpha=cfg['alpha']).fit(X_D1, y_D1)

    S_D1 = score_vectors(X_D1, y_D1, model, cfg['alpha'])
    μ    = S_D1.mean(axis=0)
    Σi   = np.linalg.pinv(np.cov(S_D1.T)
                          + cfg['λ_cov'] * np.eye(S_D1.shape[1]),
                          hermitian=True)

    # -- 2. ALT future replicates --------------------------------------------
    exc = np.zeros((cfg['N_future'], cfg['M']))
    for j in range(cfg['N_future']):
        # *alternative* stream: first n_ic IC, then mixture OC
        (_, (x_mon, y_mon), _) = generate_data(seed * cfg['N_future'] + j,
                                               n_train=cfg['n_train'],
                                               n_ic   =cfg['n_ic'],
                                               n_oc   =cfg['n_oc'],
                                               case   ='alternative')
        X_mon = scaler.transform(x_mon[:, None])
        S_mon = score_vectors(X_mon, y_mon, model, cfg['alpha'])[:cfg['M']]
        T2    = ewma_T2(S_mon, cfg['lam'], μ, Σi)
        exc[j] = T2 > UCL_const
    return exc


# 2.  Power evaluation wrapper for KG-oracle
def evaluate_power_kungang_oracle(cfg,
                                  UCL_const,
                                  N_db=50,
                                  workers=None):
    if workers is None:
        workers = max(1, multiprocessing.cpu_count() - 1)

    all_exc = []
    with ProcessPoolExecutor(max_workers=workers) as pool:
        futs = [pool.submit(process_db_kungang_power_oracle,
                            s, cfg, UCL_const)
                for s in range(N_db)]
        for fut in tqdm(as_completed(futs), total=N_db,
                        desc="DB replicates (KG-oracle power)"):
            all_exc.append(fut.result())

    all_exc = np.asarray(all_exc)          # (db , future , M)
    pod = all_exc.mean(axis=(0, 1))
    return pd.DataFrame({"i": np.arange(cfg['M']),
                         "POD": pod})


# 3.  Convenience plot
def plot_power(df, n_ic, label):
    plt.plot(df.i, df.POD, lw=2, label=label)


# ╔══════════════════════════════════════════════════════════════════════════╗
# QUICK DEMO / HOW-TO
# ╚══════════════════════════════════════════════════════════════════════════╝
if __name__ == "__main__":
    # --- config matching your earlier KG settings ---------------------------
    ALPHA = 0.001
    cfg_kg = dict(
        n_total  = 2_000,
        n_train  = 2_000,
        n_ic     = 200,
        n_oc     = 800,
        M        = 1_000,
        N_future = 1_000,
        lam      = 0.003,
        alpha    = ALPHA,            # <- 0.001
        λ_cov    = 1e-1,
    )

    # --- 1. calibrate oracle UCL -------------------------------------------
    print("Calibrating oracle UCL …")
    ucl_oracle = calibrate_oracle_ucl_kungang(cfg_kg,
                                              n_calib_db=250,
                                              n_streams_per_db=400)
    print(f"Oracle UCL (α={cfg_kg['alpha']}) = {ucl_oracle:.4g}")

    # --- 2. KG-oracle power -------------------------------------------------
    df_power_kg = evaluate_power_kungang_oracle(cfg_kg,
                                                ucl_oracle,
                                                N_db=50)


    params_boot = dict(
        alpha        = ALPHA,        # <- 0.001
        lambda_ewma  = cfg_kg['lam'],
        lambda_reg   = cfg_kg['λ_cov'],
        BO           = 100,
        BI           = 200,
        M            = cfg_kg['M'],
        p            = 100*(1-ALPHA),# 99.9 for α = 0.001
        m1=16, c1=5, m2=12, c2=3
    )


    df_power_boot = evaluate_power(n_db_replicates = 50,
                                   n_future_replicates = cfg_kg['N_future'],
                                   **params_boot)





    df_power_boot = df_power_boot.rename(columns={'time_index': 'i'})

    # --- 4. Plot comparison -------------------------------------------------
    plt.figure(figsize=(10,6))
    plot_power(df_power_boot, cfg_kg['n_ic'], "Bootstrap-MEWMA (ours)")
    plot_power(df_power_kg,   cfg_kg['n_ic'], "Kungang & Apley (oracle-CL)")
    plt.axvline(cfg_kg['n_ic'], ls='--', color='k', label='Change-point')
    plt.xlabel("Time Index $i$")
    plt.ylabel("Probability of Detection")
    plt.title("Power Comparison (Oracle-adjusted vs. Bootstrap-MEWMA)")
    plt.legend(); plt.grid(alpha=.3); plt.tight_layout(); plt.show()

df_power_boot = df_power_boot.rename(columns={'time_index': 'i'})

# --- 4. Plot comparison -------------------------------------------------
plt.figure(figsize=(10,6))
plot_power(df_power_boot, cfg_kg['n_ic'], "Bootstrap-MEWMA (ours)")
plot_power(df_power_kg,   cfg_kg['n_ic'], "Kungang & Apley (oracle-CL)")
plt.axvline(cfg_kg['n_ic'], ls='--', color='k', label='Change-point')
plt.xlabel("Time Index $i$")
plt.ylabel("Probability of Detection")
plt.title("Power Comparison (Oracle-adjusted vs. Bootstrap-MEWMA)")
plt.legend(); plt.grid(alpha=.3); plt.tight_layout(); plt.show()

# --- nicer typography --------------------------------------------------------
plt.rcParams.update({
    "font.size"       : 16,   # body text
    "axes.titlesize"  : 20,
    "axes.labelsize"  : 18,
    "legend.fontsize" : 16,
    "xtick.labelsize" : 14,
    "ytick.labelsize" : 14,
})

# --- re-plot -----------------------------------------------------------------
plt.figure(figsize=(10, 6))
plot_power(df_power_boot, cfg_kg['n_ic'], "Bootstrap-MEWMA (ours)")
plot_power(df_power_kg,   cfg_kg['n_ic'], "Kungang & Apley (oracle CL)")

plt.axvline(cfg_kg['n_ic'], ls='--', color='k', label='Change-point')
plt.yscale("log")                              # log-scale y-axis
plt.xlabel(r"Time Index $i$")
plt.ylabel("Probability of Detection")
plt.title("Power Comparison (Oracle-adjusted vs. Bootstrap-MEWMA)")
plt.grid(alpha=0.3, which="both")
plt.legend()
plt.tight_layout()
plt.show()

# ────────────────────────────────────────────────────────────────────────────
# ORACLE COMPARISON :  Bootstrap-MEWMA  vs  Zhang & Apley two-phase chart
# steady-state α = 0.001 for BOTH
# ────────────────────────────────────────────────────────────────────────────
import numpy as np, matplotlib.pyplot as plt, multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

# 1 ─── global constants / style ─────────────────────────────────────────────
ALPHA     = 0.001
LAMBDA    = 0.01          # EWMA λ  (same for both charts)
WARM_UP   = 200           # discard first 200 T² values
STEADY_N  = 1_000         # keep next 1 000 → 50 000 steady-state points
N_REP_DB  = 50            # calibration & power replicates
N_STREAMS = 1             # one stream per DB (matches your earlier convention)
RNG       = np.random.default_rng(123)

plt.rcParams.update({
    "font.size":14, "axes.titlesize":18, "axes.labelsize":16,
    "legend.fontsize":14, "xtick.labelsize":12, "ytick.labelsize":12,
    "figure.dpi":150
})

# 2 ─── data generation helpers (same as earlier) ────────────────────────────
def gen_ic(seed, n, m=16, c=5):                    #     y = 16x+5 + ε
    r = np.random.default_rng(seed)
    x = r.uniform(-np.sqrt(3), np.sqrt(3), n)
    y = m * x + c + r.normal(0, 1, n)
    return x, y

def gen_shift_stream(seed, n_ic=200, n_oc=800):
    """ 200 IC points (16x+5) + 800 mixture (0.5·16x+5  ‖  0.5·12x+3). """
    x_ic, y_ic = gen_ic(seed, n_ic)
    r  = np.random.default_rng(seed+999)
    x_oc = r.uniform(-np.sqrt(3), np.sqrt(3), n_oc)
    eps  = r.normal(0, 1, n_oc)
    mask = r.random(n_oc) < 0.5
    y_oc = np.where(mask,
                    16*x_oc + 5 + eps,             # component 1
                    12*x_oc + 3 + eps)             # component 2
    return np.concatenate([x_ic, x_oc]), np.concatenate([y_ic, y_oc])

# 3 ─── score vectors & EWMA variants ────────────────────────────────────────
def score_vecs(X, y, mdl, alpha):
    X_aug = np.hstack([np.ones((X.shape[0],1)), X])
    beta  = np.append(mdl.intercept_, mdl.coef_)
    resid = y - X_aug @ beta
    reg   = 2*alpha*np.append(0, mdl.coef_) / X.shape[0]
    return (-2*resid)[:,None]*X_aug + reg

def ewma_T2_plain(S, lam, mu, Σi):
    z = np.zeros_like(mu); out = np.empty(len(S))
    for t,s in enumerate(S):
        z = lam*s + (1-lam)*z
        d = (z-mu)[:,None]
        out[t] = (d.T @ Σi @ d).item()
    return out

def ewma_T2_k(S, lam, mu, Σi, n_train):
    """Our chart with Yeh-Apley k-factor."""
    z=np.zeros_like(mu); out=np.empty(len(S))
    for t,s in enumerate(S, start=1):
        z = lam*s + (1-lam)*z
        num = (lam/(2-lam))*(1-(1-lam)**(2*t)) + 3.72/n_train*(1-(1-lam)**t)**2
        den = (lam/(2-lam))*(1-(1-lam)**(2*t)) + 1.00/n_train*(1-(1-lam)**t)**2
        k   = np.sqrt(num/den)
        d   = (z/k - mu)[:,None]
        out[t-1] = (d.T @ Σi @ d).item()
    return out

# 4 ─── oracle-CL calibration for a given chart type ─────────────────────────
def oracle_cl(chart_type, cfg):
    """
    chart_type = 'ours' or 'kg'.  Returns constant UCL achieving α in steady-state.
    """
    pool_T2 = []
    for db in range(N_REP_DB):
        # --- Phase-I training ------------------------------------------------
        x_tr, y_tr = gen_ic(db, 2_000)
        if chart_type == 'kg':
            x_tr, y_tr = x_tr[:1_000], y_tr[:1_000]   # only first half
        scaler = StandardScaler(); X_tr = scaler.fit_transform(x_tr[:,None])
        mdl = Ridge(alpha=cfg['alpha']).fit(X_tr, y_tr)

        S_tr = score_vecs(X_tr, y_tr, mdl, cfg['alpha'])
        mu   = S_tr.mean(axis=0)
        Σi   = np.linalg.pinv(np.cov(S_tr.T)+cfg['λ_cov']*np.eye(S_tr.shape[1]),
                              hermitian=True)

        # --- one IC stream (len=1 200) --------------------------------------
        x_ic, y_ic = gen_ic(db+10_000, 1_200)
        X_ic = scaler.transform(x_ic[:,None])
        S_ic = score_vecs(X_ic, y_ic, mdl, cfg['alpha'])
        if chart_type == 'ours':
            T2 = ewma_T2_k(S_ic, LAMBDA, mu, Σi, len(x_tr))
        else:
            T2 = ewma_T2_plain(S_ic, LAMBDA, mu, Σi)
        pool_T2.extend(T2[WARM_UP:WARM_UP+STEADY_N])
    return float(np.quantile(pool_T2, 1-ALPHA))

# 5 ─── power curve for a chart type given its CL ────────────────────────────
def power_curve(chart_type, UCL, cfg):
    pod_acc = np.zeros(cfg['M'])
    for db in tqdm(range(N_REP_DB), desc=f"power:{chart_type}"):
        # model
        x_tr, y_tr = gen_ic(db, 2_000)
        if chart_type == 'kg':
            x_tr, y_tr = x_tr[:1_000], y_tr[:1_000]
        scaler=StandardScaler(); X_tr=scaler.fit_transform(x_tr[:,None])
        mdl   = Ridge(alpha=cfg['alpha']).fit(X_tr, y_tr)
        S_tr  = score_vecs(X_tr, y_tr, mdl, cfg['alpha'])
        mu    = S_tr.mean(axis=0)
        Σi    = np.linalg.pinv(np.cov(S_tr.T)+cfg['λ_cov']*np.eye(S_tr.shape[1]),
                               hermitian=True)

        # shifted stream
        x_m, y_m = gen_shift_stream(db+20_000)
        X_m = scaler.transform(x_m[:,None])
        S_m = score_vecs(X_m, y_m, mdl, cfg['alpha'])
        if chart_type == 'ours':
            T2 = ewma_T2_k(S_m, LAMBDA, mu, Σi, len(x_tr))
        else:
            T2 = ewma_T2_plain(S_m, LAMBDA, mu, Σi)
        pod_acc += (T2 > UCL)
    return pod_acc / N_REP_DB

# 6 ─── run everything ───────────────────────────────────────────────────────
if __name__ == "__main__":
    cfg = dict(alpha=0.01, λ_cov=1e-1, M=1_000)

    print("Calibrating oracle UCL for *ours* …")
    UCL_ours = oracle_cl('ours', cfg)
    print(f"  UCL_ours = {UCL_ours:.5g}")

    print("Calibrating oracle UCL for *KG* …")
    UCL_kg   = oracle_cl('kg', cfg)
    print(f"  UCL_KG   = {UCL_kg:.5g}")

    print("Computing power curves …")
    pod_ours = power_curve('ours', UCL_ours, cfg)
    pod_kg   = power_curve('kg',   UCL_kg,   cfg)

    # ——— plotting ———
    plt.figure(figsize=(10,6))
    plt.plot(pod_ours, lw=2, label="Bootstrap-MEWMA (ours)")
    plt.plot(pod_kg  , lw=2, label="Zhang & Apley (oracle CL)")
    plt.axvline(200, ls='--', c='k', label="change-point")
    plt.yscale('log'); plt.grid(alpha=.3)
    plt.xlabel("time index $i$"); plt.ylabel("Probability of Detection")
    plt.title("Oracle-adjusted power (steady-state α = 0.001)")
    plt.legend(); plt.tight_layout(); plt.show()

# ─── independent IC check (FAR) ────────────────────────────────────────────
def empirical_far(chart, UCL, cfg, N_rep=500):
    cnt, total = 0, 0
    for r in range(N_rep):
        # fit model as before
        x_tr, y_tr = gen_ic(10_000+r, 2_000)
        if chart == 'kg':
            x_tr, y_tr = x_tr[:1_000], y_tr[:1_000]
        scaler=StandardScaler(); X_tr=scaler.fit_transform(x_tr[:,None])
        mdl=Ridge(alpha=cfg['alpha']).fit(X_tr, y_tr)
        S_tr=score_vecs(X_tr, y_tr, mdl, cfg['alpha'])
        mu  = S_tr.mean(axis=0)
        Σi  = np.linalg.pinv(np.cov(S_tr.T)+cfg['λ_cov']*np.eye(S_tr.shape[1]),
                             hermitian=True)

        # one long IC stream
        x_ic,y_ic = gen_ic(20_000+r, 1_200)
        X_ic = scaler.transform(x_ic[:,None])
        S_ic = score_vecs(X_ic, y_ic, mdl, cfg['alpha'])
        if chart=='ours':
            T2 = ewma_T2_k(S_ic, LAMBDA, mu, Σi, len(x_tr))
        else:
            T2 = ewma_T2_plain(S_ic, LAMBDA, mu, Σi)

        # count exceedances AFTER warm-up
        cnt   += np.sum(T2[WARM_UP:] > UCL)
        total += len(T2) - WARM_UP
    return cnt / total


far_ours = empirical_far('ours', UCL_ours, cfg)
far_kg   = empirical_far('kg',   UCL_kg,   cfg)

print(f"steady-state FAR  (ours)  = {far_ours:.4g}")
print(f"steady-state FAR  (KG)    = {far_kg:.4g}")
print("target α = 0.001")

"""
Oracle comparison – plain EWMA, no bootstrap
• Model A: ridge on 200 points
• Model B: ridge on 100 points
• Oracle CLs from 5 000 IC obs (first 300 dropped)
"""

import numpy as np, matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler

# ——— parameters ——————————————————————————————————————————————
ALPHA    = 0.001        # desired point-wise FAR
LAMBDA   = 0.01         # EWMA smoothing
WARM_UP  = 300          # discard in CL and FAR/power
CL_POOL  = 5_000        # IC obs for each oracle CL
MON_LEN  = 1_500        # monitoring length (warm + pre + post)
RNG      = np.random.default_rng(42)

# ———  helpers ————————————————————————————————————————————————
def gen_ic(seed, n, m=16, c=5):
    r = np.random.default_rng(seed)
    x = r.uniform(-np.sqrt(3), np.sqrt(3), n)
    y = m * x + c + r.normal(0, 1, n)
    return x, y

def gen_shift_stream(seed, warm=300, pre=300, post=900):
    xw, yw = gen_ic(seed, warm)
    xp, yp = gen_ic(seed+1, pre)
    r = np.random.default_rng(seed+2)
    xo = r.uniform(-np.sqrt(3), np.sqrt(3), post)
    eps = r.normal(0, 1, post)
    mask = r.random(post) < 0.5
    yo = np.where(mask, 16*xo + 5 + eps, 12*xo + 3 + eps)
    return np.concatenate([xw, xp, xo]), np.concatenate([yw, yp, yo])

def score_vecs(X, y, mdl, alpha):
    X_aug = np.hstack([np.ones((X.shape[0],1)), X])
    beta  = np.append(mdl.intercept_, mdl.coef_)
    resid = y - X_aug @ beta
    reg   = 2*alpha*np.append(0, mdl.coef_) / X.shape[0]
    return (-2*resid)[:,None]*X_aug + reg

def ewma_T2_plain(S, lam, mu, Σi):
    z = np.zeros_like(mu); out = np.empty(len(S))
    for t,s in enumerate(S):
        z = lam*s + (1-lam)*z
        d = (z - mu)[:,None]
        out[t] = (d.T @ Σi @ d).item()
    return out

def oracle_cl(model, scaler, n_train):
    """Return constant UCL (99.9 % quantile) from 5 000 IC obs."""
    x_ref, y_ref = gen_ic(555, CL_POOL)
    S_ref = score_vecs(scaler.transform(x_ref[:,None]), y_ref, model, 0.01)

    mu  = S_ref[:n_train].mean(axis=0)
    Σi  = np.linalg.pinv(np.cov(S_ref[:n_train].T))

    T2  = ewma_T2_plain(S_ref, LAMBDA, mu, Σi)[WARM_UP:]   # drop start-up
    return float(np.quantile(T2, 1-ALPHA))

# ——— 0. fit the two models ————————————————————————————————
x200, y200 = gen_ic(0, 200)          # full data for model A

# model A (200 pts)
scalerA = StandardScaler()
XA      = scalerA.fit_transform(x200[:,None])
mdlA    = Ridge(alpha=0.01).fit(XA, y200)

# model B (first 100 pts)
scalerB = StandardScaler()
XB      = scalerB.fit_transform(x200[:100,None])
mdlB    = Ridge(alpha=0.01).fit(XB, y200[:100])

# ——— 1. oracle CLs ————————————————————————————————————————
uclA = oracle_cl(mdlA, scalerA, n_train=200)
uclB = oracle_cl(mdlB, scalerB, n_train=100)
print(f"UCL_A (200 pts) = {uclA:.5g}\nUCL_B (100 pts)  = {uclB:.5g}")

# ——— 2. monitoring stream ——————————————————————————————
x_mon, y_mon = gen_shift_stream(123, 300, 300, 900)   # len = 1500

# model A statistics
S_mA = score_vecs(scalerA.transform(x_mon[:,None]), y_mon, mdlA, 0.01)
muA  = score_vecs(XA, y200, mdlA, 0.01).mean(axis=0)
ΣiA  = np.linalg.pinv(np.cov(score_vecs(XA,y200,mdlA,0.01).T))
T2_A = ewma_T2_plain(S_mA, LAMBDA, muA, ΣiA)

# model B statistics
S_mB = score_vecs(scalerB.transform(x_mon[:,None]), y_mon, mdlB, 0.01)
muB  = score_vecs(XB, y200[:100], mdlB, 0.01).mean(axis=0)
ΣiB  = np.linalg.pinv(np.cov(score_vecs(XB,y200[:100],mdlB,0.01).T))
T2_B = ewma_T2_plain(S_mB, LAMBDA, muB, ΣiB)

# ——— 3. plot —————————————————————————————————————————————
idx = np.arange(MON_LEN)

plt.figure(figsize=(10,6))
plt.plot(idx[WARM_UP:], T2_A[WARM_UP:], c='#1f77b4', label='ours (200 pts)')
plt.plot(idx[WARM_UP:], T2_B[WARM_UP:], c='#d95f02', label='KG (100 pts)')
plt.axhline(uclA, c='#1f77b4', ls='--', label='UCL ours')
plt.axhline(uclB, c='#d95f02', ls='--', label='UCL KG')
plt.axvspan(WARM_UP, 600, facecolor='#1f77b4', alpha=0.05)    # IC region
plt.axvspan(600, MON_LEN, facecolor='#d95f02', alpha=0.05)    # OC region
plt.axvline(600, ls='--', c='k', label='shift @ 600')
plt.yscale('log')
plt.xlabel("time index $i$ (monitoring phase)")
plt.ylabel(r"$T^{2}$ statistic  (log scale)")
plt.title("Oracle-calibrated control charts (plain EWMA)")
plt.legend(); plt.grid(alpha=.3); plt.tight_layout(); plt.show()

"""
Zhang et al. (2023) – type-I error verification
Phase-I: D₁ (model fit)  +  D₂ (μ̂, Σ̂, constant UCL)

• IC distribution  y = 16 x + 5 + ε,   ε ~ N(0,1)
• EWMA on score vectors, λ = 0.01
• UCL = 99.9-th percentile of the full T² sequence on D₂   (α ≈ 0.001)
• FAR estimated on many independent IC streams (no warm-up discarded)
"""

# ───────────────────────── 0. imports ─────────────────────────
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
import multiprocessing, warnings
warnings.filterwarnings("ignore", category=UserWarning)

plt.rcParams.update({
    "font.size":14, "axes.titlesize":18, "axes.labelsize":16,
    "legend.fontsize":14, "xtick.labelsize":12, "ytick.labelsize":12,
    "figure.dpi":150
})
RNG = np.random.default_rng()

# ───────────── 1. in-control generator (curve 4.1) ───────────
def gen_ic(seed, n, m=16, c=5):
    r = np.random.default_rng(seed)
    x = r.uniform(-np.sqrt(3), np.sqrt(3), n)      # Var(x) = 1
    y = m * x + c + r.normal(0, 1, n)
    return x, y

# ───────────── 2. score vectors  &  EWMA-T² ───────────────────
def score_vecs(X, y, model, α):
    X_aug = np.hstack([np.ones((X.shape[0],1)), X])
    β     = np.append(model.intercept_, model.coef_)
    resid = y - X_aug @ β
    reg   = 2*α*np.append(0, model.coef_) / X.shape[0]
    return (-2*resid)[:,None] * X_aug + reg

def ewma_T2(S, λ, μ, Σ_inv):
    z = np.zeros_like(μ); out = np.empty(len(S))
    for t, s in enumerate(S):
        z = λ*s + (1-λ)*z
        d = (z - μ)[:,None]
        out[t] = (d.T @ Σ_inv @ d).item()
    return out

# ────── 3. one database replicate (Zhang two-phase protocol) ──
def process_db(seed, cfg):
    # 3-1. simulate stable data and split 50/50
    x, y = gen_ic(seed, cfg['n_total'])
    h = cfg['n_total'] // 2
    x_D1, y_D1, x_D2, y_D2 = x[:h], y[:h], x[h:], y[h:]

    # 3-2. fit ridge on D₁
    scaler = StandardScaler()
    X_D1   = scaler.fit_transform(x_D1[:,None])
    model  = Ridge(alpha=cfg['alpha']).fit(X_D1, y_D1)

    # 3-3. μ̂, Σ̂, and UCL from *entire* D₂ (no warm-up removed)
    X_D2 = scaler.transform(x_D2[:,None])
    S_D2 = score_vecs(X_D2, y_D2, model, cfg['alpha'])
    μ    = S_D2.mean(axis=0)
    Σ_inv = np.linalg.pinv(np.cov(S_D2.T) + cfg['λ_cov']*np.eye(S_D2.shape[1]))

    T2_D2 = ewma_T2(S_D2, cfg['λ'], μ, Σ_inv)
    UCL   = np.percentile(T2_D2, cfg['perc'])        # constant limit

    # 3-4. future IC streams → exceedance indicators
    exc = np.zeros((cfg['N_future'], cfg['M']))
    for j in range(cfg['N_future']):
        xf, yf = gen_ic(seed*cfg['N_future']+j, cfg['M'])
        Sf     = score_vecs(scaler.transform(xf[:,None]), yf, model, cfg['alpha'])
        T2_f   = ewma_T2(Sf, cfg['λ'], μ, Σ_inv)
        exc[j] = T2_f > UCL
    return exc

# ─────────────── 4. FAR driver over many DBs ────────────────
def evaluate_far(cfg, N_db=50):
    workers = max(1, multiprocessing.cpu_count()-1)
    all_exc = []
    with ProcessPoolExecutor(max_workers=workers) as pool:
        futs = [pool.submit(process_db, s, cfg) for s in range(N_db)]
        for fut in tqdm(as_completed(futs), total=N_db, desc="DB replicates"):
            all_exc.append(fut.result())
    all_exc = np.asarray(all_exc)                    # (db, rep, M)
    return pd.DataFrame({"i": np.arange(cfg['M']),
                         "FAR": all_exc.mean((0,1))})

# ───────────────────────── 5. simple FAR plot helper ────────────────────────
def plot_far(df, α_nom=0.001):
    plt.figure(figsize=(10, 6))
    plt.plot(df.i, df.FAR, lw=2, label="Empirical FAR", c="#1f77b4")
    plt.axhline(α_nom, ls="--", lw=2, c="#d95f02", label=f"Nominal α={α_nom}")
    plt.ylim(0, α_nom*100)
    plt.xlabel("Time index $i$")
    plt.ylabel("False-Alarm Rate")
    plt.title("Pointwise FAR of Zhang et al. Approach")
    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()
plot_far(df_far)# ───────────────────────── 5. simple FAR plot helper ────────────────────────

# ─────────────────────────── 6. run ─────────────────────────
if __name__ == "__main__":
    cfg = dict(
        n_total  = 2_000,      # 1 000 in D₁ + 1 000 in D₂
        M        = 1_000,      # monitoring-stream length
        N_future = 1_000,      # IC streams per DB replicate
        λ        = 0.01,       # EWMA smoothing parameter
        alpha    = 0.01,       # ridge L2 penalty
        λ_cov    = 1e-1,       # small ridge on Σ̂
        perc     = 99.9        # upper-quantile → α ≈ 0.001
    )

    df_far = evaluate_far(cfg, N_db=100)   # ↑ N_db for tighter CIs
    plot_far(df_far)

# ───────────────────────── 5. simple FAR plot helper ────────────────────────
def plot_far(df, α_nom=0.001):
    plt.figure(figsize=(10, 6))
    plt.plot(df.i, df.FAR, lw=2, label="Empirical FAR", c="#1f77b4")
    plt.axhline(α_nom, ls="--", lw=2, c="#d95f02", label=f"Nominal α={α_nom}")
    plt.ylim(0, α_nom*100)
    plt.xlabel("Time index $i$")
    plt.ylabel("False-Alarm Rate")
    plt.title("Pointwise FAR of Zhang et al. Approach")
    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()
plot_far(df_far)# ───────────────────────── 5. simple FAR plot helper ────────────────────────

#!/usr/bin/env python
# oracle_experiment.py
"""
Oracle comparison:   full-data model (200 pts)  vs.  half-data model (100 pts)

Steps
1)  Train each model on its own training set.
2)  Oracle calibration:
      • For many training replicates, simulate long IC streams
      • Collect *all* T² values, take 99.9 % quantile  → constant UCL
3)  Monitoring study:
      • 300 warm-up  + 300 IC  + 900 OC mixture
      • Evaluate FAR on the IC segment;  power on the OC segment
"""

# ─────────────────────────────────── imports ─────────────────────────────────
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import warnings; warnings.filterwarnings("ignore", category=UserWarning)

plt.rcParams.update({
    "font.size":14, "axes.titlesize":18, "axes.labelsize":16,
    "legend.fontsize":14, "xtick.labelsize":12, "ytick.labelsize":12,
    "figure.dpi":150
})
RNG = np.random.default_rng(42)

# ─────────────────────────── helpers: data & stats ──────────────────────────
def gen_ic(seed, n, m=16, c=5):
    """In-control generator   y = 16 x + 5 + ε"""
    r = np.random.default_rng(seed)
    x = r.uniform(-np.sqrt(3), np.sqrt(3), n)         # Var(x)=1
    y = m*x + c + r.normal(0,1,n)
    return x, y

def gen_shift_stream(seed, warm=300, pre=300, post=900):
    """One monitoring stream: warm-up | IC | OC mixture"""
    xw, yw = gen_ic(seed, warm)
    xp, yp = gen_ic(seed+1, pre)

    r = np.random.default_rng(seed+2)
    xo  = r.uniform(-np.sqrt(3), np.sqrt(3), post)
    eps = r.normal(0,1,post)
    mask= r.random(post) < 0.5
    yo  = np.where(mask, 16*xo+5+eps, 12*xo+3+eps)
    return np.concatenate([xw,xp,xo]), np.concatenate([yw,yp,yo])

def score_vecs(X, y, mdl, α):
    """Ridge score vector  s_i = −2 (y_i − x_iᵀβ̂) x_i  + 2αβ̂/n"""
    X_aug = np.hstack([np.ones((X.shape[0],1)), X])
    β     = np.append(mdl.intercept_, mdl.coef_)
    resid = y - X_aug @ β
    reg   = 2*α*np.append(0, mdl.coef_) / X.shape[0]
    return (-2*resid)[:,None]*X_aug + reg

def ewma_T2(S, λ, μ, Σ_inv):
    z = np.zeros_like(μ); out = np.empty(len(S))
    for t,s in enumerate(S):
        z = λ*s + (1-λ)*z
        d = (z - μ)[:,None]
        out[t] = (d.T @ Σ_inv @ d).item()
    return out

# ─────────────────────────── 1. oracle calibration ──────────────────────────
def oracle_cl(num_reps, streams_per_rep, stream_len, train_size, α):
    """
    Return constant UCL for a given training-set size.
    * Σ̂ and μ̂ come from the *same* long IC sample (true oracle).
    * No warm-up is dropped here.
    """
    T2_pool = []
    for r in tqdm(range(num_reps), desc=f"oracle train={train_size}"):
        # training data
        x_tr, y_tr = gen_ic(r, train_size)
        scaler = StandardScaler().fit(x_tr[:,None])
        mdl    = Ridge(alpha=0.01).fit(scaler.transform(x_tr[:,None]), y_tr)

        # simulate many IC streams and collect T²'s
        for s in range(streams_per_rep):
            xs, ys = gen_ic(r*10_000 + s, stream_len)
            Ss     = score_vecs(scaler.transform(xs[:,None]), ys, mdl, 0.01)
            μ      = Ss.mean(axis=0)
            Σ_inv  = np.linalg.pinv(np.cov(Ss.T))
            T2     = ewma_T2(Ss, 0.01, μ, Σ_inv)      # no warm-up removed
            T2_pool.extend(T2)

    T2_pool = np.asarray(T2_pool)
    cl      = float(np.quantile(T2_pool, 1-α))
    return cl

# ───────────────────── 2. monitoring study (FAR & power) ────────────────────
def run_monitoring(n_reps, train_size, UCL, warm=300, pre=300, post=900):
    FAR_count = 0; FAR_total = 0
    POD_hist  = np.zeros(post)

    for r in tqdm(range(n_reps), desc=f"monitor train={train_size}"):
        # training & scaler
        x_tr, y_tr = gen_ic(99_999+r, train_size)
        sc  = StandardScaler().fit(x_tr[:,None])
        mdl = Ridge(alpha=0.01).fit(sc.transform(x_tr[:,None]), y_tr)

        # reference stats for the EWMA
        # (use training sample; could also use a fresh IC set)
        S_tr = score_vecs(sc.transform(x_tr[:,None]), y_tr, mdl, 0.01)
        μ    = S_tr.mean(axis=0)
        Σ_inv= np.linalg.pinv(np.cov(S_tr.T))

        # monitoring stream
        x_m, y_m = gen_shift_stream(8888+r, warm, pre, post)
        S_mon    = score_vecs(sc.transform(x_m[:,None]), y_m, mdl, 0.01)
        T2       = ewma_T2(S_mon, 0.01, μ, Σ_inv)

        # FAR on pure-IC segment (indices warm … warm+pre-1)
        ic_slice = slice(warm, warm+pre)
        FAR_count += np.sum(T2[ic_slice] > UCL)
        FAR_total += pre

        # power on post-shift segment
        oc_slice = slice(warm+pre, warm+pre+post)
        POD_hist += (T2[oc_slice] > UCL).astype(int)

    FAR = FAR_count / FAR_total
    POD = POD_hist / n_reps
    return FAR, POD

# ─────────────────── 3. run whole experiment & plot ─────────────────────────
if __name__ == "__main__":
    # parameters
    ALPHA    = 0.001
    STREAM_L = 5_000    # length of each IC stream in oracle phase
    ORA_DB   = 100      # training replicates for oracle CL
    ORA_ST   = 50       # IC streams per training replicate
    MON_DB   = 50       # monitoring replicates
    WARM, PRE, POST = 300, 300, 900

    # ---------- oracle CLs --------------------------------------------------
    ucl_full = oracle_cl(ORA_DB, ORA_ST, STREAM_L, train_size=200, α=ALPHA)
    ucl_half = oracle_cl(ORA_DB, ORA_ST, STREAM_L, train_size=100, α=ALPHA)
    print(f"UCL_full = {ucl_full:.5g}   UCL_half = {ucl_half:.5g}")

    # ---------- monitoring: FAR & power ------------------------------------
    far_full, pod_full = run_monitoring(MON_DB, 200, ucl_full, WARM, PRE, POST)
    far_half, pod_half = run_monitoring(MON_DB, 100, ucl_half, WARM, PRE, POST)
    print(f"\nsteady-state FAR  (full) = {far_full:.4g}")
    print(f"steady-state FAR  (half) = {far_half:.4g}")

    # ---------- plots ------------------------------------------------------
    t = np.arange(POST)                         # indices 0 … 899 (shift segment)
    plt.figure(figsize=(10,6))
    plt.plot(t, pod_full, c="#1f77b4", lw=2, label="full model (200 pts)")
    plt.plot(t, pod_half, c="#d95f02", lw=2, label="half model (100 pts)")
    plt.xlabel("samples after change-point")
    plt.ylabel("Probability of detection")
    plt.title("Oracle-calibrated power curves")
    plt.grid(alpha=.3); plt.legend(); plt.tight_layout(); plt.show()

# oracle_experiment_fixed.py
"""
Oracle comparison with matched warm-up removal & consistent μ̂, Σ̂.
• warm-up W points discarded in BOTH the oracle pool and monitoring FAR.
• μ̂ and Σ̂ always estimated from the same long IC reference stream.
"""

import numpy as np, matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm;  import warnings
warnings.filterwarnings("ignore", category=UserWarning)
RNG = np.random.default_rng(42)

# ─────────────────── data generators ───────────────────
def gen_ic(seed, n, m=16, c=5):
    r = np.random.default_rng(seed)
    x = r.uniform(-np.sqrt(3), np.sqrt(3), n)
    y = m*x + c + r.normal(0,1,n)
    return x, y

def gen_shift_stream(seed, warm, pre, post):
    xw,yw = gen_ic(seed,       warm)
    xp,yp = gen_ic(seed+1,     pre)
    r     = np.random.default_rng(seed+2)
    xo    = r.uniform(-np.sqrt(3), np.sqrt(3), post)
    eps   = r.normal(0,1,post); mask = r.random(post)<.5
    yo    = np.where(mask,16*xo+5+eps,12*xo+3+eps)
    return np.r_[xw,xp,xo], np.r_[yw,yp,yo]

# ───────── score vectors & EWMA T² ─────────
def score_vecs(X,y,mdl,α):
    Xaug=np.c_[np.ones(X.shape[0]),X]
    beta=np.r_[mdl.intercept_,mdl.coef_]
    resid=y-Xaug@beta
    reg  =2*α*np.r_[0,mdl.coef_]/X.shape[0]
    return (-2*resid)[:,None]*Xaug+reg

def ewma_T2(S,lam,mu,Siginv):
    z=np.zeros_like(mu); out=np.empty(len(S))
    for t,s in enumerate(S):
        z=lam*s+(1-lam)*z
        d=(z-mu)[:,None]
        out[t]=(d.T@Siginv@d).item()
    return out

# ───────── oracle calibration (with warm-up drop) ─────────
def oracle_cl(train_n, W, stream_len, R, streams_per_R, α):
    T2_pool=[]
    for r in tqdm(range(R),desc=f"oracle n={train_n}"):
        # train model
        xtr,ytr=gen_ic(10_000+r, train_n)
        sc=StandardScaler().fit(xtr[:,None])
        mdl=Ridge(alpha=0.01).fit(sc.transform(xtr[:,None]),ytr)

        # single long IC stream to get μ̂ & Σ̂
        xs,ys=gen_ic(20_000+r, stream_len)
        S    =score_vecs(sc.transform(xs[:,None]),ys,mdl,0.01)
        mu   =S.mean(0);  Siginv=np.linalg.pinv(np.cov(S.T))

        # replicate streams_per_R small IC streams, each dropping W points
        for s in range(streams_per_R):
            xi,yi=gen_ic((30_000+r)*1000+s, W+800)   # 800 kept
            Si   =score_vecs(sc.transform(xi[:,None]),yi,mdl,0.01)
            T2   =ewma_T2(Si,0.01,mu,Siginv)[W:]     # drop first W
            T2_pool.extend(T2)

    return float(np.quantile(T2_pool,1-α))

# ───────── monitoring replicate ─────────
def run_streams(train_n,UCL,W,pre,post,Nrep):
    FARc=0; FARd=pre; POD=np.zeros(post)
    for r in tqdm(range(Nrep),desc=f"monitor n={train_n}"):
        # train
        xtr,ytr=gen_ic(40_000+r,train_n)
        sc=StandardScaler().fit(xtr[:,None])
        mdl=Ridge(alpha=0.01).fit(sc.transform(xtr[:,None]),ytr)

        # μ̂, Σ̂ from an independent IC reference (same recipe as oracle)
        xr,yr=gen_ic(50_000+r, 2000)
        Sr   =score_vecs(sc.transform(xr[:,None]),yr,mdl,0.01)
        mu   =Sr.mean(0); Siginv=np.linalg.pinv(np.cov(Sr.T))

        # monitoring stream
        xm,ym=gen_shift_stream(60_000+r,W,pre,post)
        Sm   =score_vecs(sc.transform(xm[:,None]),ym,mdl,0.01)
        T2   =ewma_T2(Sm,0.01,mu,Siginv)

        FARc += np.sum(T2[W:W+pre]>UCL)
        POD  += (T2[W+pre:]>UCL).astype(int)
    FAR=FARc/(pre*Nrep); POD/=Nrep
    return FAR,POD

# ───────── main experiment ─────────
if __name__=="__main__":
    ALPHA=0.001; WARM=300; PRE=300; POST=900
    ORA_R=100; ORA_ST=50; ORA_LEN=5000; MON_R=50

    ucl_full=oracle_cl(200,WARM,ORA_LEN,ORA_R,ORA_ST,ALPHA)
    ucl_half=oracle_cl(100,WARM,ORA_LEN,ORA_R,ORA_ST,ALPHA)
    print(f"UCL_full={ucl_full:.5g}   UCL_half={ucl_half:.5g}")

    far_f,pod_f=run_streams(200,ucl_full,WARM,PRE,POST,MON_R)
    far_h,pod_h=run_streams(100,ucl_half,WARM,PRE,POST,MON_R)
    print(f"steady-state FAR  full={far_f:.4g}   half={far_h:.4g}")

    t=np.arange(POST)
    plt.figure(figsize=(10,6))
    plt.plot(t,pod_f,lw=2,label="full model (200pts)")
    plt.plot(t,pod_h,lw=2,label="half model (100pts)")
    plt.xlabel("samples after change-point"); plt.ylabel("Probability of detection")
    plt.title("Oracle-calibrated power curves"); plt.grid(alpha=.3); plt.legend()
    plt.tight_layout(); plt.show()

# oracle_vs_half.py
"""
Oracle-calibrated comparison
────────────────────────────
* full model  : 100 training points
* half model  :  50 training points
* λ = 0.01, α = 0.001
* warm-up   : drop first 300 points everywhere
* pre-shift : next 300 points (i = 0 … 299)
* post-shift: 900 points    (i = 300 … 1199)
"""
import numpy as np, matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
np.random.seed(42)                        # global reproducibility
RNG = np.random.default_rng()

# ─────────────────────────── parameters ────────────────────────────
ALPHA   = 0.001          # desired FAR
LAMBDA  = 0.01           # EWMA λ
WARM    = 300            # points to discard (start-up)
PRE     = 300            # points kept for FAR estimate
POST    = 900            # points after change-point
IC_LEN  = 5_000          # length of each oracle reference stream
ORA_R   = 150            # # training replicates for oracle pool
ORA_ST  = 100            # IC streams per replicate (→ big pool)
MON_R   = 50             # monitoring replicates per model

# training sizes
TRAIN_FULL = 40         # full-data model
TRAIN_HALF = 20         # half-data model

# ────────────────────── data generators ────────────────────────────
def gen_ic(seed, n, m=16, c=5):
    r = np.random.default_rng(seed)
    x = r.uniform(-np.sqrt(3), np.sqrt(3), n)          # Var(x)=1
    y = m*x + c + r.normal(0, 1, n)
    return x, y

def gen_shift_stream(seed, warm, pre, post):
    """warm IC  +  pre IC  +  post mixture(16/12)"""
    xw,yw = gen_ic(seed      , warm)
    xp,yp = gen_ic(seed+1    , pre)
    r     = np.random.default_rng(seed+2)
    xo    = r.uniform(-np.sqrt(3), np.sqrt(3), post)
    eps   = r.normal(0,1,post); mask = r.random(post)<.5
    yo    = np.where(mask, 16*xo+5+eps, 12*xo+3+eps)
    return np.r_[xw,xp,xo], np.r_[yw,yp,yo]

# ───────────── score vectors & EWMA T² (no k-factor) ───────────────
def score_vecs(X,y,mdl,α):
    Xaug = np.c_[np.ones(X.shape[0]), X]
    beta = np.r_[mdl.intercept_, mdl.coef_]
    resid= y - Xaug @ beta
    reg  = 2*α * np.r_[0, mdl.coef_] / X.shape[0]
    return (-2*resid)[:,None] * Xaug + reg

def ewma_T2(S,lam,mu,Siginv):
    z = np.zeros_like(mu); out = np.empty(len(S))
    for t,s in enumerate(S):
        z   = lam*s + (1-lam)*z
        d   = (z-mu)[:,None]
        out[t] = (d.T @ Siginv @ d).item()
    return out

# ─────────────── oracle UCL (steady-state) ────────────────
def oracle_cl(train_n):
    T2_pool = []
    for r in tqdm(range(ORA_R), desc=f"oracle train={train_n}"):
        # 1° train model
        xtr,ytr = gen_ic(10_000+r, train_n)
        sc  = StandardScaler().fit(xtr[:,None])
        mdl = Ridge(alpha=0.01).fit(sc.transform(xtr[:,None]), ytr)

        # 2° μ̂, Σ̂ from *independent* IC reference (2 000 pts)
        xr,yr = gen_ic(20_000+r, 2_000)
        Sr    = score_vecs(sc.transform(xr[:,None]), yr, mdl, 0.01)
        mu    = Sr.mean(0);  Siginv = np.linalg.pinv(np.cov(Sr.T))

        # 3° many IC streams → collect T² after warm-up
        for s in range(ORA_ST):
            xs,ys = gen_ic((30_000+r)*1_000+s, IC_LEN)
            Ss    = score_vecs(sc.transform(xs[:,None]), ys, mdl, 0.01)
            T2    = ewma_T2(Ss, LAMBDA, mu, Siginv)[WARM:]  # discard WARM
            T2_pool.extend(T2)

    return float(np.quantile(T2_pool, 1-ALPHA))

# ───────────── monitoring replicates for one model ───────────────
def monitor_model(train_n, UCL):
    far_cnt = 0
    pod     = np.zeros(POST)
    for r in tqdm(range(MON_R), desc=f"monitor train={train_n}"):
        # train
        xtr,ytr = gen_ic(40_000+r, train_n)
        sc  = StandardScaler().fit(xtr[:,None])
        mdl = Ridge(alpha=0.01).fit(sc.transform(xtr[:,None]), ytr)

        # μ̂, Σ̂ from independent 2 000-pt IC stream
        xr,yr = gen_ic(50_000+r, 2_000)
        Sr    = score_vecs(sc.transform(xr[:,None]), yr, mdl, 0.01)
        mu    = Sr.mean(0);  Siginv = np.linalg.pinv(np.cov(Sr.T))

        # monitoring stream
        xm,ym = gen_shift_stream(60_000+r, WARM, PRE, POST)
        Sm    = score_vecs(sc.transform(xm[:,None]), ym, mdl, 0.01)
        T2    = ewma_T2(Sm, LAMBDA, mu, Siginv)

        # FAR on pre-shift
        far_cnt += np.sum(T2[WARM:WARM+PRE] > UCL)
        # power after change
        pod += (T2[WARM+PRE:] > UCL).astype(int)

        # keep one replicate for the chart (first one):
        if r == 0:
            chart_T2 = T2[WARM:]        # for plotting (remap index)
    far = far_cnt / (PRE * MON_R)
    pod = pod / MON_R
    return far, pod, chart_T2, UCL

# ─────────────────────────── run everything ──────────────────────────
if __name__ == "__main__":
    ucl_full = oracle_cl(TRAIN_FULL)
    ucl_half = oracle_cl(TRAIN_HALF)
    print(f"UCL_full = {ucl_full:.5g}   UCL_half = {ucl_half:.5g}")

    far_f, pod_f, chart_f, _ = monitor_model(TRAIN_FULL, ucl_full)
    far_h, pod_h, chart_h, _ = monitor_model(TRAIN_HALF, ucl_half)

    print(f"steady-state FAR  full = {far_f:.4g}")
    print(f"steady-state FAR  half = {far_h:.4g}")

    # ───── 1. Control-chart from first replicate ─────
    t = np.arange(PRE+POST)                   # 0 … 1199
    plt.figure(figsize=(10,6))
    plt.plot(t, chart_f,  label="full model (100 pts)")
    plt.plot(t, chart_h,  label="half model (50 pts)")
    plt.hlines([ucl_full], 0, PRE+POST-1, ls="--", color="#1f77b4",
               label="UCL full")
    plt.hlines([ucl_half],0, PRE+POST-1, ls="--", color="#d95f02",
               label="UCL half")
    plt.axvline(PRE, ls="--", color="k", lw=1.5, label="change-point")
    plt.axvspan(0, PRE,   color="#1f77b4", alpha=.05)
    plt.axvspan(PRE, PRE+POST, color="#d95f02", alpha=.05)
    plt.yscale("log"); plt.xlabel("time index $i$  (after warm-up)")
    plt.ylabel(r"$T^{2}$ statistic (log scale)")
    plt.title("Oracle-calibrated control charts")
    plt.legend(); plt.tight_layout()

    # ───── 2. Power curves ─────
    plt.figure(figsize=(10,6))
    plt.plot(np.arange(POST), pod_f, lw=2, label="full model (100 pts)")
    plt.plot(np.arange(POST), pod_h, lw=2, label="half model (50 pts)")
    plt.xlabel("samples after change-point"); plt.ylabel("Probability of detection")
    plt.title("Oracle-calibrated power curves")
    plt.grid(alpha=.3); plt.legend(); plt.tight_layout(); plt.show()

# oracle_vs_half_fixed.py
"""
Oracle-calibrated comparison (bug-free version)
───────────────────────────────────────────────
• full model  : 100 training points
• half model  :  50 training points
• λ = 0.01, α = 0.001  (constant UCL from oracle pool)
• warm-up     : drop first 300 points everywhere
• pre-shift   : next 300 points (i = 0 … 299)
• post-shift  : 900 points     (i = 300 … 1199)
"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from tqdm import trange, tqdm

# ─────────────────────────── parameters ────────────────────────────
ALPHA   = 0.001          # desired point-wise FAR
LAMBDA  = 0.01           # EWMA smoothing
WARM    = 300            # start-up discard
PRE     = 300            # kept for FAR estimate
POST    = 900            # post-shift horizon
IC_LEN  = 5_000          # length of each oracle reference stream
ORA_R   = 150            # # training replicates for oracle pool
ORA_ST  = 100            # IC streams per replicate  (→ big pool)
MON_R   = 50             # monitoring replicates per model

TRAIN_FULL = 100         # full-data model
TRAIN_HALF = 50          # half-data model

# ────────────────────── reproducible RNG ───────────────────────────
MASTER = np.random.default_rng(seed=42)

# ────────────────────── data generators ────────────────────────────
def gen_ic(rng: np.random.Generator, n: int, m: float = 16, c: float = 5):
    """Pure in-control data from line y = 16x + 5 + ε, Var(x)=1, ε~N(0,1)."""
    x = rng.uniform(-np.sqrt(3),  np.sqrt(3), n)
    y = m * x + c + rng.normal(0.0, 1.0, n)
    return x, y

def gen_shift_stream(rng: np.random.Generator,
                     warm: int, pre: int, post: int):
    """
    IC warm-up  +  IC pre-shift  +  OC mixture (16/12 slopes).
    All randomness comes from the *same* rng; no integer seeds needed.
    """
    xw, yw = gen_ic(rng, warm)            # warm-up IC
    xp, yp = gen_ic(rng, pre)             # pre-shift IC

    xo = rng.uniform(-np.sqrt(3),  np.sqrt(3), post)
    eps = rng.normal(0.0, 1.0, post)
    mask = rng.random(post) < 0.5
    yo   = np.where(mask, 16*xo + 5 + eps,   # component 1
                           12*xo + 3 + eps)   # component 2
    return np.r_[xw, xp, xo], np.r_[yw, yp, yo]

# ───────── score vectors & plain-EWMA T² (no k-factor) ─────────────
def score_vecs(X, y, mdl, alpha):
    """One row per observation (intercept column included)."""
    X_aug = np.c_[np.ones_like(y), X]          # add intercept term
    beta  = np.r_[mdl.intercept_, mdl.coef_]
    resid = y - X_aug @ beta
    reg   = 2*alpha * np.r_[0.0, mdl.coef_] / X_aug.shape[0]
    return (-2 * resid)[:, None] * X_aug + reg

def ewma_T2(S, lam, mu, Sig_inv):
    z   = np.zeros_like(mu)
    out = np.empty(len(S))
    for t, s in enumerate(S):
        z = lam*s + (1-lam)*z
        d = (z - mu)[:, None]
        out[t] = (d.T @ Sig_inv @ d).item()   # <─ use .item() here
    return out


# ─────────────── oracle UCL (steady-state) ────────────────
def oracle_cl(train_n: int, master_rng: np.random.Generator) -> float:
    """
    *God-mode* calibration:
      – sample ORA_R independent training databases of size train_n
      – estimate μ̂, Σ̂ from separate 2 000-pt IC reference stream
      – generate ORA_ST independent IC streams (length IC_LEN) on each db
      – collect all T² *after warm-up* → upper (1-α) quantile
    """
    pool = []
    for _ in trange(ORA_R, desc=f"oracle train={train_n}"):
        rng = np.random.default_rng(master_rng.integers(1_000_000_000))

        # 1. train model
        x_tr, y_tr = gen_ic(rng, train_n)
        scaler = StandardScaler().fit(x_tr[:, None])
        mdl    = Ridge(alpha=0.01).fit(scaler.transform(x_tr[:, None]), y_tr)

        # 2. reference stream for μ̂, Σ̂
        xr, yr = gen_ic(rng, 2_000)
        Sr     = score_vecs(scaler.transform(xr[:, None]), yr, mdl, 0.01)
        mu     = Sr.mean(axis=0)
        Sig_inv = np.linalg.pinv(np.cov(Sr.T))

        # 3. many IC streams  → T² values
        for _ in range(ORA_ST):
            xs, ys = gen_ic(rng, IC_LEN)
            Ss     = score_vecs(scaler.transform(xs[:, None]), ys, mdl, 0.01)
            T2     = ewma_T2(Ss, LAMBDA, mu, Sig_inv)[WARM:]  # skip warm-up
            pool.extend(T2)

    return float(np.quantile(pool, 1 - ALPHA))

# ──────────── monitoring replicates & metrics ────────────
def monitor_model(train_n: int, UCL: float,
                  master_rng: np.random.Generator):
    far_cnt, ttfs_list = 0, []
    pod_curve = np.zeros(POST)                     # probability-of-detection

    first_chart = None   # keep first replicate for plotting

    for r in trange(MON_R, desc=f"monitor n={train_n}"):
        rng = np.random.default_rng(master_rng.integers(1_000_000_000))

        # 1. train model
        x_tr, y_tr = gen_ic(rng, train_n)
        scaler = StandardScaler().fit(x_tr[:, None])
        mdl    = Ridge(alpha=0.01).fit(scaler.transform(x_tr[:, None]), y_tr)

        # 2. fresh μ̂, Σ̂
        xr, yr = gen_ic(rng, 2_000)
        Sr     = score_vecs(scaler.transform(xr[:, None]), yr, mdl, 0.01)
        mu     = Sr.mean(axis=0);  Sig_inv = np.linalg.pinv(np.cov(Sr.T))

        # 3. monitoring stream (IC→shift)
        xm, ym = gen_shift_stream(rng, WARM, PRE, POST)
        Sm     = score_vecs(scaler.transform(xm[:, None]), ym, mdl, 0.01)
        T2     = ewma_T2(Sm, LAMBDA, mu, Sig_inv)

        # FAR: only pre-shift section matters
        far_cnt += np.sum(T2[WARM:WARM+PRE] > UCL)

        # power: post-shift exceedances
        exc = (T2[WARM+PRE:] > UCL).astype(int)
        pod_curve += exc

        # TTFS: first 1 in exc, else POST
        idx = np.argmax(exc) if exc.any() else POST
        ttfs_list.append(idx)

        if first_chart is None:
            first_chart = (T2[WARM:], UCL)    # store for plotting

    far = far_cnt / (PRE * MON_R)
    pod = pod_curve / MON_R
    return far, pod, np.asarray(ttfs_list), first_chart

# ───────────────────────── main experiment ─────────────────────────
if __name__ == "__main__":

    # 1. oracle-level UCL’s ---------------------------------------------------
    ucl_full = oracle_cl(TRAIN_FULL, MASTER)
    ucl_half = oracle_cl(TRAIN_HALF, MASTER)
    print(f"UCL_full = {ucl_full:.5g}   UCL_half = {ucl_half:.5g}")

    # 2. monitoring phase ----------------------------------------------------
    far_f, pod_f, ttfs_f, (chart_f,_) = monitor_model(TRAIN_FULL, ucl_full, MASTER)
    far_h, pod_h, ttfs_h, (chart_h,_) = monitor_model(TRAIN_HALF, ucl_half, MASTER)

    print(f"steady-state FAR  full = {far_f:.4g}")
    print(f"steady-state FAR  half = {far_h:.4g}")

    # 3. control-chart (first replicate of each) -----------------------------
    t = np.arange(PRE + POST)
    plt.figure(figsize=(10,6))
    plt.plot(t, chart_f, label=f"full model ({TRAIN_FULL} pts)")
    plt.plot(t, chart_h, label=f"half model ({TRAIN_HALF} pts)")
    plt.hlines(ucl_full, 0, PRE+POST-1, color="#1f77b4", ls="--", lw=1.5,
               label="UCL full")
    plt.hlines(ucl_half, 0, PRE+POST-1, color="#d95f02", ls="--", lw=1.5,
               label="UCL half")
    plt.axvline(PRE, color="k", ls="--", lw=1.2, label="change-point")
    plt.axvspan(0, PRE,            color="#1f77b4", alpha=.05)
    plt.axvspan(PRE, PRE+POST,     color="#d95f02", alpha=.05)
    plt.yscale("log")
    plt.xlabel(r"time index $i$  (after warm-up)")
    plt.ylabel(r"$T^{2}$ statistic (log scale)")
    plt.title("Oracle-calibrated control charts")
    plt.legend(); plt.tight_layout()

    # 4. power curves --------------------------------------------------------
    plt.figure(figsize=(10,6))
    plt.plot(np.arange(POST), pod_f, lw=2, label=f"full model ({TRAIN_FULL})")
    plt.plot(np.arange(POST), pod_h, lw=2, label=f"half model ({TRAIN_HALF})")
    plt.xlabel("samples after change-point")
    plt.ylabel("Probability of detection")
    plt.title("Oracle-calibrated power curves")
    plt.grid(alpha=.3); plt.legend(); plt.tight_layout()

    # 5. TTFS histogram ------------------------------------------------------
    bins = np.arange(0, POST+25, 25)
    plt.figure(figsize=(10,6))
    plt.hist(ttfs_f, bins=bins, alpha=.6, label="full")
    plt.hist(ttfs_h, bins=bins, alpha=.6, label="half")
    plt.xlabel("TTFS (samples after change-point)")
    plt.ylabel("Frequency over 50 replicates")
    plt.title("Time-to-first-signal distribution")
    plt.grid(alpha=.3); plt.legend(); plt.tight_layout()

    plt.show()

# oracle_vs_half_fixed.py
"""
Oracle-calibrated KG-style comparison
─────────────────────────────────────
• full model  : 100 training points
• half model  :  50 training points
• constant UCL from oracle pool (α = 0.001)
• λ_EWMA = 0.01
• warm-up 300   |  pre-shift 300   |  post-shift 900
"""
import numpy as np, matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from tqdm import trange, tqdm

# ─────────────── experiment knobs ────────────────
ALPHA  = 0.001       # target point-wise FAR
LAMBD  = 0.01        # EWMA λ
WARM   = 300         # discarded at the chart start
PRE    = 300         # length used for FAR
POST   = 900         # length used for POD
IC_LEN = 5_000       # length of oracle IC streams
ORA_R  = 150         # # training replicates in oracle pool
ORA_ST = 100         # IC streams per replicate for oracle
MON_R  = 50          # monitoring replicates per model
N_REF  = 2_000       # size of μ̂/Σ̂ reference stream

TRAIN_FULL = 100     # “ours”
TRAIN_HALF =  50     # “KG”

MASTER = np.random.default_rng(seed=42)

# ─────────────── data generators ────────────────
def gen_ic(rng, n, m=16, c=5):
    x = rng.uniform(-np.sqrt(3), np.sqrt(3), n)
    y = m*x + c + rng.normal(0, 1, n)
    return x, y

def gen_shift_stream(rng, warm, pre, post):
    xw, yw = gen_ic(rng, warm)
    xp, yp = gen_ic(rng, pre)

    xo = rng.uniform(-np.sqrt(3), np.sqrt(3), post)
    eps = rng.normal(0, 1, post)
    yo  = np.where(rng.random(post) < .5, 16*xo + 5 + eps, 12*xo + 3 + eps)
    return np.r_[xw, xp, xo], np.r_[yw, yp, yo]

# ───────── score vectors & plain-EWMA T² ─────────
def score_vecs(X, y, mdl, alpha):
    Xaug = np.c_[np.ones_like(y), X]
    beta = np.r_[mdl.intercept_, mdl.coef_]
    resid = y - Xaug @ beta
    reg   = 2*alpha * np.r_[0, mdl.coef_] / Xaug.shape[0]
    return (-2*resid)[:, None] * Xaug + reg

def ewma_T2(S, lam, mu, Sig_inv):
    z = np.zeros_like(mu)
    out = np.empty(len(S))
    for t, s in enumerate(S):
        z = lam*s + (1-lam)*z
        d = (z - mu)[:, None]
        out[t] = (d.T @ Sig_inv @ d).item()
    return out

# ─────────────── oracle calibration ──────────────
def oracle_cl(train_n):
    pool = []
    for _ in trange(ORA_R, desc=f"oracle n={train_n}"):
        rng = np.random.default_rng(MASTER.integers(1e9))

        # train model
        x_tr, y_tr = gen_ic(rng, train_n)
        sc  = StandardScaler().fit(x_tr[:, None])
        mdl = Ridge(alpha=.01).fit(sc.transform(x_tr[:, None]), y_tr)

        # μ̂, Σ̂
        xr, yr = gen_ic(rng, N_REF)
        Sr  = score_vecs(sc.transform(xr[:, None]), yr, mdl, .01)
        mu  = Sr.mean(0)
        Sig_inv = np.linalg.pinv(np.cov(Sr.T))

        # many pure IC streams
        for _ in range(ORA_ST):
            xs, ys = gen_ic(rng, IC_LEN)
            Ss = score_vecs(sc.transform(xs[:, None]), ys, mdl, .01)
            pool.extend( ewma_T2(Ss, LAMBD, mu, Sig_inv)[WARM:] )

    return float(np.quantile(pool, 1-ALPHA))

# ─────────────── monitoring phase ────────────────
def monitor_model(train_n, UCL):
    far_curve = np.zeros(PRE)
    pod_curve = np.zeros(POST)
    ttfs      = []

    first_chart = None

    for _ in trange(MON_R, desc=f"monitor n={train_n}"):
        rng = np.random.default_rng(MASTER.integers(1e9))

        # train
        x_tr, y_tr = gen_ic(rng, train_n)
        sc  = StandardScaler().fit(x_tr[:, None])
        mdl = Ridge(alpha=.01).fit(sc.transform(x_tr[:, None]), y_tr)

        # μ̂, Σ̂
        xr, yr = gen_ic(rng, N_REF)
        Sr = score_vecs(sc.transform(xr[:, None]), yr, mdl, .01)
        mu  = Sr.mean(0);  Sig_inv = np.linalg.pinv(np.cov(Sr.T))

        # monitoring stream
        xm, ym = gen_shift_stream(rng, WARM, PRE, POST)
        Sm = score_vecs(sc.transform(xm[:, None]), ym, mdl, .01)
        T2 = ewma_T2(Sm, LAMBD, mu, Sig_inv)

        # FAR curve (point-wise)
        far_curve += (T2[WARM : WARM+PRE]  > UCL).astype(int)
        # POD curve (point-wise)
        exc = (T2[WARM+PRE:] > UCL).astype(int)
        pod_curve += exc
        # TTFS
        ttfs.append( np.argmax(exc) if exc.any() else POST )

        if first_chart is None:
            first_chart = T2[WARM:]          # keep for illustration

    far_curve /= MON_R
    pod_curve /= MON_R
    return far_curve, pod_curve, np.asarray(ttfs), first_chart

# ───────────────────── main run ─────────────────────
if __name__ == "__main__":
    ucl_full = oracle_cl(TRAIN_FULL)
    ucl_half = oracle_cl(TRAIN_HALF)
    print(f"UCL_full = {ucl_full:.5g}   UCL_half = {ucl_half:.5g}")

    far_f, pod_f, ttfs_f, chart_f = monitor_model(TRAIN_FULL, ucl_full)
    far_h, pod_h, ttfs_h, chart_h = monitor_model(TRAIN_HALF, ucl_half)

    print(f"mean FAR (full) = {far_f.mean():.4g}")
    print(f"mean FAR (half) = {far_h.mean():.4g}")

    # ───────── combined FAR & POD plot ─────────
    fig, ax = plt.subplots(figsize=(10,6))
    i_pre  = np.arange(-PRE, 0)
    i_post = np.arange(POST)

    ax.plot(i_pre,  far_f,  lw=2, label=f"full — FAR",  c="#1f77b4")
    ax.plot(i_pre,  far_h,  lw=2, label=f"half — FAR",  c="#d95f02")
    ax.axhline(ALPHA, ls="--", c="k", lw=1.5, label=f"target α={ALPHA}")

    ax.plot(i_post, pod_f,  lw=2, label=f"full — POD", c="#1f77b4")
    ax.plot(i_post, pod_h,  lw=2, label=f"half — POD", c="#d95f02")

    ax.axvline(0, ls="--", c="k", lw=1.5, label="change-point")
    ax.set_yscale("log")
    ax.set_xlim(-PRE, POST-1)
    ax.set_ylim(ALPHA/30, 1.1)
    ax.set_xlabel("time index   (pre-shift ⟵   |   ⟶ post-shift)")
    ax.set_ylabel("Probability   (log scale)")
    ax.set_title("Oracle-calibrated FAR (left) and POD (right)")
    ax.grid(alpha=.3, which="both")
    ax.legend(ncol=2)
    plt.tight_layout()

    # ───────── optional: TTFS histogram ─────────
    bins = np.arange(0, POST+25, 25)
    plt.figure(figsize=(10,6))
    plt.hist(ttfs_f, bins=bins, alpha=.6, label="full")
    plt.hist(ttfs_h, bins=bins, alpha=.6, label="half")
    plt.xlabel("TTFS (samples after change-point)")
    plt.ylabel("Frequency over 50 replicates")
    plt.title("Time-to-first-signal distribution")
    plt.grid(alpha=.3); plt.legend(); plt.tight_layout()
    plt.show()

#!/usr/bin/env python3
"""
Oracle‑calibrated comparison of two score‑vector MEWMA detectors.

––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––
Phase‑I  : train *once* on 100 points (full) and 50 points (half);
           freeze Ridge model, scaler, and Σ̂ (from TRAIN scores).
Phase‑II : build a huge in‑control pool (1000 streams × 5000 pts)
           to estimate μ (mean score) and the oracle control limit
           UCL as the (1‑α) quantile of T²; covariance is the Σ̂
           frozen from Phase‑I per Kungang et al.
Phase‑III: with the frozen detector + UCL, run monitoring replicates
           to evaluate FAR and POD.

Designed parameters follow the user’s specification.
"""

from __future__ import annotations
import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from tqdm import trange

# ───────────────────────── global knobs ──────────────────────────
ALPHA   = 0.001                # target point‑wise FAR
LAMBD   = 0.01                 # EWMA λ
WARM    = 300
PRE     = 300
POST    = 900
IC_LEN  = 1000                # per IC stream (post‑warm)
CALIB_R = 10000                # number of IC streams for oracle calibration
MON_R   = 10000                 # monitoring replicates

TRAIN_FULL = 50
TRAIN_HALF = 25
RIDGE_ALPHA = 0.01            # ℓ2 penalty used throughout

MASTER = np.random.default_rng(42)

# ───────────────────────── utilities ────────────────────────────
def gen_ic(rng: np.random.Generator, n: int, m: float = 16, c: float = 5):
    x = rng.uniform(-np.sqrt(3), np.sqrt(3), n)
    y = m * x + c + rng.normal(0, 1, n)
    return x, y

def gen_shift_stream(rng: np.random.Generator, warm: int, pre: int, post: int):
    xw, yw = gen_ic(rng, warm)
    xp, yp = gen_ic(rng, pre)

    xo = rng.uniform(-np.sqrt(3), np.sqrt(3), post)
    eps = rng.normal(0, 1, post)
    yo = np.where(rng.random(post) < 0.5,
                  16 * xo + 5 + eps,
                  12 * xo + 3 + eps)
    return np.r_[xw, xp, xo], np.r_[yw, yp, yo]

def score_vecs(X: np.ndarray, y: np.ndarray, mdl: Ridge, alpha: float):
    """Return score vectors of length p+1 (intercept included)."""
    Xaug = np.c_[np.ones_like(y), X]
    beta = np.r_[mdl.intercept_, mdl.coef_]
    resid = y - Xaug @ beta
    reg = 2 * alpha * np.r_[0, mdl.coef_] / Xaug.shape[0]
    return (-2 * resid)[:, None] * Xaug + reg

def ewma_T2(S: np.ndarray, lam: float, mu: np.ndarray, Sig_inv: np.ndarray):
    z = np.zeros_like(mu)
    T2 = np.empty(len(S))
    for t, s in enumerate(S):
        z = lam * s + (1 - lam) * z
        d = (z - mu)[:, None]
        T2[t] = (d.T @ Sig_inv @ d).item()
    return T2

# ───────────────────────── detector object ──────────────────────
@dataclass
class Detector:
    label: str
    scaler: StandardScaler
    model: Ridge
    Sig_inv: np.ndarray  # (p × p) inverse covariance from Phase‑I
    mu: np.ndarray | None = None  # filled in Phase‑II
    UCL: float | None = None      # filled in Phase‑II

# ───────────────────────── Phase I – training ───────────────────
def phase1_train(n_train: int, label: str) -> Detector:
    rng = np.random.default_rng(MASTER.integers(1e9))
    x_tr, y_tr = gen_ic(rng, n_train)

    sc = StandardScaler().fit(x_tr[:, None])
    Xtr = sc.transform(x_tr[:, None])
    mdl = Ridge(alpha=RIDGE_ALPHA).fit(Xtr, y_tr)

    S_tr = score_vecs(Xtr, y_tr, mdl, RIDGE_ALPHA)
    Sig_inv = np.linalg.pinv(np.cov(S_tr.T))

    return Detector(label, sc, mdl, Sig_inv)

# ───────────────────────── Phase II – calibration ───────────────
def phase2_calibrate(det: Detector):
    rng = np.random.default_rng(MASTER.integers(1e9))

    # First pass: build large pool of scores to estimate μ
    all_scores = []
    for _ in trange(CALIB_R, desc=f"Phase‑II pool ({det.label})", leave=False):
        x_ic, y_ic = gen_ic(rng, IC_LEN)
        S = score_vecs(det.scaler.transform(x_ic[:, None]), y_ic, det.model, RIDGE_ALPHA)
        all_scores.append(S)
    all_scores = np.vstack(all_scores)
    mu_hat = all_scores.mean(axis=0)
    det.mu = mu_hat

    # Second pass: compute T² distribution using frozen Σ and μ
    T2_vals = []
    for _ in trange(CALIB_R, desc=f"Phase‑II T² ({det.label})", leave=False):
        x_ic, y_ic = gen_ic(rng, WARM + IC_LEN)
        S = score_vecs(det.scaler.transform(x_ic[:, None]), y_ic, det.model, RIDGE_ALPHA)
        T2 = ewma_T2(S, LAMBD, mu_hat, det.Sig_inv)[WARM:]
        T2_vals.extend(T2)

    det.UCL = float(np.quantile(T2_vals, 1 - ALPHA))
    print(f"{det.label}: μ̂ set, oracle UCL = {det.UCL:.5g} from {len(T2_vals):,} stats")

# ───────────────────────── Phase III – monitoring ───────────────
@dataclass
class MonitorResult:
    FAR_curve: np.ndarray  # length PRE
    POD_curve: np.ndarray  # length POST
    TTFS:      list[int]
    example_T2: np.ndarray | None

def phase3_monitor(det: Detector) -> MonitorResult:
    far = np.zeros(PRE)
    pod = np.zeros(POST)
    ttfs = []
    example = None
    rng = np.random.default_rng(MASTER.integers(1e9))

    for r in trange(MON_R, desc=f"Phase‑III ({det.label})", leave=False):
        x_stream, y_stream = gen_shift_stream(rng, WARM, PRE, POST)
        S = score_vecs(det.scaler.transform(x_stream[:, None]), y_stream, det.model, RIDGE_ALPHA)
        T2 = ewma_T2(S, LAMBD, det.mu, det.Sig_inv)

        far += (T2[WARM:WARM+PRE] > det.UCL).astype(float)
        exc = (T2[WARM+PRE:] > det.UCL).astype(float)
        pod += exc
        ttfs.append(int(np.argmax(exc) if exc.any() else POST))

        if example is None:
            example = T2[WARM:]

    far /= MON_R
    pod /= MON_R
    return MonitorResult(far, pod, ttfs, example)

# ───────────────────────── main orchestration ───────────────────
if __name__ == "__main__":
    # Phase I
    det_full = phase1_train(TRAIN_FULL, "full (n=100)")
    det_half = phase1_train(TRAIN_HALF, "half (n=50)")

    # Phase II
    phase2_calibrate(det_full)
    phase2_calibrate(det_half)

    # Phase III
    res_full = phase3_monitor(det_full)
    res_half = phase3_monitor(det_half)

    # ────────── plotting FAR + POD on one panel ──────────
    i_pre = np.arange(-PRE, 0)
    i_post = np.arange(POST)

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(i_pre, res_full.FAR_curve,  lw=2, label="full — FAR",  c="#1f77b4")
    ax.plot(i_pre, res_half.FAR_curve, lw=2, label="half — FAR",  c="#d95f02")
    ax.axhline(ALPHA, ls="--", c="k", lw=1.2, label=f"target α={ALPHA}")

    ax.plot(i_post, res_full.POD_curve, lw=2, label="full — POD", c="#1f77b4")
    ax.plot(i_post, res_half.POD_curve, lw=2, label="half — POD", c="#d95f02")

    ax.axvline(0, ls="--", c="k", lw=1.2, label="change‑point")
    ax.set_yscale("log")
    ax.set_xlim(-PRE, POST - 1)
    ax.set_ylim(ALPHA / 30, 1.1)
    ax.set_xlabel("time index   (pre‑shift ⟵   |   ⟶ post‑shift)")
    ax.set_ylabel("Probability   (log scale)")
    ax.set_title("Oracle‑matched FAR and POD")
    ax.grid(alpha=.3, which="both")
    ax.legend(ncol=2)
    plt.tight_layout()

# ────────── plotting FAR + POD on one panel ──────────
plt.figure(figsize=(10, 6))
# --- consistent font sizes and dpi ---------------------------------
plt.rcParams.update({
    "font.size": 14,      # base size for everything
    "axes.titlesize": 18, # figure title
    "axes.labelsize": 16, # x- and y-label
    "legend.fontsize": 14,
    "xtick.labelsize": 12,
    "ytick.labelsize": 12,
    "figure.dpi": 150     # higher-res output like in helper script
})
# -------------------------------------------------------------------

# Zhang et al.'s model (full)
plt.plot(i_pre,  res_full.FAR_curve,  lw=2, c="#1f77b4", label="Our model")
plt.plot(i_post, res_full.POD_curve,  lw=2, c="#1f77b4")  # same colour for POD

# Our model (half)
plt.plot(i_pre,  res_half.FAR_curve, lw=2, c="#d95f02", label="Zhang et al.'s model")
plt.plot(i_post, res_half.POD_curve, lw=2, c="#d95f02")

# Nominal alpha line and change‑point
plt.axhline(ALPHA, ls="--", lw=2, c="k", label="Nominal α = 0.001")
plt.axvline(0,     ls="--", lw=2, c="gray", label="Change‑point")

plt.yscale("log")
plt.xlim(-PRE, POST - 1)
plt.ylim(ALPHA / 30, 1.1)

plt.xlabel("Time index $i$")
plt.ylabel("Probability (log scale)")
plt.title("Oracle‑matched FAR and POD of Zhang et al.'s Model and Our Model")
plt.grid(alpha=0.3, which="both")
plt.legend()
plt.tight_layout()
plt.show()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Algorithm-1 nested bootstrap – nonlinear oscillator, type-I verification
Nominal α = 0.01 (perc = 99.0), n_tr = 200
"""

# ── 0. imports & style ────────────────────────────────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt, tqdm, warnings, time
warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({"font.size":14, "figure.dpi":160})

RNG    = np.random.default_rng(42)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on", DEVICE.upper())

# ── 1. oscillator simulator ───────────────────────────────────────────────
BASE = dict(m1=1,m2=2,k1=1,k2=2,k3=1.5,c1=.1,c2=.2)
def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s; d=p1-p2; phi=d/(1+abs(d))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_stream(n, p, noise=0.03):
    t=np.linspace(0,30,n)
    p1,v1,p2,v2 = simulate(t,[.5,0,-.5,0],p).T
    d=p1-p2; phi=d/(1+np.abs(d))
    X=np.column_stack([p1,v1,p2,v2])
    y=(0.5*(p['m1']*v1**2+p['m2']*v2**2)
      +0.5*(p['k1']*p1**2+p['k2']*p2**2)
      +p['k3']*phi + RNG.normal(0,noise,n))
    return X,y

def make_data(seed,n_tr,n_mon):
    Xtr,ytr=gen_stream(n_tr,BASE); Xmon,ymon=gen_stream(n_mon,BASE)
    return (Xtr,ytr),(Xmon,ymon)

# ── 2. tiny MLP & helpers ────────────────────────────────────────────────
class MLP(nn.Module):
    def __init__(self,d): super().__init__(); self.net=nn.Sequential(
        nn.Linear(d,5),nn.ReLU(),nn.Linear(5,1))
    def forward(self,x): return self.net(x)

def fit_net(X,y,α,epochs,lr=1e-3):
    net=MLP(X.shape[1]).to(DEVICE)
    opt,loss_fn = optim.Adam(net.parameters(),lr=lr,weight_decay=α),nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad(); loss_fn(net(X).squeeze(),y).backward(); opt.step()
    return net

def score_vecs(X, y, net, α):
    W, b   = list(net.net[-1].parameters())
    loss_fn = nn.MSELoss()
    grads   = []
    for xi, yi in zip(X, y):
        net.zero_grad(set_to_none=True)
        (loss_fn(net(xi[None]).squeeze(), yi)
         + α*(W.square().sum() + b.square().sum())
        ).backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()

def ewma_T2(S,λ,μ,Σi,n_tr):
    z,out=np.zeros_like(μ),np.empty(len(S))
    for t,s in enumerate(S,1):
        z=λ*s+(1-λ)*z
        num=(λ/(2-λ))*(1-(1-λ)**(2*t))+(3.72/n_tr)*(1-(1-λ)**t)**2
        den=(λ/(2-λ))*(1-(1-λ)**(2*t))+(1   /n_tr)*(1-(1-λ)**t)**2
        diff=(z/np.sqrt(num/den))-μ
        out[t-1]=diff@Σi@diff
    return out

# ── 3. Algorithm-1 nested bootstrap ───────────────────────────────────────
def nested_bootstrap_CL(X,y,*,α,λ,lamΣ,BO,BI,M,perc,epochs):
    net0 = fit_net(X,y,α,epochs)                      # Step 1
    μ0, Σ0 = score_vecs(X,y,net0,α).mean(0), np.cov(score_vecs(X,y,net0,α).T)
    Σi0 = np.linalg.pinv(Σ0+lamΣ*np.eye(Σ0.shape[0]),hermitian=True)

    n=len(X); T2store=[]
    for _ in range(BO):                               # outer bootstrap
        boot = RNG.choice(n,n,replace=True); oob=np.setdiff1d(np.arange(n),boot)
        net_b = fit_net(X[boot],y[boot],α,epochs)
        Sb    = score_vecs(X[boot],y[boot],net_b,α)
        μb    = Sb.mean(0)
        Σib   = np.linalg.pinv(np.cov(Sb.T)+lamΣ*np.eye(Sb.shape[1]),hermitian=True)
        S_oob = score_vecs(X[oob],y[oob],net_b,α)
        for _ in range(BI):                           # inner bootstrap
            idx = RNG.choice(len(S_oob),M,replace=True)
            T2store.append(ewma_T2(S_oob[idx],λ,μb,Σib,n))
    CL = np.percentile(np.vstack(T2store), perc, axis=0)  # point-wise
    return CL, μ0, Σi0, net0

# ── 4. one database replicate ─────────────────────────────────────────────
def process_db(seed,cfg):
    (Xtr,ytr),_ = make_data(seed,cfg['n_tr'],0)
    sx,sy = StandardScaler(),StandardScaler()
    Xtr_t=torch.as_tensor(sx.fit_transform(Xtr),device=DEVICE,dtype=torch.float32)
    ytr_t=torch.as_tensor(sy.fit_transform(ytr[:,None]).ravel(),
                          device=DEVICE,dtype=torch.float32)

    CL, μ0, Σi0, net0 = nested_bootstrap_CL(
        Xtr_t,ytr_t,α=cfg['α'],λ=cfg['λ'],lamΣ=cfg['lamΣ'],
        BO=cfg['BO'],BI=cfg['BI'],M=cfg['M'],perc=cfg['perc'],epochs=cfg['ep'])

    alarms=[]
    for j in range(cfg['N_future']):                  # many IC streams
        _,(Xmon,ymon)=make_data(seed*cfg['N_future']+j,0,cfg['M'])
        Xmon_t=torch.as_tensor(sx.transform(Xmon),device=DEVICE,dtype=torch.float32)
        ymon_t=torch.as_tensor(sy.transform(ymon[:,None]).ravel(),
                               device=DEVICE,dtype=torch.float32)
        S = score_vecs(Xmon_t, ymon_t, net0, cfg['α'])[:cfg['M']]
        alarms.append(ewma_T2(S,cfg['λ'],μ0,Σi0,len(Xtr_t)) > CL)
    return np.vstack(alarms)          # (N_future, M)

# ── 5. FAR driver (serial, GPU-friendly) ──────────────────────────────────
def evaluate_far(cfg,N_db):
    exc=[]
    for s in tqdm.tqdm(range(N_db),desc="DB replicates"):
        exc.append(process_db(s,cfg))
    return np.vstack(exc).mean(0)     # point-wise FAR

# ── 6. main ───────────────────────────────────────────────────────────────
if __name__=="__main__":
    CFG=dict(
        n_tr   = 140,
        M      = 500,
        α      = 1e-3,
        λ      = 0.01,
        lamΣ   = 5.0,      # ← stronger Σ-ridge
        BO     = 100,      # ← raise/lower as needed
        BI     = 100,
        perc   = 99.0,
        ep     = 400,
        N_future = 1_000   # IC streams per DB
    )
    N_DB = 50

    t0=time.perf_counter()
    far = evaluate_far(CFG,N_DB)
    print(f"Elapsed {time.perf_counter()-t0:.1f} s")


    plt.figure(figsize=(10,6))
    plt.semilogy(x_plot, far_plot,lw=2,label="Empirical FAR")
    plt.axhline(0.01,ls="--",c="#d95f02",lw=2,label="Nominal α = 0.01")
    plt.ylim(1e-4,1e-0); plt.xlim(skip,CFG['M'])
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
    plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
    plt.grid(alpha=.3,which="both"); plt.legend(); plt.tight_layout(); plt.show()

# --- plotting (show all indices 0 … M-1) -----------------
x_plot   = np.arange(CFG['M'])   # 0 … 999
far_plot = far                   # entire FAR curve

plt.figure(figsize=(10,6))
plt.semilogy(x_plot, far_plot, lw=2, label="Empirical FAR")
plt.axhline(0.01, ls="--", c="#d95f02", lw=2, label="Nominal α = 0.01")
plt.ylim(1e-8, 1e-0); plt.xlim(0, CFG['M'])
plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
plt.grid(alpha=.3, which="both"); plt.legend(); plt.tight_layout(); plt.show()

# ----- plotting ------------------------------------------------------------
x_plot   = np.arange(CFG['M'])   # 0 … M-1
far_plot = far

plt.figure(figsize=(10,6))
plt.semilogy(x_plot, far_plot, lw=2, label="Empirical FAR")
plt.axhline(0.01, ls="--", c="#d95f02", lw=2, label="Nominal α = 0.01")
plt.ylim(1e-4, 1)               # adjust as you like
plt.xlim(0, CFG['M'])
plt.xlabel("Time index $i$")
plt.ylabel("False-alarm rate (log)")
plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
plt.grid(alpha=.3, which="both")
plt.legend()
plt.tight_layout()
plt.show()

# ======================================================================
# 7 · Overall (marginal) FAR estimates
# ----------------------------------------------------------------------
# far           : point-wise empirical FAR  (NumPy array, shape = (M,))
# CFG['M']      : chart length
# skip (optional) : indices you might want to discard as warm-up
# ======================================================================

# --- overall across the entire chart ----------------------------------
overall_FAR = far.mean()
print(f"\nOverall point-wise FAR  (indices 0 – {CFG['M']-1}): {overall_FAR:.6f}")

# --- overall after skipping the first `skip` indices ------------------
skip = 200                                 # ← adjust / comment out if unwanted
overall_FAR_skip = far[skip:].mean()
print(f"Overall point-wise FAR  (indices {skip} – {CFG['M']-1}): "
      f"{overall_FAR_skip:.6f}")

# --- (optional) 95 % Wald CIs for the two estimates -------------------
N_replications = N_DB * CFG['N_future']    # total IC stre_]()*

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Algorithm-1 nested bootstrap – nonlinear oscillator, type-I verification
Nominal α = 0.01 (perc = 99.0), n_tr = 200
"""

# ── 0. imports & style ────────────────────────────────────────────────────
import numpy as np, torch, torch.nn as nn, torch.optim as optim
from scipy.integrate import odeint
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt, tqdm, warnings, time
warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({"font.size":14, "figure.dpi":160})

RNG    = np.random.default_rng(4)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Running on", DEVICE.upper())

# ── 1. oscillator simulator ───────────────────────────────────────────────
BASE = dict(m1=1,m2=2,k1=1,k2=2,k3=1.5,c1=.1,c2=.2)
def simulate(t, init, p):
    m1,m2,k1,k2,k3,c1,c2 = (p[k] for k in ("m1","m2","k1","k2","k3","c1","c2"))
    def f(s, _, m1,m2,k1,k2,k3,c1,c2):
        p1,v1,p2,v2 = s; d=p1-p2; phi=d/(1+abs(d))
        return [v1,
                (-k1*p1 - c1*v1 + k3*phi)/m1,
                v2,
                (-k2*p2 - c2*v2 - k3*phi)/m2]
    return odeint(f, init, t, args=(m1,m2,k1,k2,k3,c1,c2))

def gen_stream(n, p, noise=0.03):
    t=np.linspace(0,90,n)
    p1,v1,p2,v2 = simulate(t,[.5,0,-.5,0],p).T
    d=p1-p2; phi=d/(1+np.abs(d))
    X=np.column_stack([p1,v1,p2,v2])
    y=(0.5*(p['m1']*v1**2+p['m2']*v2**2)
      +0.5*(p['k1']*p1**2+p['k2']*p2**2)
      +p['k3']*phi + RNG.normal(0,noise,n))
    return X,y

def make_data(seed,n_tr,n_mon):
    Xtr,ytr=gen_stream(n_tr,BASE); Xmon,ymon=gen_stream(n_mon,BASE)
    return (Xtr,ytr),(Xmon,ymon)

# ── 2. tiny MLP & helpers ────────────────────────────────────────────────
class MLP(nn.Module):
    def __init__(self,d): super().__init__(); self.net=nn.Sequential(
        nn.Linear(d,5),nn.ReLU(),nn.Linear(5,1))
    def forward(self,x): return self.net(x)

def fit_net(X,y,α,epochs,lr=1e-3):
    net=MLP(X.shape[1]).to(DEVICE)
    opt,loss_fn = optim.Adam(net.parameters(),lr=lr,weight_decay=α),nn.MSELoss()
    for _ in range(epochs):
        opt.zero_grad(); loss_fn(net(X).squeeze(),y).backward(); opt.step()
    return net

def score_vecs(X, y, net, α):
    W, b   = list(net.net[-1].parameters())
    loss_fn = nn.MSELoss()
    grads   = []
    for xi, yi in zip(X, y):
        net.zero_grad(set_to_none=True)
        (loss_fn(net(xi[None]).squeeze(), yi)
         + α*(W.square().sum() + b.square().sum())
        ).backward()
        grads.append(torch.cat([W.grad.flatten(), b.grad]))
    return torch.stack(grads).cpu().numpy()

def ewma_T2(S,λ,μ,Σi,n_tr):
    z,out=np.zeros_like(μ),np.empty(len(S))
    for t,s in enumerate(S,1):
        z=λ*s+(1-λ)*z
        num=(λ/(2-λ))*(1-(1-λ)**(2*t))+(3.72/n_tr)*(1-(1-λ)**t)**2
        den=(λ/(2-λ))*(1-(1-λ)**(2*t))+(1   /n_tr)*(1-(1-λ)**t)**2
        diff=(z/np.sqrt(num/den))-μ
        out[t-1]=diff@Σi@diff
    return out

# ── 3. Algorithm-1 nested bootstrap ───────────────────────────────────────
def nested_bootstrap_CL(X,y,*,α,λ,lamΣ,BO,BI,M,perc,epochs):
    net0 = fit_net(X,y,α,epochs)                      # Step 1
    μ0, Σ0 = score_vecs(X,y,net0,α).mean(0), np.cov(score_vecs(X,y,net0,α).T)
    Σi0 = np.linalg.pinv(Σ0+lamΣ*np.eye(Σ0.shape[0]),hermitian=True)

    n=len(X); T2store=[]
    for _ in range(BO):                               # outer bootstrap
        boot = RNG.choice(n,n,replace=True); oob=np.setdiff1d(np.arange(n),boot)
        net_b = fit_net(X[boot],y[boot],α,epochs)
        Sb    = score_vecs(X[boot],y[boot],net_b,α)
        μb    = Sb.mean(0)
        Σib   = np.linalg.pinv(np.cov(Sb.T)+lamΣ*np.eye(Sb.shape[1]),hermitian=True)
        S_oob = score_vecs(X[oob],y[oob],net_b,α)
        for _ in range(BI):                           # inner bootstrap
            idx = RNG.choice(len(S_oob),M,replace=True)
            T2store.append(ewma_T2(S_oob[idx],λ,μb,Σib,n))
    CL = np.percentile(np.vstack(T2store), perc, axis=0)  # point-wise
    return CL, μ0, Σi0, net0

# ── 4. one database replicate ─────────────────────────────────────────────
def process_db(seed,cfg):
    (Xtr,ytr),_ = make_data(seed,cfg['n_tr'],0)
    sx,sy = StandardScaler(),StandardScaler()
    Xtr_t=torch.as_tensor(sx.fit_transform(Xtr),device=DEVICE,dtype=torch.float32)
    ytr_t=torch.as_tensor(sy.fit_transform(ytr[:,None]).ravel(),
                          device=DEVICE,dtype=torch.float32)

    CL, μ0, Σi0, net0 = nested_bootstrap_CL(
        Xtr_t,ytr_t,α=cfg['α'],λ=cfg['λ'],lamΣ=cfg['lamΣ'],
        BO=cfg['BO'],BI=cfg['BI'],M=cfg['M'],perc=cfg['perc'],epochs=cfg['ep'])

    alarms=[]
    for j in range(cfg['N_future']):                  # many IC streams
        _,(Xmon,ymon)=make_data(seed*cfg['N_future']+j,0,cfg['M'])
        Xmon_t=torch.as_tensor(sx.transform(Xmon),device=DEVICE,dtype=torch.float32)
        ymon_t=torch.as_tensor(sy.transform(ymon[:,None]).ravel(),
                               device=DEVICE,dtype=torch.float32)
        S = score_vecs(Xmon_t, ymon_t, net0, cfg['α'])[:cfg['M']]
        alarms.append(ewma_T2(S,cfg['λ'],μ0,Σi0,len(Xtr_t)) > CL)
    return np.vstack(alarms)          # (N_future, M)

# ── 5. FAR driver (serial, GPU-friendly) ──────────────────────────────────
def evaluate_far(cfg,N_db):
    exc=[]
    for s in tqdm.tqdm(range(N_db),desc="DB replicates"):
        exc.append(process_db(s,cfg))
    return np.vstack(exc).mean(0)     # point-wise FAR

# ── 6. main ───────────────────────────────────────────────────────────────
if __name__=="__main__":
    CFG=dict(
        n_tr   = 100,
        M      = 200,
        α      = 1e-3,
        λ      = 0.01,
        lamΣ   = 2.0,
        BO     = 100,
        BI     = 200,
        perc   = 99.0,
        ep     = 1000,
        N_future = 1_000
    )
    N_DB = 50

    tic = time.perf_counter()
    far = evaluate_far(CFG,N_DB)
    print(f"\nElapsed {time.perf_counter()-tic:.1f} s  (DEVICE = {DEVICE})")

    x = np.arange(CFG['M'])
    plt.figure(figsize=(10,6))
    plt.semilogy(x, far, lw=2, label="Empirical FAR")
    plt.axhline(0.01, ls="--", c="#d95f02", lw=2, label="Nominal α = 0.01")
    plt.ylim(1e-4,1e-0); plt.xlim(0,CFG['M'])
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate (log)")
    plt.title("Non-linear oscillator – Algorithm-1 point-wise FAR")
    plt.grid(alpha=.3, which="both"); plt.legend(); plt.tight_layout(); plt.show()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Linear-model score-based EWMA — type-I error verification
(Algorithm 1 + ridge Σ̂-regulariser λ_cov·I; nominal α = 0.001)
"""
# ── 0. Imports & style ────────────────────────────────────────────────────
import numpy as np, pandas as pd, matplotlib.pyplot as plt, multiprocessing
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.model_selection import KFold

plt.rcParams.update({
    "font.size": 14, "axes.titlesize": 18, "axes.labelsize": 16,
    "legend.fontsize": 14, "xtick.labelsize": 12, "ytick.labelsize": 12,
    "figure.dpi": 150
})
RNG = np.random.default_rng()

# ── 1. Data generator ────────────────────────────────────────────────────
def generate_data(seed, n_train, n_ic, n_oc,
                  m_ic=16, c_ic=5, m1=16, c1=5, m2=12, c2=3, case='null'):
    rng = np.random.default_rng(seed)
    x_tr = rng.uniform(-np.sqrt(3), np.sqrt(3), n_train)
    y_tr = m_ic * x_tr + c_ic + rng.normal(0, 1, n_train)

    x_ic = rng.uniform(-np.sqrt(3), np.sqrt(3), n_ic)
    y_ic = m_ic * x_ic + c_ic + rng.normal(0, 1, n_ic)

    x_oc = rng.uniform(-np.sqrt(3), np.sqrt(3), n_oc)
    if case == 'null':
        y_oc = m_ic * x_oc + c_ic + rng.normal(0, 1, n_oc)
    else:
        mask, eps = rng.random(n_oc) < .5, rng.normal(0, 1, n_oc)
        y_oc = np.where(mask, m1*x_oc + c1 + eps,
                               m2*x_oc + c2 + eps)
    return (x_tr, y_tr), (np.r_[x_ic, x_oc], np.r_[y_ic, y_oc])

# ── 2. Score vectors & EWMA helpers ───────────────────────────────────────
def score_vectors(X, y, model, alpha):
    X_aug = np.hstack([np.ones((len(X), 1)), X])
    beta  = np.r_[model.intercept_, model.coef_]
    resid = y - X_aug @ beta
    reg   = 2 * alpha * np.r_[0, model.coef_] / len(X)
    return (-2 * resid)[:, None] * X_aug + reg

def ewma_T2(S, lam, μ, Σi, n_train):
    z, out = np.zeros_like(μ), np.empty(len(S))
    for i, s in enumerate(S, 1):
        z = lam * s + (1-lam) * z
        num = (lam/(2-lam)) * (1-(1-lam)**(2*i)) + (3.72/n_train)*(1-(1-lam)**i)**2
        den = (lam/(2-lam)) * (1-(1-lam)**(2*i)) + (1   /n_train)*(1-(1-lam)**i)**2
        k   = np.sqrt(num / den)
        diff = (z/k - μ)[:, None]
        out[i-1] = (diff.T @ Σi @ diff).item()
    return out

# ── 3. Nested bootstrap ──────────────────────────────────────────────────
def bootstrap_ucl(x_tr, y_tr, *, lam, alpha, λ_cov, BO, BI, M, perc):
    n, p = len(x_tr), 2                           # intercept + slope
    paths = np.empty((BO*BI, M))

    for b in range(BO):
        boot_idx = RNG.choice(n, n, replace=True)
        oob_idx  = np.setdiff1d(np.arange(n), boot_idx)

        scaler_b = StandardScaler().fit(x_tr[boot_idx, None])
        X_b  = scaler_b.transform(x_tr[boot_idx, None]);  y_b  = y_tr[boot_idx]
        X_oo = scaler_b.transform(x_tr[oob_idx,  None]);  y_oo = y_tr[oob_idx]

        model_b = Ridge(alpha=alpha).fit(X_b, y_b)

        Sb  = score_vectors(X_b,  y_b,  model_b, alpha)
        μb  = Sb.mean(0)
        Σib = np.linalg.pinv(np.cov(Sb.T) + λ_cov*np.eye(p), hermitian=True)

        S_oo = score_vectors(X_oo, y_oo, model_b, alpha)

        for j in range(BI):
            S_draw = S_oo[RNG.choice(len(S_oo), M, replace=True)]
            paths[b*BI + j] = ewma_T2(S_draw, lam, μb, Σib, n)

    return np.percentile(paths, perc, axis=0)

# ── 4. One database replicate ────────────────────────────────────────────
def process_db(seed, cfg):
    (x_tr, y_tr), _ = generate_data(seed, cfg['n_train'],
                                    cfg['n_ic'], cfg['n_oc'])

    cv = KFold(n_splits=5, shuffle=True, random_state=seed)
    alphas = np.logspace(-4, 1, 20)
    scaler = StandardScaler().fit(x_tr[:, None])
    Xtr    = scaler.transform(x_tr[:, None])

    model_cv = RidgeCV(alphas=alphas, cv=cv, scoring='neg_mean_squared_error')
    model_cv.fit(Xtr, y_tr)
    alpha_star = model_cv.alpha_

    base_model = Ridge(alpha=alpha_star).fit(Xtr, y_tr)

    S_tr = score_vectors(Xtr, y_tr, base_model, alpha_star)
    p    = S_tr.shape[1]
    Σi   = np.linalg.pinv(np.cov(S_tr.T) + cfg['λ_cov']*np.eye(p), hermitian=True)
    μ    = S_tr.mean(0)

    UCL = bootstrap_ucl(x_tr, y_tr, lam=cfg['lam'], alpha=alpha_star,
                        λ_cov=cfg['λ_cov'], BO=cfg['BO'], BI=cfg['BI'],
                        M=cfg['M'], perc=cfg['perc'])

    exc = np.zeros((cfg['N_future'], cfg['M']), dtype=bool)
    for j in range(cfg['N_future']):
        _, (x_mon, y_mon) = generate_data(seed*cfg['N_future']+j,
                                          cfg['n_train'], cfg['n_ic'],
                                          cfg['n_oc'], case='null')
        S_mon = score_vectors(scaler.transform(x_mon[:, None]), y_mon,
                              base_model, alpha_star)[:cfg['M']]
        exc[j] = ewma_T2(S_mon, cfg['lam'], μ, Σi, cfg['n_train']) > UCL
    return exc

# ── 5. FAR driver ────────────────────────────────────────────────────────
def evaluate_far(cfg, N_db=20, workers=None):
    workers = max(1, multiprocessing.cpu_count()-1) if workers is None else workers
    futures, results = [], []
    with ProcessPoolExecutor(max_workers=workers) as pool:
        for s in range(N_db):
            futures.append(pool.submit(process_db, s, cfg))
        for f in tqdm(as_completed(futures), total=N_db, desc="DB replicates"):
            results.append(f.result())

    ex  = np.array(results)                    # (N_db,N_future,M)
    far = ex.mean(axis=(0,1))
    return pd.DataFrame({"t": np.arange(cfg['M']), "FAR": far})

# ── 6. Plot helper ───────────────────────────────────────────────────────
def plot_far(df, α_nom=0.001):
    plt.figure(figsize=(10,6))
    plt.plot(df.t, df.FAR, lw=2, label="Empirical FAR")
    plt.axhline(α_nom, ls="--", lw=2, color="#d95f02", label=f"Nominal α = {α_nom}")
    plt.ylim(0, α_nom*2)
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate")
    plt.title("Point-wise FAR"); plt.grid(alpha=.3); plt.legend(); plt.tight_layout()
    plt.show()

# ── 7. Main ──────────────────────────────────────────────────────────────
if __name__ == "__main__":
    cfg = dict(
        # sample sizes
        n_train = 2_000, n_ic = 1_000, n_oc = 0, M = 1_000,

        # EWMA parameter
        lam = 0.01,

        # covariance regulariser
        λ_cov = 1e-5,               # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

        # nested bootstrap
        BO = 100, BI = 200, perc = 99.9,

        # monitoring replicates
        N_future = 5_000
    )

    print("Evaluating empirical FAR …")
    df_far = evaluate_far(cfg, N_db=50)        # raise N_db for tighter CIs
    plot_far(df_far)

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Linear-model score-based EWMA — type-I error verification
(Algorithm 1 + ridge Σ̂-regulariser λ_cov·I; nominal α = 0.001)
"""
# ── 0. Imports & style ────────────────────────────────────────────────────
import numpy as np, pandas as pd, matplotlib.pyplot as plt, multiprocessing
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.model_selection import KFold

plt.rcParams.update({
    "font.size": 14, "axes.titlesize": 18, "axes.labelsize": 16,
    "legend.fontsize": 14, "xtick.labelsize": 12, "ytick.labelsize": 12,
    "figure.dpi": 150
})
RNG = np.random.default_rng()

# ── 1. Data generator ────────────────────────────────────────────────────
def generate_data(seed, n_train, n_ic, n_oc,
                  m_ic=16, c_ic=5, m1=16, c1=5, m2=12, c2=3, case='null'):
    rng = np.random.default_rng(seed)
    x_tr = rng.uniform(-np.sqrt(3), np.sqrt(3), n_train)
    y_tr = m_ic * x_tr + c_ic + rng.normal(0, 1, n_train)

    x_ic = rng.uniform(-np.sqrt(3), np.sqrt(3), n_ic)
    y_ic = m_ic * x_ic + c_ic + rng.normal(0, 1, n_ic)

    x_oc = rng.uniform(-np.sqrt(3), np.sqrt(3), n_oc)
    if case == 'null':
        y_oc = m_ic * x_oc + c_ic + rng.normal(0, 1, n_oc)
    else:
        mask, eps = rng.random(n_oc) < .5, rng.normal(0, 1, n_oc)
        y_oc = np.where(mask, m1*x_oc + c1 + eps,
                               m2*x_oc + c2 + eps)
    return (x_tr, y_tr), (np.r_[x_ic, x_oc], np.r_[y_ic, y_oc])

# ── 2. Score vectors & EWMA helpers ───────────────────────────────────────
def score_vectors(X, y, model, alpha):
    X_aug = np.hstack([np.ones((len(X), 1)), X])
    beta  = np.r_[model.intercept_, model.coef_]
    resid = y - X_aug @ beta
    reg   = 2 * alpha * np.r_[0, model.coef_] / len(X)
    return (-2 * resid)[:, None] * X_aug + reg

def ewma_T2(S, lam, μ, Σi, n_train):
    z, out = np.zeros_like(μ), np.empty(len(S))
    for i, s in enumerate(S, 1):
        z = lam * s + (1-lam) * z
        num = (lam/(2-lam)) * (1-(1-lam)**(2*i)) + (3.72/n_train)*(1-(1-lam)**i)**2
        den = (lam/(2-lam)) * (1-(1-lam)**(2*i)) + (1   /n_train)*(1-(1-lam)**i)**2
        k   = np.sqrt(num / den)
        diff = (z/k - μ)[:, None]
        out[i-1] = (diff.T @ Σi @ diff).item()
    return out

# ── 3. Nested bootstrap ──────────────────────────────────────────────────
def bootstrap_ucl(x_tr, y_tr, *, lam, alpha, λ_cov, BO, BI, M, perc):
    n, p = len(x_tr), 2                           # intercept + slope
    paths = np.empty((BO*BI, M))

    for b in range(BO):
        boot_idx = RNG.choice(n, n, replace=True)
        oob_idx  = np.setdiff1d(np.arange(n), boot_idx)

        scaler_b = StandardScaler().fit(x_tr[boot_idx, None])
        X_b  = scaler_b.transform(x_tr[boot_idx, None]);  y_b  = y_tr[boot_idx]
        X_oo = scaler_b.transform(x_tr[oob_idx,  None]);  y_oo = y_tr[oob_idx]

        model_b = Ridge(alpha=alpha).fit(X_b, y_b)

        Sb  = score_vectors(X_b,  y_b,  model_b, alpha)
        μb  = Sb.mean(0)
        Σib = np.linalg.pinv(np.cov(Sb.T) + λ_cov*np.eye(p), hermitian=True)

        S_oo = score_vectors(X_oo, y_oo, model_b, alpha)

        for j in range(BI):
            S_draw = S_oo[RNG.choice(len(S_oo), M, replace=True)]
            paths[b*BI + j] = ewma_T2(S_draw, lam, μb, Σib, n)

    return np.percentile(paths, perc, axis=0)

# ── 4. One database replicate ────────────────────────────────────────────
def process_db(seed, cfg):
    (x_tr, y_tr), _ = generate_data(seed, cfg['n_train'],
                                    cfg['n_ic'], cfg['n_oc'])

    cv = KFold(n_splits=5, shuffle=True, random_state=seed)
    alphas = np.logspace(-4, 1, 20)
    scaler = StandardScaler().fit(x_tr[:, None])
    Xtr    = scaler.transform(x_tr[:, None])

    model_cv = RidgeCV(alphas=alphas, cv=cv, scoring='neg_mean_squared_error')
    model_cv.fit(Xtr, y_tr)
    alpha_star = model_cv.alpha_

    base_model = Ridge(alpha=alpha_star).fit(Xtr, y_tr)

    S_tr = score_vectors(Xtr, y_tr, base_model, alpha_star)
    p    = S_tr.shape[1]
    Σi   = np.linalg.pinv(np.cov(S_tr.T) + cfg['λ_cov']*np.eye(p), hermitian=True)
    μ    = S_tr.mean(0)

    UCL = bootstrap_ucl(x_tr, y_tr, lam=cfg['lam'], alpha=alpha_star,
                        λ_cov=cfg['λ_cov'], BO=cfg['BO'], BI=cfg['BI'],
                        M=cfg['M'], perc=cfg['perc'])

    exc = np.zeros((cfg['N_future'], cfg['M']), dtype=bool)
    for j in range(cfg['N_future']):
        _, (x_mon, y_mon) = generate_data(seed*cfg['N_future']+j,
                                          cfg['n_train'], cfg['n_ic'],
                                          cfg['n_oc'], case='null')
        S_mon = score_vectors(scaler.transform(x_mon[:, None]), y_mon,
                              base_model, alpha_star)[:cfg['M']]
        exc[j] = ewma_T2(S_mon, cfg['lam'], μ, Σi, cfg['n_train']) > UCL
    return exc

# ── 5. FAR driver ────────────────────────────────────────────────────────
def evaluate_far(cfg, N_db=20, workers=None):
    workers = max(1, multiprocessing.cpu_count()-1) if workers is None else workers
    futures, results = [], []
    with ProcessPoolExecutor(max_workers=workers) as pool:
        for s in range(N_db):
            futures.append(pool.submit(process_db, s, cfg))
        for f in tqdm(as_completed(futures), total=N_db, desc="DB replicates"):
            results.append(f.result())

    ex  = np.array(results)                    # (N_db,N_future,M)
    far = ex.mean(axis=(0,1))
    return pd.DataFrame({"t": np.arange(cfg['M']), "FAR": far})

# ── 6. Plot helper ───────────────────────────────────────────────────────
def plot_far(df, α_nom=0.001):
    plt.figure(figsize=(10,6))
    plt.plot(df.t, df.FAR, lw=2, label="Empirical FAR")
    plt.axhline(α_nom, ls="--", lw=2, color="#d95f02", label=f"Nominal α = {α_nom}")
    plt.ylim(0, α_nom*2)
    plt.xlabel("Time index $i$"); plt.ylabel("False-alarm rate")
    plt.title("Point-wise FAR"); plt.grid(alpha=.3); plt.legend(); plt.tight_layout()
    plt.show()

# ── 7. Main ──────────────────────────────────────────────────────────────
if __name__ == "__main__":
    cfg = dict(
        # sample sizes
        n_train = 2_000, n_ic = 1_000, n_oc = 0, M = 1_000,

        # EWMA parameter
        lam = 0.01,

        # covariance regulariser
        λ_cov = 1e-5,               # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

        # nested bootstrap
        BO = 200, BI = 500, perc = 99.9,

        # monitoring replicates
        N_future = 5_000
    )

    print("Evaluating empirical FAR …")
    df_far = evaluate_far(cfg, N_db=5)        # raise N_db for tighter CIs
    plot_far(df_far)

"""
=======================================================================
Linear-model score-based EWMA – type-I error verification
=======================================================================
Left curve  : constant UCL (Zhang et al. 2023 protocol)
Right curve : bootstrap UCL  (bias-corrected k-factor)
Nominal α   : 0.001   (UCL = 99.9-th percentile)
"""

# ───────────────────────────────── 0. imports ───────────────────────────────
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing, warnings, itertools, tqdm

warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({
    "font.size":14, "axes.titlesize":18, "axes.labelsize":16,
    "legend.fontsize":14, "xtick.labelsize":12, "ytick.labelsize":12,
    "figure.dpi":150
})
RNG = np.random.default_rng()

# ───────────────────────────── 1. shared helpers ────────────────────────────
def gen_ic(seed, n, m=16, c=5):
    """IC generator   y = m·x + c + ε ,   ε∼N(0,1)  ,  Var(x)=1."""
    r = np.random.default_rng(seed)
    x = r.uniform(-np.sqrt(3), np.sqrt(3), n)
    y = m*x + c + r.normal(0, 1, n)
    return x, y

def score_vecs(X, y, model, α):
    X_aug = np.hstack([np.ones((X.shape[0],1)), X])
    β     = np.append(model.intercept_, model.coef_)
    resid = y - X_aug @ β
    reg   = 2*α*np.append(0, model.coef_) / X.shape[0]
    return (-2*resid)[:,None]*X_aug + reg

def ewma_T2(S, λ, μ, Σ_inv):
    z   = np.zeros_like(μ)
    out = np.empty(len(S))
    for t,s in enumerate(S):
        z = λ*s + (1-λ)*z
        d = (z-μ)[:,None]
        out[t] = (d.T @ Σ_inv @ d).item()
    return out

# ─────────────── 2-A. constant-UCL (two-phase) replicate ────────────────────
def db_constant(seed, cfg):
    # split stable data 50/50
    x, y = gen_ic(seed, cfg['n_total'])
    h = cfg['n_total']//2
    scaler = StandardScaler()
    X_D1   = scaler.fit_transform(x[:h,None])
    model  = Ridge(alpha=cfg['alpha']).fit(X_D1, y[:h])

    # μ̂, Σ̂, T² on D₂
    X_D2 = scaler.transform(x[h:,None])
    S_D2 = score_vecs(X_D2, y[h:], model, cfg['alpha'])
    μ     = S_D2.mean(0)
    Σ_inv = np.linalg.pinv(np.cov(S_D2.T)+cfg['λ_cov']*np.eye(S_D2.shape[1]))
    UCL   = np.percentile(ewma_T2(S_D2, cfg['λ'], μ, Σ_inv), cfg['perc'])

    exc = np.zeros((cfg['N_future'], cfg['M']))
    for j in range(cfg['N_future']):
        xf,yf = gen_ic(seed*cfg['N_future']+j, cfg['M'])
        Sf    = score_vecs(scaler.transform(xf[:,None]), yf, model, cfg['alpha'])
        exc[j] = ewma_T2(Sf, cfg['λ'], μ, Σ_inv) > UCL
    return exc

# ─────────────── 2-B. bootstrap-UCL replicate (bias-corr. k) ────────────────
def ewma_T2_k(S, λ, μ, Σ_inv, n_train):
    """EWMA-T² with bias-corr. k-factor (Lucas & Saccucci style)."""
    z, out = np.zeros_like(μ), np.empty(len(S))
    for t,s in enumerate(S, start=1):
        z = λ*s + (1-λ)*z
        num = (λ/(2-λ))*(1-(1-λ)**(2*t)) + (3.72/n_train)*(1-(1-λ)**t)**2
        den = (λ/(2-λ))*(1-(1-λ)**(2*t)) + (1   /n_train)*(1-(1-λ)**t)**2
        k   = np.sqrt(num/den)
        d   = (z/k - μ)[:,None]
        out[t-1] = (d.T @ Σ_inv @ d).item()
    return out

def bootstrap_ucl(x_tr, y_tr, cfg):
    """Outer BO×BI bootstrap → distribution of EWMA-T² with k."""
    scaler = StandardScaler(); Xtr = scaler.fit_transform(x_tr[:,None])
    n      = len(x_tr)
    model  = Ridge(alpha=cfg['alpha']).fit(Xtr, y_tr)
    S_tr   = score_vecs(Xtr, y_tr, model, cfg['alpha'])
    μ      = S_tr.mean(0)
    Σ_inv  = np.linalg.pinv(np.cov(S_tr.T)+cfg['λ_cov']*np.eye(S_tr.shape[1]))

    res = np.empty((cfg['BO']*cfg['BI'], cfg['M']))
    for b in range(cfg['BO']):
        idx_boot = RNG.choice(n, n, True)
        idx_oob  = np.setdiff1d(np.arange(n), idx_boot)
        # OOB scores
        S_oob = score_vecs(Xtr[idx_oob], y_tr[idx_oob],
                           Ridge(alpha=cfg['alpha']).fit(Xtr[idx_boot], y_tr[idx_boot]),
                           cfg['alpha'])
        for j in range(cfg['BI']):
            S_draw = S_oob[RNG.choice(len(S_oob), cfg['M'], True)]
            res[b*cfg['BI']+j] = ewma_T2_k(S_draw, cfg['λ'], μ, Σ_inv, n)
    return np.percentile(res, cfg['perc'], 0)

def db_bootstrap(seed, cfg):
    # training data
    x_tr,y_tr = gen_ic(seed, cfg['n_train'])
    scaler = StandardScaler(); Xtr = scaler.fit_transform(x_tr[:,None])
    model  = Ridge(alpha=cfg['alpha']).fit(Xtr, y_tr)
    S_tr   = score_vecs(Xtr, y_tr, model, cfg['alpha'])
    μ      = S_tr.mean(0)
    Σ_inv  = np.linalg.pinv(np.cov(S_tr.T)+cfg['λ_cov']*np.eye(S_tr.shape[1]))
    # UCL via bootstrap
    UCL    = bootstrap_ucl(x_tr, y_tr, cfg)

    exc = np.zeros((cfg['N_future'], cfg['M']))
    for j in range(cfg['N_future']):
        xf,yf = gen_ic(seed*cfg['N_future']+j, cfg['M'])
        Sf    = score_vecs(scaler.transform(xf[:,None]), yf, model, cfg['alpha'])
        exc[j] = ewma_T2_k(Sf, cfg['λ'], μ, Σ_inv, cfg['n_train']) > UCL
    return exc

# ─────────────── 3. generic FAR driver (works for either rep fn) ────────────
def evaluate_far(rep_fn, cfg, N_db=20):
    workers = max(1, multiprocessing.cpu_count()-1)
    all_exc = []
    with ProcessPoolExecutor(max_workers=workers) as pool:
        futs = [pool.submit(rep_fn, s, cfg) for s in range(N_db)]
        for fut in tqdm.tqdm(as_completed(futs), total=N_db,
                             desc=f"{rep_fn.__name__} DB replicates"):
            all_exc.append(fut.result())
    all_exc = np.asarray(all_exc)                         # (db, rep, M)
    return all_exc.mean((0,1))

# ───────────────────────────── 4. configuration ─────────────────────────────
cfg_const = dict(
    n_total  = 2_000,     # 1 000 + 1 000
    M        = 1_000,
    N_future = 1_000,
    λ        = 0.01,
    alpha    = 0.01,
    λ_cov    = 1e-1,
    perc     = 99.9
)
cfg_boot  = dict(
    n_train  = 2_000,     # train set
    M        = 1_000,
    N_future = 1_000,
    λ        = 0.01,
    alpha    = 0.01,
    λ_cov    = 1e-2,
    BO       = 100,
    BI       = 200,
    perc     = 99.9
)

# ───────────────────────────── 5. run & plot ────────────────────────────────
if __name__ == "__main__":
    print("Running constant-UCL experiment …")
    far_const = evaluate_far(db_constant,  cfg_const, N_db=50)
    print("Running bootstrap-UCL experiment …")
    far_boot  = evaluate_far(db_bootstrap, cfg_boot,  N_db=50)

    t = np.arange(cfg_const['M'])
    fig, ax = plt.subplots(figsize=(10,6))
    ax.semilogy(t, far_const, lw=2, label="Zhang et al's Two-sample, Constant CL")
    ax.semilogy(t, far_boot , lw=2, label="Our Bootstrap CL")
    ax.axhline(0.001, ls='--', lw=2, color='#d95f02', label="Nominal α=0.001")

    ax.set_xlabel("Time index $i$")
    ax.set_ylabel("False-Alarm Rate  (Log Scale)")
    ax.set_title("Point-wise FAR Comparison")
    ax.set_ylim(1e-6, 1e-0)          # adjust as desired
    ax.grid(alpha=0.3, which='both'); ax.legend(); plt.tight_layout(); plt.show()

"""
=======================================================================
Linear-model score-based EWMA — type-I error verification
=======================================================================
Left curve  : constant UCL  (Zhang et al. 2023 two-phase)
Right curve : bootstrap UCL (vector CL, no-k statistic)   ← matches old file
Nominal α   : 0.001   (CL = 99.9-th percentile)
"""

# ───────────────────────────── 0. imports & style ──────────────────────────
import numpy as np, matplotlib.pyplot as plt, tqdm, warnings, multiprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from concurrent.futures import ProcessPoolExecutor, as_completed

warnings.filterwarnings("ignore", category=UserWarning)
plt.rcParams.update({
    "font.size":14, "axes.titlesize":18, "axes.labelsize":16,
    "legend.fontsize":14, "xtick.labelsize":12, "ytick.labelsize":12,
    "figure.dpi":150
})
RNG = np.random.default_rng()

# ───────────────────────────── 1. shared helpers ───────────────────────────
def gen_ic(seed, n, m=16, c=5):
    r = np.random.default_rng(seed)
    x = r.uniform(-np.sqrt(3), np.sqrt(3), n)
    y = m*x + c + r.normal(0, 1, n)
    return x, y

def score_vecs(X, y, model, α):
    X_aug = np.hstack([np.ones((X.shape[0],1)), X])
    β     = np.append(model.intercept_, model.coef_)
    resid = y - X_aug @ β
    reg   = 2*α*np.append(0, model.coef_) / X.shape[0]
    return (-2*resid)[:,None] * X_aug + reg

def ewma_T2(S, λ, μ, Σ_inv):
    z, out = np.zeros_like(μ), np.empty(len(S))
    for t,s in enumerate(S):
        z = λ*s + (1-λ)*z
        d = (z-μ)[:,None]
        out[t] = (d.T @ Σ_inv @ d).item()
    return out

def ewma_T2_k(S, λ, μ, Σ_inv, n_tr):
    z, out = np.zeros_like(μ), np.empty(len(S))
    for t,s in enumerate(S,1):
        z = λ*s + (1-λ)*z
        num=(λ/(2-λ))*(1-(1-λ)**(2*t))+(3.72/n_tr)*(1-(1-λ)**t)**2
        den=(λ/(2-λ))*(1-(1-λ)**(2*t))+(1   /n_tr)*(1-(1-λ)**t)**2
        d  = (z/np.sqrt(num/den)-μ)[:,None]
        out[t-1] = (d.T @ Σ_inv @ d).item()
    return out

# ─────────────── 2-A. constant-UCL (Zhang two-phase) replicate ─────────────
def db_constant(seed, cfg):
    x,y = gen_ic(seed, cfg['n_total'])
    h   = cfg['n_total']//2
    scaler=StandardScaler()
    X1 = scaler.fit_transform(x[:h,None])
    model=Ridge(alpha=cfg['alpha']).fit(X1, y[:h])

    X2   = scaler.transform(x[h:,None])
    S2   = score_vecs(X2, y[h:], model, cfg['alpha'])
    μ    = S2.mean(0)
    Σinv = np.linalg.pinv(np.cov(S2.T)+cfg['λ_cov']*np.eye(S2.shape[1]))
    UCL  = np.percentile(ewma_T2(S2, cfg['λ'], μ, Σinv), cfg['perc'])

    exc=np.zeros((cfg['N_future'], cfg['M']))
    for j in range(cfg['N_future']):
        xf,yf=gen_ic(seed*cfg['N_future']+j, cfg['M'])
        Sf=score_vecs(scaler.transform(xf[:,None]), yf, model, cfg['alpha'])
        exc[j]=ewma_T2(Sf, cfg['λ'], μ, Σinv) > UCL
    return exc

# ─────────────── 2-B. bootstrap-UCL replicate (no-k statistic) ─────────────
def bootstrap_ucl(x, y, cfg):
    scaler=StandardScaler(); X=scaler.fit_transform(x[:,None])
    model=Ridge(alpha=cfg['alpha']).fit(X, y)
    S    = score_vecs(X, y, model, cfg['alpha'])
    μ,Σinv=S.mean(0), np.linalg.pinv(np.cov(S.T)+cfg['λ_cov']*np.eye(S.shape[1]))
    n=len(x)
    res=np.empty((cfg['BO']*cfg['BI'], cfg['M']))
    for b in range(cfg['BO']):
        boot=RNG.choice(n,n,True); oob=np.setdiff1d(np.arange(n),boot)
        model_b=Ridge(alpha=cfg['alpha']).fit(X[boot],y[boot])
        S_oob  = score_vecs(X[oob], y[oob], model_b, cfg['alpha'])
        for j in range(cfg['BI']):
            S_draw=S_oob[RNG.choice(len(S_oob), cfg['M'], True)]
            res[b*cfg['BI']+j]=ewma_T2_k(S_draw,cfg['λ'],μ,Σinv,n)   # k IN CL
    return np.percentile(res, cfg['perc'],0)

def db_bootstrap(seed, cfg):
    x_tr,y_tr = gen_ic(seed, cfg['n_train'])
    scaler=StandardScaler(); Xtr=scaler.fit_transform(x_tr[:,None])
    model=Ridge(alpha=cfg['alpha']).fit(Xtr,y_tr)
    S_tr=score_vecs(Xtr,y_tr,model,cfg['alpha'])
    μ,Σinv=S_tr.mean(0), np.linalg.pinv(np.cov(S_tr.T)+cfg['λ_cov']*np.eye(S_tr.shape[1]))
    UCL = bootstrap_ucl(x_tr, y_tr, cfg)   # vector CL (k-factor inside)

    exc=np.zeros((cfg['N_future'], cfg['M']))
    for j in range(cfg['N_future']):
        xf,yf=gen_ic(seed*cfg['N_future']+j, cfg['M'])
        Sf=score_vecs(scaler.transform(xf[:,None]), yf, model, cfg['alpha'])
        exc[j]=ewma_T2(Sf, cfg['λ'], μ, Σinv) > UCL         # ← no-k statistic
    return exc

# ─────────────── 3. FAR evaluator (shared) ────────────────────────────────
def evaluate_far(rep_fn, cfg, N_db=20):
    workers=max(1,multiprocessing.cpu_count()-1)
    all_exc=[]
    with ProcessPoolExecutor(max_workers=workers) as pool:
        futs=[pool.submit(rep_fn,s, cfg) for s in range(N_db)]
        for fut in tqdm.tqdm(as_completed(futs), total=N_db,
                             desc=f"{rep_fn.__name__} DBs"):
            all_exc.append(fut.result())
    return np.asarray(all_exc).mean((0,1))

# ─────────────── 4. configuration ─────────────────────────────────────────
cfg_const=dict(
    n_total  = 2_000,
    M=1_000, N_future=1_000,
    λ=0.01, alpha=0.01, λ_cov=1e-1, perc=99.9
)
cfg_boot =dict(
    n_train = 2_000,
    M=1_000, N_future=1_000,
    λ=0.01, alpha=0.01, λ_cov=1e-2,
    BO=100, BI=200, perc=99.9
)

# ─────────────── 5. run & plot (linear y-axis) ────────────────────────────
if __name__=="__main__":
    print("Running constant-UCL …");  f_const=evaluate_far(db_constant, cfg_const, N_db=50)
    print("Running bootstrap-UCL …"); f_boot =evaluate_far(db_bootstrap, cfg_boot , N_db=50)

    t=np.arange(cfg_const['M'])
    fig, ax = plt.subplots(figsize=(10,6))
    ax.semilogy(t, far_const, lw=2, label="Zhang et al's Two-sample, Constant CL")
    ax.semilogy(t, far_boot , lw=2, label="Our Bootstrap CL")
    ax.axhline(0.001, ls='--', lw=2, color='#d95f02', label="Nominal α=0.001")

    ax.set_xlabel("Time index $i$")
    ax.set_ylabel("False-alarm rate")
    ax.set_title("Point-wise FAR Comparison")
    ax.set_ylim(1e-6, 1e-0)
    ax.grid(alpha=0.3); ax.legend(); plt.tight_layout(); plt.show()

"""
Linear-model score-based EWMA — type-I error verification
( pointwise False–Alarm Rate ≈ nominal α = 0.001 )
"""

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 0. Imports & global style
# ╚═══════════════════════════════════════════════════════════════════════════╝
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
from tqdm import tqdm

# --- publication fonts / colours -------------------------------------------
plt.rcParams.update({
    "font.size"       : 14,
    "axes.titlesize"  : 18,
    "axes.labelsize"  : 16,
    "legend.fontsize" : 14,
    "xtick.labelsize" : 12,
    "ytick.labelsize" : 12,
    "figure.dpi"      : 150
})

RNG = np.random.default_rng()


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 1.  Data generator
# ╚═══════════════════════════════════════════════════════════════════════════╝
# ─────────────────────────────────────────────────────────────────────────────
#  HEADLINE  (for plots / paper caption)
# ─────────────────────────────────────────────────────────────────────────────
"""
Linear-model score-based EWMA — type-I error verification
( pointwise False-Alarm Rate ≈ nominal α = 0.001 )

Set-up
  • Training & IC data     :  y = 16 x + 5 + ε              ← curve (4.1)
  • OC data for “power”    :  50 %  y = 16 x + 5 + ε
                              50 %  y = 12 x + 3 + ε        ← curve (4.2)
    (For FAR studies we keep case='null', therefore every stream is IC.)
"""

# … imports & plotting style unchanged …


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 1. Data generator  (adapted slopes & intercepts)
# ╚═══════════════════════════════════════════════════════════════════════════╝
def generate_data(seed, n_train, n_ic, n_oc,
                  m_ic=16,  c_ic=5,                 # ← curve (4.1)
                  m1=16, c1=5,                      # component 1   (4.1)
                  m2=12, c2=3,                      # component 2   (4.2)
                  case='null'):
    """
    • In-control (training & IC monitoring):
          y = m_ic · x + c_ic + ε          (here 16x + 5)
    • OC stream for power studies (case!='null'):
          50 %  y = m1 · x + c1 + ε        (16x + 5)
          50 %  y = m2 · x + c2 + ε        (12x + 3)
    • For type-I/FAR runs we pass case='null' so the mixture is *not* used.
    """
    rng = np.random.default_rng(seed)

    # training sample
    x_tr = rng.uniform(-np.sqrt(3), np.sqrt(3), n_train)
    y_tr = m_ic * x_tr + c_ic + rng.normal(0, 1, n_train)

    # in-control monitoring slice
    x_ic = rng.uniform(-np.sqrt(3), np.sqrt(3), n_ic)
    y_ic = m_ic * x_ic + c_ic + rng.normal(0, 1, n_ic)

    # out-of-control slice (only if we later set case!='null')
    x_oc = rng.uniform(-np.sqrt(3), np.sqrt(3), n_oc)
    if case == 'null':
        y_oc = m_ic * x_oc + c_ic + rng.normal(0, 1, n_oc)
    else:
        mask = rng.random(n_oc) < 0.5
        ε    = rng.normal(0, 1, n_oc)
        y_oc = np.where(mask,
                        m1 * x_oc + c1 + ε,   # curve (4.1)
                        m2 * x_oc + c2 + ε)   # curve (4.2)

    # concatenate IC + OC to form full monitoring stream
    x_mon = np.concatenate([x_ic, x_oc])
    y_mon = np.concatenate([y_ic, y_oc])

    return (x_tr, y_tr), (x_mon, y_mon), n_ic



# ╔═══════════════════════════════════════════════════════════════════════════╗
# 2.  Score vectors and EWMA helpers
# ╚═══════════════════════════════════════════════════════════════════════════╝
def score_vectors(X, y, model, alpha):
    X_aug = np.hstack([np.ones((X.shape[0], 1)), X])
    β     = np.append(model.intercept_, model.coef_)
    resid = y - X_aug @ β
    reg   = 2 * alpha * np.append(0, model.coef_) / X.shape[0]
    return (-2 * resid)[:, None] * X_aug + reg


def ewma_stats_no_k(S, lam, μ, Σ_inv):
    z = np.zeros_like(μ)
    out = np.empty(len(S))
    for t, s in enumerate(S):
        z = lam * s + (1-lam) * z
        diff = (z - μ)[:, None]
        out[t] = (diff.T @ Σ_inv @ diff).item()
    return out


def ewma_stats_with_k(S, lam, μ, Σ_inv, n_train):
    z = np.zeros_like(μ)
    out = np.empty(len(S))
    for t, s in enumerate(S, start=1):
        z = lam * s + (1-lam) * z
        num = (lam/(2-lam)) * (1 - (1-lam)**(2*t)) + (3.72/n_train) * (1-(1-lam)**t)**2
        den = (lam/(2-lam)) * (1 - (1-lam)**(2*t)) + (1   /n_train)  * (1-(1-lam)**t)**2
        k   = np.sqrt(num / den)
        diff = (z / k - μ)[:, None]
        out[t-1] = (diff.T @ Σ_inv @ diff).item()
    return out


def bootstrap_ucl(x_tr, y_tr, *, lam, alpha, λ_cov, BO, BI, M, perc):
    scaler = StandardScaler()
    Xtr = scaler.fit_transform(x_tr[:, None])
    model = Ridge(alpha=alpha).fit(Xtr, y_tr)

    S_tr = score_vectors(Xtr, y_tr, model, alpha)
    μ  = S_tr.mean(axis=0)
    Σ  = np.cov(S_tr.T) + λ_cov * np.eye(S_tr.shape[1])
    Σi = np.linalg.pinv(Σ, hermitian=True)

    n = len(x_tr)
    res = np.empty((BO*BI, M))

    for b in range(BO):
        idx_boot = RNG.choice(n, n, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n), idx_boot)

        X_b   = scaler.fit_transform(x_tr[idx_boot, None])
        y_b   = y_tr[idx_boot]
        X_oob = scaler.transform(x_tr[idx_oob, None])
        y_oob = y_tr[idx_oob]

        model_b = Ridge(alpha=alpha).fit(X_b, y_b)
        S_oob   = score_vectors(X_oob, y_oob, model_b, alpha)

        for j in range(BI):
            S_draw = S_oob[RNG.choice(len(S_oob), M, replace=True)]
            res[b*BI + j] = ewma_stats_with_k(S_draw, lam, μ, Σi, n)

    return np.percentile(res, perc, axis=0)


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 3.  Single-database replicate
# ╚═══════════════════════════════════════════════════════════════════════════╝
def process_db(seed, cfg):
    (x_tr, y_tr), _, _ = generate_data(seed,
                                       n_train=cfg['n_train'],
                                       n_ic=cfg['n_ic'],
                                       n_oc=cfg['n_oc'])
    scaler = StandardScaler()
    Xtr = scaler.fit_transform(x_tr[:, None])
    model = Ridge(alpha=cfg['alpha']).fit(Xtr, y_tr)

    S_tr = score_vectors(Xtr, y_tr, model, cfg['alpha'])
    μ  = S_tr.mean(axis=0)
    Σ  = np.cov(S_tr.T) + cfg['λ_cov'] * np.eye(S_tr.shape[1])
    Σi = np.linalg.pinv(Σ, hermitian=True)

    # UCL (once per DB replicate)
    UCL = bootstrap_ucl(x_tr, y_tr, lam=cfg['lam'],
                        alpha=cfg['alpha'], λ_cov=cfg['λ_cov'],
                        BO=cfg['BO'], BI=cfg['BI'], M=cfg['M'], perc=cfg['perc'])

    # --- future replicates under the *null* -------------------------------
    exc = np.zeros((cfg['N_future'], cfg['M']))
    for j in range(cfg['N_future']):
        (_, (x_mon, y_mon), _) = generate_data(seed*cfg['N_future']+j,
                                               n_train=cfg['n_train'],
                                               n_ic=cfg['n_ic'],
                                               n_oc=cfg['n_oc'],
                                               case='null')
        X_mon = scaler.transform(x_mon[:, None])
        S_mon = score_vectors(X_mon, y_mon, model, cfg['alpha'])[:cfg['M']]
        T2    = ewma_stats_no_k(S_mon, cfg['lam'], μ, Σi)
        exc[j] = T2 > UCL
    return exc


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 4.  FAR evaluation driver
# ╚═══════════════════════════════════════════════════════════════════════════╝
def evaluate_far(cfg, N_db=20, workers=None):
    if workers is None:
        workers = max(1, multiprocessing.cpu_count() - 1)

    all_ex = []
    with ProcessPoolExecutor(max_workers=workers) as ex:
        futs = [ex.submit(process_db, seed, cfg) for seed in range(N_db)]
        for _ in tqdm(as_completed(futs), total=N_db, desc="DB replicates"):
            all_ex.append(_.result())

    all_ex = np.array(all_ex)                     # (N_db, N_future, M)
    far    = all_ex.mean(axis=(0, 1))
    return pd.DataFrame({"t": np.arange(cfg['M']), "FAR": far})


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 5.  Plot helper
# ╚═══════════════════════════════════════════════════════════════════════════╝
def plot_far(df, α_nom=0.001):
    plt.figure(figsize=(10, 6))
    plt.plot(df.t, df.FAR, lw=2, label="Empirical FAR", color='#1f77b4')
    plt.axhline(α_nom, ls='--', lw=2, color='#d95f02', label=f"Nominal α={α_nom}")
    plt.ylim(0, α_nom*2)
    plt.xlabel("Time Index $i$")
    plt.ylabel("False Alarm Rate")
    plt.title("Pointwise FAR")
    plt.grid(alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 6.  Main run  (adjust BO/BI downward if you only want a smoke-test)
# ╚═══════════════════════════════════════════════════════════════════════════╝
if __name__ == "__main__":
    cfg = dict(
        # data sizes
        n_train = 2_000,
        n_ic    = 200,
        n_oc    = 800,
        M       = 1_000,

        # EWMA / model
        lam     = 0.01,     # EWMA λ
        alpha   = 0.01,     # ridge L2
        λ_cov   = 1e-2,      # Σ regularisation

        # bootstrap
        BO   = 100,
        BI   = 200,
        perc = 99.9,        # ⇒ nominal α = 0.001

        # future reps per DB
        N_future = 1_000,
    )

    print("\nEvaluating empirical FAR …")
    df_far = evaluate_far(cfg, N_db=50)           # increase N_db for tighter CIs
    plot_far(df_far)

results = {
    "config": cfg,
    "far_curve": df_far,  # (t, FAR)
    "timestamp": pd.Timestamp.now(),
    "description": "Linear model EWMA false-alarm rate at α = 0.001"
}


import pickle

with open("linear_ewma_far.pkl", "wb") as f:
    pickle.dump(results, f)

"""
Kungang Zhang et al. (2023) – type-I error verification
-------------------------------------------------------
Phase-I split: 50 % for model fitting (D₁) + 50 % for UCL estimation (D₂)

 • In-control process:  y = 16 x + 5 + ε,   ε ∼ N(0,1)
 • EWMA on score vectors, λ = 0.01
 • Constant UCL = 99.9-th percentile of T² sequence on D₂  (α ≈ 0.001)
 • FAR estimated from many future IC replicates
"""

# ─────────────────────────────────── 0. imports ────────────────────────────
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm, trange
import multiprocessing

plt.rcParams.update({
    "font.size":14, "axes.titlesize":18, "axes.labelsize":16,
    "legend.fontsize":14, "xtick.labelsize":12, "ytick.labelsize":12,
    "figure.dpi":150
})
RNG = np.random.default_rng()

# ───────────────────── 1. IC data generator – curve (4.1) ───────────────────
def generate_null_data(seed, n, m=16, c=5):
    rng = np.random.default_rng(seed)
    x = rng.uniform(-np.sqrt(3), np.sqrt(3), n)          # Var(x)=1
    y = m * x + c + rng.normal(0, 1, n)
    return x, y

# ───────────────────── 2. score vectors & MEWMA helpers ─────────────────────
def score_vectors(X, y, model, alpha):
    X_aug = np.hstack([np.ones((X.shape[0], 1)), X])
    β = np.append(model.intercept_, model.coef_)
    resid = y - X_aug @ β
    reg = 2 * alpha * np.append(0, model.coef_) / X.shape[0]
    return (-2 * resid)[:, None] * X_aug + reg

def ewma_T2(S, lam, μ, Σ_inv):
    z = np.zeros_like(μ)
    out = np.empty(len(S))
    for t, s in enumerate(S):
        z = lam * s + (1 - lam) * z
        diff = (z - μ)[:, None]
        out[t] = (diff.T @ Σ_inv @ diff).item()
    return out

# ───── 3. one DB replicate following Zhang’s two-phase (D₁/D₂) protocol ─────
def process_db_kungang(seed, cfg):
    # 3-1. split stable data
    x, y = generate_null_data(seed, cfg['n_total'])
    n_half = cfg['n_total'] // 2
    x_D1, y_D1 = x[:n_half], y[:n_half]   # training
    x_D2, y_D2 = x[n_half:], y[n_half:]   # Phase-I for μ̂ & UCL

    # 3-2. fit model on D₁
    scaler = StandardScaler()
    X_D1 = scaler.fit_transform(x_D1[:, None])
    model = Ridge(alpha=cfg['alpha']).fit(X_D1, y_D1)

    # 3-3. Σ̂ from D₁
    S_D1 = score_vectors(X_D1, y_D1, model, cfg['alpha'])
    Σ = np.cov(S_D1.T) + cfg['λ_cov'] * np.eye(S_D1.shape[1])
    Σ_inv = np.linalg.pinv(Σ, hermitian=True)

    # 3-4. μ̂ and UCL from D₂
    X_D2 = scaler.transform(x_D2[:, None])
    S_D2 = score_vectors(X_D2, y_D2, model, cfg['alpha'])
    μ = S_D2.mean(axis=0)                             # ← mean from D₂
    T2_D2 = ewma_T2(S_D2, cfg['lam'], μ, Σ_inv)
    UCL = np.percentile(T2_D2, cfg['perc'])           # constant limit

    # 3-5. future IC streams to estimate FAR
    exc = np.zeros((cfg['N_future'], cfg['M']))
    for j in range(cfg['N_future']):
        xf, yf = generate_null_data(seed * cfg['N_future'] + j, cfg['M'])
        Xf = scaler.transform(xf[:, None])
        Sf = score_vectors(Xf, yf, model, cfg['alpha'])
        T2_f = ewma_T2(Sf, cfg['lam'], μ, Σ_inv)      # uses μ̂, Σ̂ from Phase-I
        exc[j] = (T2_f > UCL)
    return exc

# ─────────────── 4. FAR driver (unchanged, just calls process_db) ───────────
def evaluate_far_kungang(cfg, N_db=50):
    workers = max(1, multiprocessing.cpu_count() - 1)
    all_exc = []
    with ProcessPoolExecutor(max_workers=workers) as pool:
        futs = [pool.submit(process_db_kungang, s, cfg) for s in range(N_db)]
        for fut in tqdm(as_completed(futs), total=N_db, desc="DB replicates"):
            all_exc.append(fut.result())
    all_exc = np.asarray(all_exc)                     # shape (db, rep, M)
    return pd.DataFrame({
        "i": np.arange(cfg['M']),
        "FAR": all_exc.mean(axis=(0, 1))
    })

# ───────────────────────── 5. simple FAR plot helper ────────────────────────
def plot_far(df, α_nom=0.001):
    plt.figure(figsize=(10, 6))
    plt.plot(df.i, df.FAR, lw=2, label="Empirical FAR", c="#1f77b4")
    plt.axhline(α_nom, ls="--", lw=2, c="#d95f02", label=f"Nominal α={α_nom}")
    plt.ylim(0, α_nom*20)
    plt.xlabel("Time index $i$")
    plt.ylabel("False-Alarm Rate")
    plt.title("Pointwise FAR of Zhang et al. two-phase chart")
    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()

# ─────────────────────────────────── 6. run ─────────────────────────────────
if __name__ == "__main__":
    cfg = dict(
        n_total  = 2_000,   # 1 000 in D₁ + 1 000 in D₂
        M        = 1_000,   # samples per monitoring stream
        N_future = 1_000,   # streams per DB replicate
        lam      = 0.01,    # EWMA λ (as in paper)
        alpha    = 0.01,    # ridge penalty
        λ_cov    = 1e-1,    # Σ regulariser
        perc     = 99.9,    # → α = 0.001
    )

    df_far2 = evaluate_far_kungang(cfg, N_db=50)   # raise N_db for tighter SEs
    plot_far(df_far2)

import matplotlib.pyplot as plt

def plot_two_far_curves(df_far, df_far2, α_nom=0.001):
    plt.figure(figsize=(10, 6))

    # Blue: our method
    plt.plot(df_far.t, df_far.FAR, label="Our method", color="#1f77b4", lw=2)

    # Orange: Zhang2023
    plt.plot(df_far2.t, df_far2.FAR, label="Zhang et al. (2023)", color="#d95f02", lw=2)

    # Nominal alpha reference
    plt.axhline(α_nom, color='gray', linestyle='--', lw=1.5, label=f"Nominal $\\alpha$={α_nom}")

    # Log scale
    plt.yscale('log')
    plt.ylim(1e-5, 1)  # Adjust as needed

    # Labels and legend
    plt.xlabel("Time Index $i$")
    plt.ylabel("False Alarm Rate (log scale)")
    plt.title("Pointwise False Alarm Rate Comparison (log scale)")
    plt.legend()
    plt.grid(alpha=0.3, which='both')
    plt.tight_layout()
    plt.show()

plot_two_far_curves(df_far, df_far2)

df_far2

import matplotlib.pyplot as plt
import numpy as np

def plot_two_far_curves(df_boot, df_const,
                        alpha_nom=0.001,
                        title="Point-wise FAR Comparison"):
    """
    df_boot  : DataFrame from *our* bootstrap method  (cols: 't', 'FAR')
    df_const : DataFrame from Zhang two-phase method (cols: 'i', 'FAR')
    """
    # ── make sure both have the same x-axis column name ────────────────────
    if 't' in df_boot.columns:
        x_boot = df_boot['t'].values
    else:                              # fallback: use the index
        x_boot = df_boot.index.values
    y_boot = df_boot['FAR'].values

    if 'i' in df_const.columns:
        x_const = df_const['i'].values
    elif 't' in df_const.columns:
        x_const = df_const['t'].values
    else:
        x_const = df_const.index.values
    y_const = df_const['FAR'].values

    # ── plot ───────────────────────────────────────────────────────────────
    fig, ax = plt.subplots(figsize=(10, 6))

    ax.semilogy(x_const, y_const, lw=2, ls="--",           # ← dashed line
            label="Method of [20]",                    # ← new legend text
            color="#ff7f0e")

    ax.semilogy(x_boot , y_boot , lw=2,
                label="Our bootstrap CL",
                color="#1f77b4")        # matplotlib’s ‘tab:blue’

    ax.axhline(alpha_nom, ls="--", lw=2, color="#d95f02",
               label=f"Nominal α={alpha_nom}")

    ax.set_xlabel("Sample index $i$")
    ax.set_ylabel("False-alarm Rate (log scale)")
    ax.set_title(title)
    ax.set_ylim(1e-6, 1e-0)
    ax.grid(alpha=0.3, which="both")
    ax.legend()
    plt.tight_layout()
    plt.show()

# ───────────────────── example call ─────────────────────
plot_two_far_curves(df_far, df_far2)

"""
Linear-model score-based EWMA — type-I error verification
( pointwise False–Alarm Rate ≈ nominal α = 0.001 )
"""

# ╔═══════════════════════════════════════════════════════════════════════════╗
# 0. Imports & global style
# ╚═══════════════════════════════════════════════════════════════════════════╝
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
from tqdm import tqdm

# --- publication fonts / colours -------------------------------------------
plt.rcParams.update({
    "font.size"       : 14,
    "axes.titlesize"  : 18,
    "axes.labelsize"  : 16,
    "legend.fontsize" : 14,
    "xtick.labelsize" : 12,
    "ytick.labelsize" : 12,
    "figure.dpi"      : 150
})

RNG = np.random.default_rng()


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 1.  Data generator
# ╚═══════════════════════════════════════════════════════════════════════════╝
# ─────────────────────────────────────────────────────────────────────────────
#  HEADLINE  (for plots / paper caption)
# ─────────────────────────────────────────────────────────────────────────────
"""
Linear-model score-based EWMA — type-I error verification
( pointwise False-Alarm Rate ≈ nominal α = 0.001 )

Set-up
  • Training & IC data     :  y = 16 x + 5 + ε              ← curve (4.1)
  • OC data for “power”    :  50 %  y = 16 x + 5 + ε
                              50 %  y = 12 x + 3 + ε        ← curve (4.2)
    (For FAR studies we keep case='null', therefore every stream is IC.)
"""

# … imports & plotting style unchanged …


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 1. Data generator  (adapted slopes & intercepts)
# ╚═══════════════════════════════════════════════════════════════════════════╝
def generate_data(seed, n_train, n_ic, n_oc,
                  m_ic=16,  c_ic=5,                 # ← curve (4.1)
                  m1=16, c1=5,                      # component 1   (4.1)
                  m2=12, c2=3,                      # component 2   (4.2)
                  case='null'):
    """
    • In-control (training & IC monitoring):
          y = m_ic · x + c_ic + ε          (here 16x + 5)
    • OC stream for power studies (case!='null'):
          50 %  y = m1 · x + c1 + ε        (16x + 5)
          50 %  y = m2 · x + c2 + ε        (12x + 3)
    • For type-I/FAR runs we pass case='null' so the mixture is *not* used.
    """
    rng = np.random.default_rng(seed)

    # training sample
    x_tr = rng.uniform(-np.sqrt(3), np.sqrt(3), n_train)
    y_tr = m_ic * x_tr + c_ic + rng.normal(0, 1, n_train)

    # in-control monitoring slice
    x_ic = rng.uniform(-np.sqrt(3), np.sqrt(3), n_ic)
    y_ic = m_ic * x_ic + c_ic + rng.normal(0, 1, n_ic)

    # out-of-control slice (only if we later set case!='null')
    x_oc = rng.uniform(-np.sqrt(3), np.sqrt(3), n_oc)
    if case == 'null':
        y_oc = m_ic * x_oc + c_ic + rng.normal(0, 1, n_oc)
    else:
        mask = rng.random(n_oc) < 0.5
        ε    = rng.normal(0, 1, n_oc)
        y_oc = np.where(mask,
                        m1 * x_oc + c1 + ε,   # curve (4.1)
                        m2 * x_oc + c2 + ε)   # curve (4.2)

    # concatenate IC + OC to form full monitoring stream
    x_mon = np.concatenate([x_ic, x_oc])
    y_mon = np.concatenate([y_ic, y_oc])

    return (x_tr, y_tr), (x_mon, y_mon), n_ic



# ╔═══════════════════════════════════════════════════════════════════════════╗
# 2.  Score vectors and EWMA helpers
# ╚═══════════════════════════════════════════════════════════════════════════╝
def score_vectors(X, y, model, alpha):
    X_aug = np.hstack([np.ones((X.shape[0], 1)), X])
    β     = np.append(model.intercept_, model.coef_)
    resid = y - X_aug @ β
    reg   = 2 * alpha * np.append(0, model.coef_) / X.shape[0]
    return (-2 * resid)[:, None] * X_aug + reg


def ewma_stats_no_k(S, lam, μ, Σ_inv):
    z = np.zeros_like(μ)
    out = np.empty(len(S))
    for t, s in enumerate(S):
        z = lam * s + (1-lam) * z
        diff = (z - μ)[:, None]
        out[t] = (diff.T @ Σ_inv @ diff).item()
    return out


def ewma_stats_with_k(S, lam, μ, Σ_inv, n_train):
    """
    EWMA path with finite-sample k-correction.
    Parameters
    ----------
    S       : (M, p) array – score vectors to feed the chart
    lam     : float       – EWMA smoothing λ
    μ       : (p,)        – *reference* mean of score vectors **for this outer bootstrap**
    Σ_inv   : (p,p)       – inverse covariance matrix for the same outer bootstrap
    n_train : int         – training-sample size used in k(·)
    """
    z   = np.zeros_like(μ)
    out = np.empty(len(S))
    for t, s in enumerate(S, start=1):
        z = lam * s + (1.0 - lam) * z

        # 0.632-style variance-inflation factor (Eq. 3.19 in the paper)
        num = (lam / (2 - lam)) * (1 - (1 - lam) ** (2 * t)) + (3.72 / n_train) * (1 - (1 - lam) ** t) ** 2
        den = (lam / (2 - lam)) * (1 - (1 - lam) ** (2 * t)) + (1.00 / n_train) * (1 - (1 - lam) ** t) ** 2
        k   = np.sqrt(num / den)

        diff     = (z / k - μ)[:, None]
        out[t-1] = (diff.T @ Σ_inv @ diff).item()
    return out


# ─────────────────────────────────────────────────────────────────────────────
# 2½.  Control-limit bootstrap – now uses μ_b and Σ_b from *outer* bootstrap
# ─────────────────────────────────────────────────────────────────────────────
def bootstrap_ucl(x_tr, y_tr, *, lam, alpha, λ_cov, BO, BI, M, perc):
    """Nested bootstrap with outer-specific centring / scaling."""
    n      = len(x_tr)
    scaler = StandardScaler()

    # storage for all inner-loop EWMA paths
    res = np.empty((BO * BI, M))

    for b in range(BO):

        # ----- outer (training-sample) bootstrap ----------------------------
        idx_boot = RNG.choice(n, n, replace=True)
        idx_oob  = np.setdiff1d(np.arange(n), idx_boot)

        X_b   = scaler.fit_transform(x_tr[idx_boot, None])
        y_b   = y_tr[idx_boot]
        X_oob = scaler.transform(x_tr[idx_oob, None])
        y_oob = y_tr[idx_oob]

        model_b = Ridge(alpha=alpha).fit(X_b, y_b)

        # score vectors for *outer* bootstrap sample and its OOB complement
        S_b       = score_vectors(X_b,   y_b,   model_b, alpha)
        S_oob     = score_vectors(X_oob, y_oob, model_b, alpha)

        μ_b       = S_b.mean(axis=0)
        Σ_b       = np.cov(S_b.T) + λ_cov * np.eye(S_b.shape[1])
        Σi_b      = np.linalg.pinv(Σ_b, hermitian=True)

        # ----- inner bootstrap loop ----------------------------------------
        for j in range(BI):
            S_draw = S_oob[RNG.choice(len(S_oob), M, replace=True)]
            idx    = b * BI + j
            res[idx] = ewma_stats_with_k(S_draw, lam, μ_b, Σi_b, n)

    # point-wise upper percentiles give the time-varying UCL
    return np.percentile(res, perc, axis=0)


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 3.  Single-database replicate
# ╚═══════════════════════════════════════════════════════════════════════════╝
def process_db(seed, cfg):
    (x_tr, y_tr), _, _ = generate_data(seed,
                                       n_train=cfg['n_train'],
                                       n_ic=cfg['n_ic'],
                                       n_oc=cfg['n_oc'])
    scaler = StandardScaler()
    Xtr = scaler.fit_transform(x_tr[:, None])
    model = Ridge(alpha=cfg['alpha']).fit(Xtr, y_tr)

    S_tr = score_vectors(Xtr, y_tr, model, cfg['alpha'])
    μ  = S_tr.mean(axis=0)
    Σ  = np.cov(S_tr.T) + cfg['λ_cov'] * np.eye(S_tr.shape[1])
    Σi = np.linalg.pinv(Σ, hermitian=True)

    # UCL (once per DB replicate)
    UCL = bootstrap_ucl(x_tr, y_tr, lam=cfg['lam'],
                        alpha=cfg['alpha'], λ_cov=cfg['λ_cov'],
                        BO=cfg['BO'], BI=cfg['BI'], M=cfg['M'], perc=cfg['perc'])

    # --- future replicates under the *null* -------------------------------
    exc = np.zeros((cfg['N_future'], cfg['M']))
    for j in range(cfg['N_future']):
        (_, (x_mon, y_mon), _) = generate_data(seed*cfg['N_future']+j,
                                               n_train=cfg['n_train'],
                                               n_ic=cfg['n_ic'],
                                               n_oc=cfg['n_oc'],
                                               case='null')
        X_mon = scaler.transform(x_mon[:, None])
        S_mon = score_vectors(X_mon, y_mon, model, cfg['alpha'])[:cfg['M']]
        T2    = ewma_stats_no_k(S_mon, cfg['lam'], μ, Σi)
        exc[j] = T2 > UCL
    return exc


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 4.  FAR evaluation driver
# ╚═══════════════════════════════════════════════════════════════════════════╝
def evaluate_far(cfg, N_db=20, workers=None):
    if workers is None:
        workers = max(1, multiprocessing.cpu_count() - 1)

    all_ex = []
    with ProcessPoolExecutor(max_workers=workers) as ex:
        futs = [ex.submit(process_db, seed, cfg) for seed in range(N_db)]
        for _ in tqdm(as_completed(futs), total=N_db, desc="DB replicates"):
            all_ex.append(_.result())

    all_ex = np.array(all_ex)                     # (N_db, N_future, M)
    far    = all_ex.mean(axis=(0, 1))
    return pd.DataFrame({"t": np.arange(cfg['M']), "FAR": far})


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 5.  Plot helper
# ╚═══════════════════════════════════════════════════════════════════════════╝
def plot_far(df, α_nom=0.001):
    plt.figure(figsize=(10, 6))
    plt.plot(df.t, df.FAR, lw=2, label="Empirical FAR", color='#1f77b4')
    plt.axhline(α_nom, ls='--', lw=2, color='#d95f02', label=f"Nominal α={α_nom}")
    plt.ylim(0, α_nom*2)
    plt.xlabel("Time Index $i$")
    plt.ylabel("False Alarm Rate")
    plt.title("Pointwise FAR")
    plt.grid(alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()


# ╔═══════════════════════════════════════════════════════════════════════════╗
# 6.  Main run  (adjust BO/BI downward if you only want a smoke-test)
# ╚═══════════════════════════════════════════════════════════════════════════╝
if __name__ == "__main__":
    cfg = dict(
        # data sizes
        n_train = 2_000,
        n_ic    = 200,
        n_oc    = 800,
        M       = 1_000,

        # EWMA / model
        lam     = 0.01,     # EWMA λ
        alpha   = 0.01,     # ridge L2
        λ_cov   = 1e-2,      # Σ regularisation

        # bootstrap
        BO   = 100,
        BI   = 200,
        perc = 99.9,        # ⇒ nominal α = 0.001

        # future reps per DB
        N_future = 1_000,
    )

    print("\nEvaluating empirical FAR …")
    df_far = evaluate_far(cfg, N_db=50)           # increase N_db for tighter CIs
    plot_far(df_far)

import matplotlib.pyplot as plt
import numpy as np

def plot_two_far_curves(df_boot, df_const,
                        alpha_nom=0.001,
                        title="Pointwise FAR Comparison"):
    """
    df_boot  : DataFrame from *our* bootstrap method  (cols: 't', 'FAR')
    df_const : DataFrame from Zhang two-phase method (cols: 'i', 'FAR')
    """
    # ── make sure both have the same x-axis column name ────────────────────
    if 't' in df_boot.columns:
        x_boot = df_boot['t'].values
    else:                              # fallback: use the index
        x_boot = df_boot.index.values
    y_boot = df_boot['FAR'].values

    if 'i' in df_const.columns:
        x_const = df_const['i'].values
    elif 't' in df_const.columns:
        x_const = df_const['t'].values
    else:
        x_const = df_const.index.values
    y_const = df_const['FAR'].values

    # ── plot ───────────────────────────────────────────────────────────────
    fig, ax = plt.subplots(figsize=(10, 6))

    ax.semilogy(x_const, y_const, lw=2, linestyle=(0, (5, 1, 1, 1)),           # ← dashed line
            label="FAR using CL of Zhang et al.'s method",                    # ← new legend text
            color="#ff7f0e")

    ax.semilogy(x_boot , y_boot , lw=2,
                label="FAR using our bootstrap CL",
                color="#1f77b4")        # matplotlib’s ‘tab:blue’

    ax.axhline(alpha_nom, ls="--", lw=2, color="#d95f02",
               label=f"Nominal FAR α={alpha_nom}")

    ax.set_xlabel("Sample index $i$")
    ax.set_ylabel("False-alarm rate (log scale)")
    ax.set_title(title)
    ax.set_ylim(1e-6, 1e-0)
    ax.grid(alpha=0.3, which="both")
    ax.legend()
    plt.tight_layout()
    plt.show()

# ───────────────────── example call ─────────────────────
plot_two_far_curves(df_far, df_far2)

